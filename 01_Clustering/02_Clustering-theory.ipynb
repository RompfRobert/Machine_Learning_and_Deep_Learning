{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "253px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"clustanomaly\"></a>\n",
        "# Clustering / anomaly detection\n",
        "\n",
        "Motivation: Discern patterns in the data, group together \"similar\" instances.\n",
        "\n",
        "Good question is: What is \"similar\"?\n",
        "Will be influenced by\n",
        "- Measurment of variables\n",
        "- Measurement of distance\n",
        "\n",
        "\n",
        "[source](https://slideplayer.com/slide/4992642/)\n",
        "\n",
        "<a href=\"https://slideplayer.com/4992642/16/images/3/What+is+a+natural+grouping+among+these+objects.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1nCx76lYU0ex_ASjL2iFnFFC_K6wwtps3\" width=60%></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "uL2XriyEyli0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The space of measurements is multi dimensional (as many dimensions as measured variables), we would like to **discover some \"neighborhoods\"** in the multi dimensional data. It is a key question, what **measurements** we choose, or whether we manipulate, i.e. transform the raw data before we measure distance.\n",
        "\n",
        "Since we are especially bad at looking at multi dimensional representations (above 3D), many times this activity does not only bring visualization but also representation/ dimensionality reduction with it.\n",
        "\n",
        "**There is a deep connection between \"spatial\" distance in data space and representation learning.**\n",
        "\n",
        "**Moreover if we discern the similarities between groups of datapoints, we can discern the \"dissimilar\" ones, that is the \"outliers\", or \"anomalous ones\"** \n",
        "\n",
        "<a href=\"https://i.stack.imgur.com/97PXE.png\"><img src=\"https://drive.google.com/uc?export=view&id=1KAEm9B19hocXTCGhsAZrQLmmKr3HYS3K\" width=400 heigth=400></a>\n"
      ],
      "metadata": {
        "id": "8ylv1G4QyohT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77y4P54u4AkB"
      },
      "source": [
        "\n",
        "## Causes of \"outliers\" \n",
        "\n",
        "[source](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/#three)\n",
        "\n",
        "* **Data Entry Errors:** Human errors such as errors caused during data collection, recording, or entry can cause outliers in data. For example: Annual income of a customer is  100,000 USD. Accidentally, the data entry operator puts an additional zero in the figure. Now the income becomes 1,000,000 USD which is 10 times higher. Evidently, this will be the outlier value when compared with rest of the population.\n",
        "\n",
        "* **Measurement Error:** It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty. For example: There are 10 weighing machines. 9 of them are correct, 1 is faulty. Weight measured by people on the faulty machine will be higher / lower than the rest of people in the group. The weights measured on faulty machine can lead to outliers.\n",
        "\n",
        "* **Experimental Error:** Another cause of outliers is experimental error. For example: In a 100m sprint of 7 runners, one runner missed out on concentrating on the ‘Go’ call which caused him to start late. Hence, this caused the runner’s run time to be longer than other runners. His total run time can be an outlier.\n",
        "\n",
        "* **Intentional Outlier:** This is commonly found in self-reported measures that involves sensitive data. For example: Teens would typically under report the amount of alcohol that they consume. Only a fraction of them would report actual value. Here actual values might look like outliers because the rest of the teens are under reporting the consumption.\n",
        "\n",
        "* **Data Processing Error:** Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.\n",
        "\n",
        "* **Sampling error:** For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset.\n",
        "\n",
        "* **Natural Outlier:** When an outlier is not artificial (due to error), it is a natural outlier. For instance: In my last assignment with a renowned insurance company, I noticed that the performance of top 50 financial advisors was far higher than rest of the population. Surprisingly, it was not due to any error. Hence, whenever we perform any data mining activity with advisors, we treat this segment separately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting outliers with basic descriptives\n",
        "\n",
        "<a href=\"https://pro.arcgis.com/en/pro-app/help/analysis/geoprocessing/charts/GUID-0E2C3730-C535-40CD-8152-80D794A996A7-web.png\"><img src=\"https://drive.google.com/uc?export=view&id=1ilU8v5Di-xO4aUpZ7M8LamAzMs3aETzk\" width=300 heigth=300></a>\n",
        "\n",
        "<a href=\"http://www.pybloggers.com/wp-content/uploads/2015/09/ratherreadblog.comwp-contentuploads201509iris_scatter-9c511da385a5344b661e2153e84c28382116721d.png\"><img src=\"https://drive.google.com/uc?export=view&id=18aXCTtROoCkzNLPgTrB1uaDgehJbL6nG\" width=400 heigth=400></a>\n",
        "<a href=\"https://i.stack.imgur.com/AQexi.png\"><img src=\"https://drive.google.com/uc?export=view&id=1k9x7qsYHzKcu8StRyvRU89rOdXKCd1VP\" width=400 heigth=400></a>\n",
        "\n",
        "\n",
        "[source](https://stackoverflow.com/questions/46068074/double-box-plots-in-ggplot2)\n",
        "\n",
        "Note that outliers in box plots are typically defined as data points further removed than the interquantile range (Q3-Q1) multiplied by a certain constant (e.g., 1.5) from Q1 (downwards) or Q3 (upwards).\n"
      ],
      "metadata": {
        "id": "i6pfsMlQy0wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we take seriously the justification of why we can use box plots as descriptives, we take on the assumption that the data is coming from some Gaussian distributions, where the **centers of mass (centroids) represent the \"prototypical\" entities.**\n",
        "\n",
        "<a href=\"http://axle-lang.org/images/kmeans.svg\"><a href=\"http://drive.google.com/uc?export=view&id=1l0oMj3EakRbopy7846aQaIZUmDUQ25d9\"><img src=\"https://drive.google.com/uc?export=view&id=1Z-eKlJ44YARb3vrnyICVv2jR2dxRLzwi\" width=400 heigth=400></a></a>\n",
        "\n",
        "\n",
        "This a very strong assumption, which does not hold for many distributions, e.g. in the case of a uniform distribution it would not make much sense to consider a value outlier on the basis of its distance (measured in interquartile distance) from the mean or the quartiles."
      ],
      "metadata": {
        "id": "ygSzd0m3y_GK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKx6-0bb4AkJ"
      },
      "source": [
        "## Most basic clustering: k-means\n",
        "\n",
        "Let us first only use the measured distances between data points in the input space. \n",
        "The most basic, distance based clustering approach in this case is the **k-means algorithm**. It is based on two important assumptions:\n",
        "\n",
        "- The number of clusters is a given parameter, namely $k$\n",
        "- The objective is to minimize the distances from cluster centroids, more concretely, to minimize the sum of the squared distances from the centroids:\n",
        "\n",
        "$$\\sum_n \\|\\mathbf x_n - \\mu_{\\mathbf x_n}\\|^2$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQC1cCOW4AkK"
      },
      "source": [
        "Although (somewhat surprisingly), the problem is [NP-hard](https://en.wikipedia.org/wiki/NP-hardness), the $k$-means algorithm itself is a simple iteration:\n",
        "\n",
        "<a href=\"https://openi.nlm.nih.gov/imgs/512/338/4507578/PMC4507578_sensors-15-13132-g015.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Yvm7n186-VYh50Aqdy2KzET7S107Mzc1\" width=200 heigth=200></a>\n",
        "<a href=\"https://www.jeremyjordan.me/content/images/2016/12/kmeans.gif\"><img src=\"https://drive.google.com/uc?export=view&id=19RUrEFQ9mWoIf2l2IDXbogeC_WpSqRKD\" width=400 heigth=400></a>\n",
        "\n",
        "[source](https://www.jeremyjordan.me/grouping-data-points-with-k-means-clustering/)\n",
        "\n",
        "Since the algorithm increases the sum of squared distances at each iteration and there are only a finite number of possible cluster assignments, it is guaranteed to converge, but can converge to a local minimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pro's and con's\n",
        "\n",
        "**Pro:**\n",
        "- Easy algorithm\n",
        "- Can scale pretty well\n",
        "- In practice it can converge quickly\n",
        "\n",
        "**Con:**\n",
        "- Can converge to local minimum\n",
        "- Susceptible to initial conditions (random start!!)\n",
        "- Have to set $k$ manually\n",
        "- It does not work well with clusters (in the original data) of different size and different density\n",
        "- Assumes \"globular\" data distribution\n",
        "- Can fail with one global cluster\n",
        "\n"
      ],
      "metadata": {
        "id": "cAge8Zf-0vMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization\n",
        "__Furthest first heuristic:__ As for initialization, a frequently used heuristic is to choose the first center at random from the data points, and choose the consecutive ones by picking up the data point which is the furthest from all centers chosen before. A randomized version of this strategy is the so called $k$-means++ initialization, which chooses the centers randomly, but with a probability proportional to the distance from the already chosen centers.\n",
        "\n",
        "__Repeated runs:__ Because of the danger of finding a (very) suboptimal solution, the algorithm is typically run several times with different initializations.\n",
        " "
      ],
      "metadata": {
        "id": "CLRJskEh016p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing $k$ \n",
        "Choosing $k$ is pretty problematic!\n",
        "\n",
        "**Elbow \"rule\"**: Gradually increase $k$, and stop when there is a steep inflection (\"elbow\") in the summarized distances. (Remember: Choose the minimum of expressive models...) \n",
        "\n",
        "<a href=\"https://www.jeremyjordan.me/content/images/2016/12/BzwBY.png\"><img src=\"https://drive.google.com/uc?export=view&id=1KWh5rdWDmP_k_ZVPcpMC9OzqLn7NbQ91\" width=400 heigth=400></a>"
      ],
      "metadata": {
        "id": "06pUQ_b903Xg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO9v72qR4AkL"
      },
      "source": [
        "## Gaussian mixture modeling\n",
        "\n",
        "Assumptions:\n",
        "- Unlabeled data points, want to find categories that they fall into\n",
        "- Gaussian Mixture model: data mixture of Gaussian (normal) distributions \n",
        "- Distribution determined according to latent (hidden) variables\n",
        "- Number of underlying variables (clusters, i.e. Gaussian mixture components) to be chosen\n",
        "\n",
        "Unlike $k$-means, this is an explicitly probabilistic model. Given the number of classes/components, say $k$, the model's parameters are\n",
        "\n",
        "- the parameters associated with each component Gaussian distribution: means and covariances $\\mu_1, \\Sigma_1, \\dots, \\mu_k, \\Sigma_k$\n",
        "- a $\\Pi$ probability distribution over the components/classes (the components' \"weights\")\n",
        "\n",
        "The hypothesis is that each and every data point is independently generated by the following generative process:\n",
        "\n",
        "- First pick up a random component/class according to the probabilities in $\\Pi$,\n",
        "- then generate a random sample from the Gaussian associated with the component/class chosen in the first step.\n",
        "\n",
        "Notice that this stochastic model contains the chosen component/class as a __hidden/latent random variable__: it plays an important role in the data generation process but its value is missing from the data available to us. Our task is exactly to find the value of this hidden variable for each data point."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multidimensional Gaussian\n",
        "\n",
        "Reminder of normal distribution: bell shape curve with location (mean) and width according to parameters.\n",
        "\n",
        "<a href=\"https://www.mathsisfun.com/data/images/normal-distribution-2.svg\"><img src=\"http://drive.google.com/uc?export=view&id=1XpTnTIQf6cd9Jn3FtBVSZQzGDsf1b-rX\" width=500 heigth=500></a>\n",
        "\n",
        "\n",
        "In two dimensional space any type of ellipse \n",
        "- Gaussian has parameters in two directions -> bell shape streteched and compressed in two directions\n",
        "- Rotation also possible\n",
        "\n",
        "<a href=\"https://cpb-us-e1.wpmucdn.com/sites.northwestern.edu/dist/c/788/files/2016/12/blog8-tz9vff.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1LkiZPuvLblj12aQ7OVWnpD-xpYIXh7K8\" width=500 heigth=500></a>"
      ],
      "metadata": {
        "id": "WL3MOv6P1dpH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l_D2mPy4AkM"
      },
      "source": [
        "#### Maximum likelihood estimation of parameters\n",
        "In accordance with [Bayes's theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) (with a uniform prior!!), we try to find the most probable model parameters by maximum likelihood estimation (MLE): If the set of all possible parameter vectors is $\\Theta$ then we want to find the $\\theta\\in \\Theta$ value for which\n",
        "\n",
        "$$\n",
        "    P(\\theta \\mid \\mathrm{data}) \\propto P(\\mathrm{data} \\mid \\theta)\n",
        "$$\n",
        "is maximal, that is, we are looking for \n",
        "\n",
        "$$\n",
        "\\underset{\\theta\\in \\Theta}{\\operatorname{argmax}}P(\\mathrm{data} \\mid \\theta).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4P57bJ34AkM"
      },
      "source": [
        "In certain cases MLE problems have closed form solutions (by taking partial derivatives), but in the case of more complex models typically numerical methods are used. For the Mixture of Gaussian problem the most used numerical methods is the\n",
        "Expectation Maximization (EM) algorithm  [\"expectation maximization\" (Wikipedia)](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdqcLYzs4AkN"
      },
      "source": [
        "#### Expectation Maximization (EM)\n",
        "\n",
        "EM is actually a _family_ of similar algorithms to do MLE for a wide range of models with hidden/latent variables. It can be applied in situations where it is (relatively) easy to calculate \n",
        "- the (probability distributions of the) hidden variable values for the data points if the model parameters are known, and\n",
        "- the MLE of the model parameters if the (probability distributions of the) hidden variable values are known,\n",
        "\n",
        "but there is a complicated interdependence between these two.\n",
        "\n",
        "The algorithm starts with a random initialization of the model parameters, and then proceeds with an iteration consisting of the following two steps\n",
        "\n",
        "- __E step:__ Calculate the distribution of the hidden variable values for the data points on the basis of the actual model parameters.  \n",
        "- __M step:__ Recalculate the model parameters by doing MLE on the filled in data points.\n",
        "\n",
        "until convergence. In the case of a Gaussian mixture, the E step consists of calculating the probabilities of belonging to the components/clusters for each data points, while the M step is the estimation of the component weights and Gaussian parameters using the statistics of the data points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Advantages and disadvantages\n",
        "\n",
        "<a href=\"https://www.mathworks.com/help/examples/stats/win64/GaussianMixtureModelsExample_05.png\"><img src=\"https://drive.google.com/uc?export=view&id=1IoTZoJY3jGv-XkNs6D1NwhggDLer31B6\" width=500 heigth=500></a>\n",
        "\n",
        "__Advantages__\n",
        "- In two or higher dimensional space Gaussian distribution can take any **elliptical** shape (K-means can only take circular shapes)\n",
        "- **Likelihood of being in particular cluster** can be estimated\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1BzdAQKj8X8x8MZJWkYXkhTaw3IS3L1p-\"><img src=\"https://drive.google.com/uc?export=view&id=1rvwIEiM-nX3vboODzoeG_7NUGi19sTyx\" width=500 heigth=500></a>\n",
        "\n",
        "__Disadvantages__\n",
        "- Susceptible to **initial conditions** (random start!!)\n",
        "- The **number of components** is still a hyperparameter to be set\n",
        "- Gaussian **sensitive to outlier**\n",
        "- Assumption: **central value** = mean value that is normal, and mean=median=mode\n",
        "- Can converge to a **local minimum**, in particular, can converge to **degenerate** \"solutions\", e.g., with a single point component or with components with the same parameters."
      ],
      "metadata": {
        "id": "fLhRy-j64vvI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T13:44:57.331208Z",
          "start_time": "2019-08-21T13:44:57.322588Z"
        },
        "hideCode": true,
        "hidePrompt": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "-e9_gfGu4AkO",
        "outputId": "334cba6f-8f1b-44a2-c6a3-bb2fe586bc49"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/cGhxnMveD18\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/cGhxnMveD18\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QpNygXp4AkQ"
      },
      "source": [
        "#### Relationship to k-means\n",
        "\n",
        "The similarity between k-means and Gaussian mixture EM is not accidental: in fact, k-means can be seen as a simplified version of the latter where the **weights are restricted to be equal, only \"sphere-like\" Gaussians** are allowed, and the E-step works with **\"hard\" (one-hot) approximations** of the latent value distributions.\n",
        "\n",
        "In fact, k-means is often used to _initialize_ a Gaussian mixture EM with useful starting values.\n",
        "\n",
        "#### See also\n",
        "[Gaussian Mixtures in Scikit](http://scikit-learn.org/stable/modules/mixture.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r9Wqsr34AkQ"
      },
      "source": [
        "### Using gaussians for anomaly detection\n",
        "\n",
        "<a href=\"https://1.bp.blogspot.com/-9pslFt9Yt08/V3VyKRpWn6I/AAAAAAAAAkY/hjwKMBO_1wAcWr2zBON_ZAG-R8Te9A15QCLcB/s640/Screen%2BShot%2B2016-06-30%2Bat%2B2.25.05%2BPM.png\"><img src=\"https://drive.google.com/uc?export=view&id=1fTg3Fzgy7v0KZWO5Lr4iqCHPqHklK4R_\" width=600 heigth=600></a>\n",
        "\n",
        "(Original source: Andrew Ng's coursera course on Machine Learning.)\n",
        "\n",
        "**Interpretation: This configuration is of low probability -> Anomaly!**\n",
        "\n",
        "It is worth mentioning that robust statistics are more suitable for this kind of anomaly detection tasks.\n",
        "One of the methods utilizing such is Minimum Covariance Determinant from [Scikit](http://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html) (Unimodal Gauss preferred!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM8lsqbu4AkR"
      },
      "source": [
        "## A good \"workhorse\": DBSCAN\n",
        "(Density-based spatial clustering of applications with noise)\n",
        "\n",
        "<a href=\"https://image.slidesharecdn.com/dbscan-150327023217-conversion-gate01/95/db-scan-5-638.jpg?cb=1432096268\"><img src=\"https://drive.google.com/uc?export=view&id=1zSnYdJ4zwA2eUS6ASVb2qC5OHLvrk3-i\" width=600 heigth=600></a>\n",
        "\n",
        "[Source](https://www.slideshare.net/SKAhsan/db-scan-46350917)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does it work?\n",
        "The central concept the algorithm is built on is that of a __core sample__: a core sample is a data point which is in a sufficiently dense area of the data, that is, there are \"sufficiently many\" samples in a \"small enough\" neighborhood around it, where the \"sufficiently many\" and \"small enough\" have to be specified as input parameters to the algorithm. \n",
        "\n",
        "- The algorithm proceeds by recursively collecting core samples, their neighbors that are also core samples and so on in the same cluster.\n",
        "- In addition to core samples, clusters can also contain \"**peripheral**\" data points that are the neighbors of some core samples but themselves are not core samples.\n",
        "- In contrast to other clustering methods, with DBSCAN data points can end up with **clusterless outliers** if they are neither core samples nor peripherals.\n"
      ],
      "metadata": {
        "id": "cIbSfxYp5z1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pro's and con's\n",
        "\n",
        "**Pro:** \n",
        "- We don't have to define cluster number before as parameter!\n",
        "- Can handle arbitrary shape of clusters\n",
        "- Outliers do not influence it\n",
        "- Scales really well\n",
        "\n",
        "**Con:** \n",
        "- Not totally deterministic\n",
        "- Does not work well on problems with radically different densities with default settings\n",
        "- It depends strongly on the similarity metric used\n",
        "- Difficulty in separating adjacent clusters"
      ],
      "metadata": {
        "id": "Bs8pn4SE52Fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A visualization of how dbscan proceeds"
      ],
      "metadata": {
        "id": "ojMJpPuZ6rVG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T13:45:34.504404Z",
          "start_time": "2019-08-21T13:45:34.492573Z"
        },
        "hideCode": true,
        "hidePrompt": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "9VHdoTtt4AkR",
        "outputId": "82f6b0ba-24b2-4700-e0be-5480dfc96edd"
      },
      "source": [
        "iframe = '<iframe src=\"https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/\" width=\"600\" height=\"600\"></iframe>'\n",
        "HTML(iframe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe src=\"https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/\" width=\"600\" height=\"600\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See also\n",
        "\n",
        "[A journey to custering: Introducton to DBSCAN](https://medium.com/@i.v./a-journey-to-clustering-introduction-to-dbscan-e724fa899b6f)\n",
        "\n",
        "Improved version: [HDBSCAN](https://github.com/scikit-learn-contrib/hdbscan)\n"
      ],
      "metadata": {
        "id": "DyX0JZ5p69K0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Density based clustering for anomaly detection: [Local Outlier Factor (LOF)](https://en.wikipedia.org/wiki/Local_outlier_factor)\n",
        "\n",
        "Outlier:\n",
        "- Compare local density of an object to the local densities of its neighbors \n",
        "- Local density defined as distance it takes for one point to be reached from it's neighbours \n",
        "- Within cluster - anything fulfilling density requirements -> within local area of density\n",
        "- Outlier -> outside of areas of density\n",
        "- Areas of highest density taken to define areas of low density-> Local Outlier Factor\n",
        "\n",
        "The local density is estimated by the typical distance at which a point can be \"reached\" from its neighbors. The definition of \"reachability distance\" used in LOF is an additional measure to produce more stable results within clusters.\"\n",
        "\n",
        "<a href=\"https://upload.wikimedia.org/wikipedia/commons/4/4e/LOF-idea.svg\"><img src=\"http://drive.google.com/uc?export=view&id=1uP5n2gX4vGYowNu7W0ENR6o6KMG0wBUz\" weight=600 heigth=600></a>\n",
        "\n",
        "For newer neighborhood based methods see for example: [A Neighborhood-Based Clustering Algorithm](https://pdfs.semanticscholar.org/64ae/90e0ccaf46a0df38e6bfb6e5c418a7ce1848.pdf)\n"
      ],
      "metadata": {
        "id": "ptERWrtr7OWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How do we measure the goodness of clustering?\n"
      ],
      "metadata": {
        "id": "ooMe02sU7yFx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f68XsQD84AkR"
      },
      "source": [
        "### Internal measures\n",
        "\n",
        "- Quality evaluated based on data that the clustering was based on itself -> internal\n",
        "- Has to make implicity assumption on clustering objective!!\n",
        "\n",
        "**Goals:** \n",
        "- high similariy within cluster: intra-cluster similarity (low scatter, $S_i$), \n",
        "- low similariy between clusters: inter-cluster similarity (separation, $M_{ij}$), \n",
        "- stability (if we remove or perturb some datapoints, no large-scale change should occure)\n",
        "\n",
        "\n",
        "(Connection with [Fisher information](https://en.wikipedia.org/wiki/Fisher_information#Matrix_form))\n",
        "\n",
        "- M: distance between centroids\n",
        "- S: distance of elements within clusters\n",
        "\n",
        "**Indices:**\n",
        "- Davies–Bouldin index: $f(\\frac{S_i+S_j}{M_{ij}})$\n",
        "- Dunn index $f(\\frac{min(M_{ij})}{max(S_i)})$\n",
        "- Sihouette Coefficient $s(i) = \\frac{b(i)-a(i)}{max(a(i),b(i))}$, where $a(i)$ is the average deviation from the elements of the own cluster and $b(i)$ is the lowest distance from other clusters. This gives an overall \"view\" over all datapoints and the similairty of clusters.\n",
        "- SSE: Sum of Squared Error:  squared sum of distances of points from cluster centroids.\n",
        "\n",
        "**Practical methods:**\n",
        "- \"Grid-search\", \"elbow method\": Iteratively increase cluster number, calculate one of the indices above\n",
        "- Cross-validation (k * CV)based stability test: Average distance of holdout datapoints from the clusters. [see more](https://arxiv.org/pdf/1702.02658.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### External measures\n",
        "- Comparison against some ground truth\n",
        "- Performance when used in practical tasks, e.g., in clustering-based search\n",
        "\n",
        "Beside these, if we have some labels at least for part of the dataset, we can utilize some error metrics from classification.\n"
      ],
      "metadata": {
        "id": "JjNy4WcX75FY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's recap: Approaches to clustering\n",
        "\n",
        "1. Distance or centroid based methods \n",
        "2. Distribution based/probability based methods (can be seen as exension of distance)\n",
        "3. Density based methods / neighborhood\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://charlesmartin14.files.wordpress.com/2012/10/spec.png\"><img src=\"https://drive.google.com/uc?export=view&id=1pkZ4d4ZitCb8IIO4g7LbDYnWKLh51tDa\" width=400 heigth=400></a> \n"
      ],
      "metadata": {
        "id": "Kd7g7C_F7198"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Can we use clusters for representation?\n",
        "\n",
        "Sure!\n",
        "\n",
        "Let us store the \"centroids\", that is \"best representatives of their groups\" and recalculate the distance of data points from the centroids. see [this question](https://stats.stackexchange.com/questions/288668/clustering-as-dimensionality-reduction)\n",
        "\n",
        "But wait, we are effectively doing is reducing the data dimensionality! (se eg. t-SNE later)\n",
        "\n",
        "Yes, that's correct. As mentioned, there is a deep connection between clustering and dimensionality reduction that we'll elaborate further later.\n",
        "\n",
        "In fact, in [vector quantization](https://en.wikipedia.org/wiki/Vector_quantization) clustering is used even more radically -- for data compression: In that case, centroids are stored and data points are simply replaced by the index of the cluster they belong to. A well known application of this method is the compression of images with a high number of colors to one with a restricted palette: the colors of the new palette are exactly the cluster centroids in the color space of the picture:\n",
        "\n",
        "<a href=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_001.png\"><img src=\"https://drive.google.com/uc?export=view&id=12-IqSeddh5uMu8_ynhV8TnJ88uauSc5-\" width=60%></a>\n",
        "\n",
        "<a href=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_002.png\"><img src=\"https://drive.google.com/uc?export=view&id=1FzOEq2fxPiDgmktFHv5LPz5SKSBccdvN\" width=60%></a>\n",
        "\n",
        "<a href=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_003.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Ih7IGUx_CA4sbOx0lB95cGuj_84cED84\" width=60%></a>\n",
        "\n",
        "(Source: [Color Quantization using K-Means, scikit-learn example](https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html))\n"
      ],
      "metadata": {
        "id": "dbe6i4Cp8AVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering for recommendations?\n",
        "\n",
        "Recommender engines were (and still are) one of the motivators behind clustering algorithms, since discovering the similarity based grouping in the data makes it possible to come up with recommendations. (\"Other people also liked this item\".)\n",
        "\n",
        "Though recommender engines are out of scope for this course, [this](https://hackernoon.com/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75) tutorial is well worth checking.\n"
      ],
      "metadata": {
        "id": "oEktiYtA8SO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems / challenges in clustering\n",
        "\n",
        "- How to handle different attributes (numeric, categorical)\n",
        "- Scalability (Will it run on _that_ big of a dataset?)\n",
        "- Too many dimensions (Curse of dimensionality)\n",
        "- Non-normalized attributes\n",
        "- Non spherical clusters\n",
        "- Effect of outliers\n",
        "- Convergence guarantees and robustness to random inicialization\n",
        "- Dependence on order of data (especially in streaming scenarios)\n",
        "- Hard (exclusive) or \"soft\", fuzzy goups\n",
        "- Adding new datapoints\n",
        "\n",
        "Each algorithm has it's pros and cons in these regards, it is worth considering before we use them!\n",
        "\n",
        "Consult [Scikit-Learn: Overview of clustering methods](http://scikit-learn.org/stable/modules/clustering.html)\n",
        "\n",
        "<a href=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png\"><img src=\"https://drive.google.com/uc?export=view&id=1vRmxesQaLoJacOo7AGW4C6CnZwabBmJ0\" width=800 heigth=800></a>"
      ],
      "metadata": {
        "id": "zTu7F4458T2M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh1JJlg4AkS"
      },
      "source": [
        "### Curse of dimensionality!\n",
        "\n",
        "If we have the same amount of measurements (data points) with _a lot_ higher dimensionality (variables), chances are, that distances will grow that big, that finding relations will be nearly impossible! (more on this later)\n",
        "\n",
        "<a href=\"http://www.visiondummy.com/wp-content/uploads/2014/04/curseofdimensionality.png\"><img src=\"https://drive.google.com/uc?export=view&id=1rZ3tRLxGoDRi4KgHTwaRUi5L_BSPGMx-\" width=600 heigth=600></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKdpWdho4AkS"
      },
      "source": [
        "+ The possible distances between points grow with the number of dimensions: the diameter of the $n$-dimensional unit-hypercube is $\\sqrt n$.\n",
        "+ As the figure shows, regions taking up the same percentage of the feature space can be well separated in lower dimensions but overlap in higher dimensions.\n",
        "+ It can even be shown that for certain distributions (e.g., when all attributes are identically and independently distributed), \n",
        "\n",
        "  > the relative difference of the distances of the closest (MinDist) and farthest\n",
        "(MaxDist) data points of an independently selected point tends to 0 as dimensionality\n",
        "increases:\n",
        "$$\\lim_{n \\rightarrow \\infty}\\frac{MaxDist-MinDist}{MinDist} = 0$$\n",
        "In such cases, the notion of distance becomes meaningless. \n",
        "\n",
        "  ([source](http://sa-ijas.stat.unipd.it/sites/sa-ijas.stat.unipd.it/files/22(1)_Balbi.pdf))"
      ]
    }
  ]
}