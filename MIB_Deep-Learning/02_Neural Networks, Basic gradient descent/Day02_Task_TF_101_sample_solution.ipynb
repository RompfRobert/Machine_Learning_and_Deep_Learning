{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYv9_ReNCHAn"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# Inputs\n",
        "x = tf.placeholder(tf.float32, shape=(1,3))\n",
        "y = tf.placeholder(tf.float32, shape=(1,))\n",
        "\n",
        "# Parameters\n",
        "W = tf.Variable([[3.0,2.0,0.5]], dtype=tf.float32)\n",
        "b = tf.Variable(1.0, dtype=tf.float32)\n",
        "\n",
        "# linear_model\n",
        "linear_model = tf.matmul(x,tf.transpose(W))+b\n",
        "# This could be alternatively written tf.reduce_sum(tf.matmul(x,W, transpose_b=True)+b)\n",
        "\n",
        "# Here we calculate the squared loss of the \"\"\"model\"\"\"\n",
        "loss = tf.square(linear_model - y)\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initalize\n",
        "    sess.run(init)\n",
        "    \n",
        "    returned_loss = sess.run(loss, {x: [[1.0, 2.0, 3.0]], y: [13,]})\n",
        "    \n",
        "    # This should be the place for modifying the weights and bias to reduce error,\n",
        "    # but we will learn about this later on\n",
        "\n",
        "\n",
        "print(\"Returned loss:\",returned_loss[0])\n",
        "assert round(returned_loss[0].item(),2)==12.25\n",
        "# And yes, this is how a single numpy.float32's python float value can be acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5h-UJnHk59pm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}