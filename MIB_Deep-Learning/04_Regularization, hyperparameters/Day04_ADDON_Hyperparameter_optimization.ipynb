{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP0q7iNnj_RA"
      },
      "source": [
        "# Hyperparameter search\n",
        "\n",
        "<img src=\"https://i.kym-cdn.com/photos/images/newsfeed/000/531/557/a88.jpg\">\n",
        "\n",
        "Or: What if we use machine learning to look for parameters for machine learning models? :-P\n",
        "\n",
        "(Fun fact: The irony of the situation did occur to many - see the film \"Inception\"and the model \"Inception-v1\" - we will talk about it next class.)\n",
        "\n",
        "\n",
        "## Approaches:\n",
        "- Manual tuning aka. best practice / “folk wisdom”\n",
        "    - It can get obsolete in surprisingly short time, eg. [Bengio 2012](https://arxiv.org/abs/1206.5533) suggests values for various hyperparameters based on experience (which is interesting), but still uses prominently \"layerwise pretraining\"-et, which is totally forgotten by now. (It was quite tedious, it did not bring as much as people hoped for, but noone formally proved it wrong..)\n",
        "- Scientifically well founded method\n",
        "    - We simply don't have analytic understanding\n",
        "    - If we don't have that, how do we know anything if not by simulation? See next point...\n",
        "- Application of search methods\n",
        "    - We cast the search for optimal parameters as an optimization problem\n",
        "    \n",
        "## Broader context: AutoML\n",
        "\n",
        "<img src=\"https://d3ansictanv2wj.cloudfront.net/image1-3250b5283382455a64a9b3fabc39f6c2.jpg\" width=55%>\n",
        "\n",
        "We call **AutoML** all those activities in Data Science whereby we want to automate tasks (not infrequently with machine learning).\n",
        "\n",
        "Thus can be:\n",
        "- Automation of data cleaning / feature engineering  eg. [here](https://github.com/featuretools/featuretools/)\n",
        "- Automatic setting of hyperparameters\n",
        "- Search for neural components (eg. the Swish activation function we mentioned before : [Searching for Activation Functions](https://arxiv.org/abs/1710.05941))\n",
        "- Neural architecture search - that is the automatic design of neural \"layouts\" specifically designed for a given problem. See more [here](https://www.oreilly.com/ideas/what-is-neural-architecture-search)\n",
        "\n",
        "**For a broader overview it is worth studying the recent paper [AutoML: A Survey of the State-of-the-Art](https://arxiv.org/abs/1908.00709).**\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?7export=view&id=1xp2zv7RJCQWA7BAQP7nsoS6A1Ugko5jO\" width=85%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9X4iAnrj_RE"
      },
      "source": [
        "## How does this search space look like?\n",
        "- Individual evaluation steps are very expensive (even in case of short “trial runs”)\n",
        "- We do not know much about the space, we don't think it is convex it can have non-trivial interactions between the dimensions, though there ougth to be wider \"plateaus\" with good solutions\n",
        "- Exploration / exploitation tradeoff\n",
        "- Albeit very well parallelizable even at first glance\n",
        "\n",
        "## Gridsearch\n",
        "\n",
        "We create a hypercube in the parameter space which we divide into unified regions (still tolerable in number), choose a point in each region (a combination of parameters) and then iteratively start training with that combination.\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/02p4O.png\" width=450 heigth=450 align=\"left\">\n",
        "<img src=\"https://i.imgur.com/MsYDib8.jpg\" width=450 heigth=450 align=\"right\" style=\"float:left;margin:0 10px 10px 0\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E5QUrvoj_RF"
      },
      "source": [
        "### Simple gridsearch example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "L2EODBakj_RF"
      },
      "source": [
        "import itertools\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "\n",
        "log_dir = \"logs\"\n",
        "\n",
        "model_path = log_dir + \"/model.ckpt\"\n",
        "\n",
        "# Task parameters\n",
        "\n",
        "input_size = 784\n",
        "n_classes = 10\n",
        "\n",
        "# Data \n",
        "\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "n_train = np.shape(mnist.train.images)[0]\n",
        "\n",
        "def create_and_train_model(n_hidden_layers=3,\n",
        "                           hidden_layer_size=128,\n",
        "                           initial_lr=0.5,\n",
        "                           batch_size=100,\n",
        "                           constant_lr_epochs = 15,\n",
        "                           lr_decay = 0.8,\n",
        "                           early_stopping_patience = 3,\n",
        "                           drop_out_keep = 0.8,\n",
        "                           train_writer_path=log_dir + \"/train\",\n",
        "                           valid_writer_path=log_dir + \"/valid\",\n",
        "                           model_path=model_path,\n",
        "                           save_below_valid_loss=np.inf):\n",
        "    \"Create and train a model with the given parameters.\"\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Input\n",
        "\n",
        "    keep_prob = tf.placeholder_with_default(1.0, shape=(), name=\"keep_prob\")\n",
        "    learning_rate = tf.placeholder(tf.float32, shape=(), name=\"learning_rate\")\n",
        "    X = tf.placeholder(tf.float32, shape=[None, input_size], name=\"X\")\n",
        "    Y = tf.placeholder(tf.float32, shape=[None, n_classes], name=\"Y\")\n",
        "\n",
        "    # Model internals\n",
        "\n",
        "    # hidden layers\n",
        "\n",
        "    hidden_layer_sizes =  n_hidden_layers * [hidden_layer_size]\n",
        "    layer_sizes = [input_size] + hidden_layer_sizes\n",
        "    last_layer = X\n",
        "\n",
        "    for n in range(len(hidden_layer_sizes)):\n",
        "        cur_layer_input_size = layer_sizes[n]\n",
        "        cur_layer_output_size = layer_sizes[n+1]\n",
        "        weights = tf.Variable(tf.random_normal([cur_layer_input_size, cur_layer_output_size]),\n",
        "                              name=\"W_hidden_\" + str(n))\n",
        "        bias = tf.Variable(tf.random_normal([cur_layer_output_size]),\n",
        "                           name=\"b_hidden_\" + str(n))\n",
        "        activations = tf.sigmoid(last_layer @ weights + bias)\n",
        "        last_layer = tf.nn.dropout(activations, keep_prob=keep_prob)\n",
        "\n",
        "    # Output layer \n",
        "    \n",
        "    W = tf.Variable(tf.random_normal([cur_layer_output_size, n_classes]), name=\"W\")\n",
        "    b = tf.Variable(tf.random_normal([n_classes]), name=\"b\")\n",
        "    logits = tf.add(last_layer @ W, b, name=\"logits\")\n",
        "\n",
        "    # Loss and optimizer\n",
        "\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits),\n",
        "                          name=\"loss\")\n",
        "    train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "    # Accuracy\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(logits,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
        "\n",
        "    # Init, save & summary opts\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    accuracy_summary = tf.summary.scalar(\"Accuracy_Summary\", accuracy)\n",
        "    loss_summary = tf.summary.scalar(\"Loss_Summary\", loss)\n",
        "    merged = tf.summary.merge_all()\n",
        "    \n",
        "    # Training\n",
        "    ##########\n",
        "\n",
        "    train_writer = tf.summary.FileWriter(train_writer_path, graph=tf.get_default_graph())\n",
        "    valid_writer = tf.summary.FileWriter(valid_writer_path, graph=tf.get_default_graph())\n",
        "    n_epoch_steps = n_train // batch_size\n",
        "    min_valid_loss = np.inf\n",
        "    valid_accuracy_for_min_loss = 0\n",
        "    n_deteriorating_losses = 0\n",
        "    step = 1\n",
        "    epoch = 1\n",
        "    lr = initial_lr\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "        while True:\n",
        "            batch = mnist.train.next_batch(batch_size)\n",
        "            _ = sess.run([train_step],\n",
        "                         feed_dict={X: batch[0],\n",
        "                                    Y: batch[1],\n",
        "                                    keep_prob: drop_out_keep,\n",
        "                                    learning_rate: lr})\n",
        "            if step % n_epoch_steps == 0: # we are at the end of an epoch\n",
        "                train_loss_val, train_accuracy_val, train_summary \\\n",
        "                    = sess.run([loss, accuracy, merged],\n",
        "                               feed_dict={X: mnist.train.images,\n",
        "                                          Y: mnist.train.labels})\n",
        "                train_writer.add_summary(train_summary, step)\n",
        "                valid_loss_val, valid_accuracy_val, valid_summary \\\n",
        "                    = sess.run([loss, accuracy, merged],\n",
        "                               feed_dict={X: mnist.validation.images,\n",
        "                                          Y: mnist.validation.labels})\n",
        "                valid_writer.add_summary(valid_summary, step)\n",
        "                print(f\"Epoch {epoch}. Train loss: {train_loss_val}, train_accuracy: {train_accuracy_val}, valid loss: {valid_loss_val}, valid accuracy: {valid_accuracy_val}.\")\n",
        "                if valid_loss_val >= min_valid_loss:\n",
        "                    n_deteriorating_losses += 1\n",
        "                    print(\"Deteriorating loss no.\", n_deteriorating_losses)\n",
        "                    if n_deteriorating_losses == early_stopping_patience:\n",
        "                        print(\"Patience's run out, stopping.\")\n",
        "                        break\n",
        "                else:\n",
        "                    n_deteriorating_losses = 0\n",
        "                    min_valid_loss = valid_loss_val\n",
        "                    valid_accuracy_for_min_loss = valid_accuracy_val\n",
        "                    if min_valid_loss < save_below_valid_loss:\n",
        "                        print(f\"Saving the model since new loss {min_valid_loss} is the global minimum so far.\")\n",
        "                        saver.save(sess, model_path)\n",
        "                epoch += 1\n",
        "                if epoch > constant_lr_epochs:\n",
        "                    lr *= lr_decay\n",
        "                    print(f\"Learning rate set to {lr}.\")\n",
        "            step += 1\n",
        "        print(f\"The best loss on the the validation set was {min_valid_loss}.\")\n",
        "        return min_valid_loss, valid_accuracy_for_min_loss\n",
        "\n",
        "    \n",
        "def grid_search(layer_sizes, layer_ns, initial_learning_rates):\n",
        "    \"Perform a grid search with the given parameter lists.\"\n",
        "    min_valid_loss = np.inf\n",
        "    configs = itertools.product(layer_sizes, layer_ns, initial_learning_rates)\n",
        "    for layer_size, layer_n, initial_lr in configs:\n",
        "        print(\"=====================================\")\n",
        "        print(f\"Training model with layer size {layer_size}, layer number {layer_n} and initial learning rate {initial_lr}.\")\n",
        "        cur_model_name = f\"ls{layer_size}_ln{layer_n}_ilr{initial_lr}\"\n",
        "        cur_valid_loss, \\\n",
        "            cur_valid_acc = create_and_train_model(n_hidden_layers=layer_n,\n",
        "                                                   hidden_layer_size=layer_size,\n",
        "                                                   initial_lr=initial_lr,\n",
        "                                                   train_writer_path=log_dir + \"/\" + cur_model_name + \"_train\",\n",
        "                                                   valid_writer_path=log_dir + \"/\" + cur_model_name + \"_valid\",\n",
        "                                                   model_path=model_path,\n",
        "                                                   save_below_valid_loss=min_valid_loss)\n",
        "        if cur_valid_loss < min_valid_loss:\n",
        "            min_valid_loss = cur_valid_loss\n",
        "            accuracy_for_min_valid_loss = cur_valid_acc\n",
        "            best_model_name = cur_model_name\n",
        "        print(f\"The current minimal valid loss is {min_valid_loss} with {accuracy_for_min_valid_loss} accuracy, produced by model {best_model_name}.\")\n",
        "\n",
        "\n",
        "# Perform the actual grid search\n",
        "\n",
        "grid_search([32, 64, 128], [1, 2, 3], [1., 0.1, 0.01])\n",
        "\n",
        "# Measure performance on the test data set\n",
        "\n",
        "tf.reset_default_graph()\n",
        "saver = tf.train.import_meta_graph(model_path + \".meta\")\n",
        "\n",
        "with tf.Session() as sess:\n",
        "     print(\"Restoring the best model to test it.\")\n",
        "     saver.restore(sess, model_path)\n",
        "     test_loss_val, test_accuracy_val = sess.run([\"loss:0\", \"accuracy:0\"],\n",
        "                                                 feed_dict={\"X:0\": mnist.test.images,\n",
        "                                                            \"Y:0\": mnist.test.labels})\n",
        "     print(f\"The loss on the the test set is {test_loss_val}.\")\n",
        "     print(f\"The accuracy on the test set is {test_accuracy_val}.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxNpOkCCj_RL"
      },
      "source": [
        "### Valid loss on different models\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1-7aZgdcItHFmt9lNlwBCxzN6cpL88pfU\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Dg1VvMj_RN"
      },
      "source": [
        "\n",
        "## Random search \n",
        "\n",
        "We would think, that randomly trying combinations is not that fruitful.\n",
        "In fact, we are not that lucky to be right.\n",
        "[Bergstra and Bengio 2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) analyzed the question, and came up with the surprising result, that doing random search is a bit more effective in many cases than a grid based solution (given the same computational budget).\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=13aQyREuB9eEJBRsR1ExLbquB2sz5-zvJ\" width=70%>\n",
        "\n",
        "\n",
        "## “Exotic” optimalization solutions\n",
        "\n",
        "**They are not exotic at all, we can just not talk about everything. There are complete paradigms out there that we did not discuss, and can any time make a comeback!**\n",
        "\n",
        "Our starting point: We ought to do better than random search!\n",
        "\n",
        "What can we use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BslhFi-ij_RO"
      },
      "source": [
        "### Bayesian learning\n",
        "\n",
        "We can utilize the paradigm of bayesian learning for the search of hyperparameters.\n",
        "\n",
        "We did not go into detail about the bayesian methods of probability, so we don't discuss this further here. Some intorductory resources can be found [here](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el7MI93Mj_RO"
      },
      "source": [
        "### \"Evolutionary computation\" / \"Genetic Algorithms\"\n",
        "\n",
        "Also a biologically inspired computational method, which is based on the notion of \"inheritance\" and selection of good solution (parts).\n",
        "\n",
        "#### Main types:\n",
        "\n",
        "<img src=\"http://slideplayer.com/slide/3200475/11/images/7/What+are+the+different+types+of+EAs.jpg\" width=500 heigth=500>\n",
        "\n",
        "We only detail this set of learning methods, because people applied it to hyperparameter search, or more importantly to architecture search under the name of \"neuroevolution\". SOme researchers created a domain specific language that defines possible structure candidates for neural networks, over which genetic methods can optimize. See more [here](https://arxiv.org/abs/1801.01563).\n",
        "\n",
        "#### Basics of genetic algorithms\n",
        "\n",
        "We describe one possible solution to a problem with a \"chromosome\" of binary values, where each location defines one \"choice\" in a space of settings.\n",
        "\n",
        "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22170200/Capture1-300x186.png\">\n",
        "\n",
        "If we relax the binary assumption and allow for the usage of scalar values also, we can easily cast the hyperparameter search problem as a \"chromosome\" of categorical and scalar values representing choices and settings for the training.\n",
        "\n",
        "The only thing we have to pay attention to is to ensure \"conformance\" during the genetic operations so as all chromosomes can be rightly interpreted as parameter combinations. This is usually pretty easy.\n",
        "\n",
        "##### Algorithm\n",
        "\n",
        "<img src=\"http://www.pohlheim.com/Papers/mpga_gal95/fig1.gif\">\n",
        "\n",
        "##### Fitness\n",
        "\n",
        "Again - and rather unsurprisingly - we need some kind of objective function for optimization, which is called \"fitness\" in this domain. This behaves the same as any cost function, but we have no restrictions on it, it does not have to be differentiable or even very smooth. The only criterion: in general it should allow for some kind of ranking.\n",
        "\n",
        "##### Selection\n",
        "\n",
        "It could be easily Top N - and many times it is - but if we aim to balance diversity with \"pressure\" towards goodness and avoiding \"monoculture\", we can choose more complicated methods, like tournament selection, though diversity is also strongly influenced by crossover and mutation operators.\n",
        "<img src=\"http://slideplayer.com/slide/5231384/16/images/35/Example:+Tournament+selection.jpg\" width=65%>\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/zBHSp.png\" width=65%>\n",
        "\n",
        "##### Combination / \"crossover\"\n",
        "\n",
        "<img src=\"https://image.slidesharecdn.com/seminaronga-1228280582477612-9/95/genetic-algorithms-made-easy-12-728.jpg?cb=1378336663\" width=500 heigth=500>\n",
        "\n",
        "##### Mutation\n",
        "\n",
        "<img src=\"https://lh6.ggpht.com/_NNjxeW9ewEc/TNrUJ0Z8KlI/AAAAAAAARKg/zcS23GZlcvw/tmp1C17_thumb_thumb.jpg?imgmax=800\" width=500 heigth=500>\n",
        "\n",
        "Please observe the tree structure! GA-s can rather elegantly represent decision trees for example!\n",
        "\n",
        "It is also owrth noting, that in the example above,the structure itself changed, since we switched to an operator with two arguments, so if we handle this right, we can cause structural change also and we may not need crossover at all, thus we simulate \"asexual reproduction\".\n",
        "\n",
        "#### Advantages\n",
        "\n",
        "- Parallel and distributed solutions appeared rather early, thus quite unsophisticated hardware could support the training.\n",
        "Let me show you! :-)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=0B2Mh1BtG1i-JMTJCQTY0QTFENjAxNUU4QjowLjE\" width=50%>\n",
        "\n",
        "- It can easily and naturally represent categorical values, it can handle scalars, it is very flexible. It can incorporate many constraints which can be part of the representation as well as the operations.\n",
        "\n",
        "Summary: **Evolutionary computation is not dead, just sleeping!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZev7L3j_RP"
      },
      "source": [
        "#### Genetic algorithm basic example\n",
        "\n",
        "There is a pretty straightforward example for using GA for hyperparameter search:\n",
        "\n",
        "[Let's evolve a neural network with genetic algorithm](https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164)\n",
        "\n",
        "It tunes four parameters:\n",
        "- Number of layers (depth)\n",
        "- Neurons per layer (width)\n",
        "- Activation function\n",
        "- Type of optimizer (SGD or else)\n",
        "\n",
        "The code can be found [here](https://github.com/harvitronix/neural-network-genetic-algorithm/blob/master/train.py)\n",
        "\n",
        "#### Evolutionary search as a direct optimizer for neural networks\n",
        "\n",
        "Interestingly, evolutionary methods can be also used directly for the gradient free optimization of neural models. This solution can be rather interesting in situations where reinforcement learning methods would be the only options. For a long time, evolutionary methods were forgotten, but there is now some kind of revival going on.\n",
        "\n",
        "If we compare or even better COMBINE gradient based methods with evolutionary ones, we can have better results! (Think: combination of gradients and mutation, different behavior in case of local gradient gaps, robustness...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-16T11:49:59.824558Z",
          "start_time": "2019-04-16T11:49:59.804516Z"
        },
        "id": "sSsvpnBOj_RQ",
        "outputId": "925b6597-a4cd-47f4-fa08-8a076aae710d"
      },
      "source": [
        "from IPython.display import HTML\n",
        "HTML(\"\"\"<video controls=\"\"  loop=\"\" preload=\"metadata\" class=\"arve-video fitvidsignore\"><source type=\"video/mp4\" src=\"https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/gradient_gap_composite.mp4\"></video>\n",
        "\"\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video autoplay=\"\" controls=\"\"  loop=\"\" preload=\"metadata\" class=\"arve-video fitvidsignore\"><source type=\"video/mp4\" src=\"https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/gradient_gap_composite.mp4\"></video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raFIY1Bpj_RR"
      },
      "source": [
        "But well, it also has some drawbacks..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-16T11:51:01.675483Z",
          "start_time": "2019-04-16T11:51:01.665157Z"
        },
        "id": "5YIb_Ijzj_RS",
        "outputId": "33badf42-1bf5-4737-d852-701cfd2af0b3"
      },
      "source": [
        "HTML(\"\"\"<video controls=\"\" controlslist=\"nodownload\" loop=\"\" preload=\"metadata\" class=\"arve-video fitvidsignore\"><source type=\"video/mp4\" src=\"https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/narrowing_path_composite.mp4\"></video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video autoplay=\"\" controls=\"\" controlslist=\"nodownload\" loop=\"\" preload=\"metadata\" class=\"arve-video fitvidsignore\"><source type=\"video/mp4\" src=\"https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/narrowing_path_composite.mp4\"></video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5cT2Bqoj_RS"
      },
      "source": [
        "**Well worth reading further [here](https://eng.uber.com/deep-neuroevolution/).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyhyr9yEj_RS"
      },
      "source": [
        "### Reinforcement learning\n",
        "\n",
        "In case of reinforcement learning we have \"sparse teaching signal\", so not every input has an associated \"label\".\n",
        "\n",
        "**This is a HUGE  paradigm in itself, pretty cutting edge**, but sadly out of scope for the curent course.\n",
        "\n",
        "Google “AutoML” has some Reinforcement Learning solutions in it.\n",
        "More details [here](https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html) and [here]( https://arxiv.org/abs/1611.02167).\n",
        "\n",
        "\n",
        "Some recent news about AutoML [here](https://news.mit.edu/2019/convolutional-neural-network-automation-0321#.XJeRl6BoKM4.facebook).\n",
        "\n",
        "### Other interesting approaches\n",
        "\n",
        "There are other paradigms inspired by nature, like: [\"Particle swarm\"](http://www.swarmintelligence.org/tutorials.php) vagy [\"Ant colony\"](https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms) methods.\n",
        "\n",
        "And there is a remarkable **global optimization method**:\n",
        "\n",
        "[A Global Optimization Algorithm Worth Using](http://blog.dlib.net/2017/12/a-global-optimization-algorithm-worth.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-16T12:44:29.627160Z",
          "start_time": "2019-04-16T12:44:29.609290Z"
        },
        "id": "JYQyCs9Fj_RT",
        "outputId": "a80ecc81-99de-46d8-c3cc-2d2eada40573"
      },
      "source": [
        "HTML(\"\"\"<video controls=\"\" controlslist=\"nodownload\" loop=\"\" preload=\"metadata\" class=\"arve-video fitvidsignore\"><source type=\"video/webm\" src=\"http://dlib.net/find_max_global_example.webm\"></video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video controls=\"\" controlslist=\"nodownload\" loop=\"\" preload=\"metadata\" class=\"arve-video fitvidsignore\"><source type=\"video/webm\" src=\"http://dlib.net/find_max_global_example.webm\"></video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0WNhkZwj_RT"
      },
      "source": [
        "# Famous last words:\n",
        "\n",
        "It also could have occured to us in case of previous models - eg. RandomForest and co. - that setting the hyperparameters is non-trivial.\n",
        "Naturally there was a need for an \"out of the box\" solution: [Sklearn-Hyperopt](https://github.com/hyperopt/hyperopt-sklearn)\n",
        "\n",
        "Where there is one solution, there is many more :-)\n",
        "- [auto-sklearn](https://automl.github.io/auto-sklearn/master/)\n",
        "- [NeuPy](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html)\n",
        "- [Sklearn-DEAP](https://github.com/rsteca/sklearn-deap)\n",
        "- [Optunity](https://github.com/claesenm/optunity)\n",
        "- [TPOT](https://github.com/EpistasisLab/tpot)\n",
        "- [Karoo gp](https://kstaats.github.io/karoo_gp/)\n",
        "- [Spearmint](https://github.com/JasperSnoek/spearmint)\n",
        "- [NEAT](https://github.com/CodeReclaimers/neat-python)\n",
        "- [McFly](https://github.com/NLeSC/mcfly/wiki/Technical-documentation)\n",
        "- [FAR-HO](https://github.com/lucfra/FAR-HO)\n",
        "\n",
        "There are long lists of libraries for example [here](https://medium.com/@mikkokotila/a-comprehensive-list-of-hyperparameter-optimization-tuning-solutions-88e067f19d9). This is an infinite subject."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9WX3RSKj_RT"
      },
      "source": [
        "In fact, there is also a combined library for **Neural Architecture Search and hyperparameter optimization** for \"the laymen\" called [Autokeras](https://github.com/keras-team/autokeras). This area is definitely getting commoditized!"
      ]
    }
  ]
}