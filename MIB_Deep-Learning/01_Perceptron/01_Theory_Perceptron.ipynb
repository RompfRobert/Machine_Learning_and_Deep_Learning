{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV7Q61JGtPC2"
      },
      "source": [
        "<a id=\"demo\"></a>\n",
        "# Short demo\n",
        "\n",
        "Perceptrons are just another model in [Scikit](http://scikit-learn.org/stable/modules/neural_networks_supervised.html), isn't it? \n",
        "\n",
        "<a href=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_mlp_alpha_0011.png\"><img src=\"https://drive.google.com/uc?export=view&id=19crcivUVK83eyGEgKWo9qB7MFH8YiCah\" width=400 heigth=400></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W78azNgBVfU",
        "outputId": "9f5d5c01-34e7-4963-a2b6-2dab3e7d07a6"
      },
      "source": [
        "import warnings \n",
        "warnings.filterwarnings('ignore') #For presentation purposes\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "X = [[0., 0.], [1., 1.]]\n",
        "y = [0, 1]\n",
        "\n",
        "\n",
        "clf = Perceptron(random_state=1)\n",
        "\n",
        "clf.fit(X, y) \n",
        "\n",
        "new_input=[[2., 2.], [-1., -2.]]\n",
        "pred = clf.predict(new_input)\n",
        "\n",
        "print(\"Predictions:\", pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions: [1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "all_inputs = X + new_input\n",
        "all_outs = list(y) + list(pred)\n",
        "sns.scatterplot(x=[x[0] for x in all_inputs], \n",
        "                y=[x[1] for x in all_inputs],\n",
        "                hue = all_outs,\n",
        "                style=[\"TRAIN\" for _ in range(len(X))] + [\"TEST\" for _ in range(len(new_input))],\n",
        "                s=100)\n",
        "plt.xlabel(\"var1\")\n",
        "plt.ylabel(\"var2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pKm-dJMH6UPh",
        "outputId": "2ba465ca-eb1a-4b26-e6af-25b201a42ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSe3GJRKBVfW"
      },
      "source": [
        "Well, could be, but _totally_ not! :-)\n",
        "\n",
        "Let's see them in detail!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP_GF8KUtPC6"
      },
      "source": [
        "# Perceptron model \n",
        "\n",
        "## \"The Psychologists\"\n",
        "\n",
        "- Original motivation of earliest neural model, perceptrons, from \"electronic\" modeling of perception\n",
        "- Influence of Psychology still visible in AI: visual processing, acoustic processing, natural language processing\n",
        "\n",
        "### The Hero: \n",
        "\n",
        "The psychologist Frank Rosenblatt and the Mark I perceptron:\n",
        "\n",
        "<a href=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRozuUtQVt1EyFVVfovXp5tC9iP3f5mM7tMy3jAlVaarA7gf_zE\"><img src=\"https://drive.google.com/uc?export=view&id=1py-fIFQIj1Hp9sutMJWFoiyxU-3JJUBy\" width=600 heigth=600></a>\n",
        "\n",
        "\n",
        "### The hardware:\n",
        "<a href=\"https://s3.amazonaws.com/s3.timetoast.com/public/uploads/photos/7146113/Mark-I.jpeg?1477813660\"><img src=\"https://drive.google.com/uc?export=view&id=13uW_M5ouDSCtptLXqF92OzsoSAvqfGEo\" width=600 heigth=600></a>\n",
        "\n",
        "- Original perceptron models and their update mechanisms not aimed at digital computers but specialized analog hardware! \n",
        "\n",
        "(Hardware innovation - though not analog, but specialized - is kicking in again, see [here](https://www.engineering.com/Hardware/ArticleID/16753/The-Great-Debate-of-AI-Architecture.aspx).)\n",
        "\n",
        "\n",
        "### The learning algorithm:\n",
        "<a href=\"http://www.rutherfordjournal.org/images/TAHC_perceptron.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1K7BFvi9OCytrCYcNakp8PycEb7cVla-A\" width=600 heigth=600></a>\n",
        "\n",
        "Inspiration of \"learning rules\" from [\"Hebbian learning\"](https://en.wikipedia.org/wiki/Hebbian_theory):\n",
        "- Neural learning relying on local information only\n",
        "- Correlation patters of neuron firing strengthen the synaptic connections\n",
        "- Colloquially: \"What fires together, wires together\"\n",
        "- Rather vague learning rule not applicable in practice\n",
        "- Rather limited solutions until advent of backpropagation\n",
        "\n",
        "(Backprop also extreme, since it relies too heavily on a distant supervision signal - so there is maybe a way to have a \"semi-hebbian\" learning procedure - see \"synthetic gradients\" in a later lecture.) \n",
        "\n",
        "\n",
        "## Biological motivation (recapitulation)\n",
        "\n",
        "### Representation\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1tedMjIowYM8Y68C8fRrZJ2JRsFRQx1P5\"><img src=\"https://drive.google.com/uc?export=view&id=1nF5EfVhgia0Zzpm5woRlBJ-KXSthV9e9\"></a>\n",
        "\n",
        "### Thresholding\n",
        "\n",
        "- Neurons do not \"fire\" continuously\n",
        "- Their activation has to be modeled with some kind of a step function\n",
        "\n",
        "<a href=\"http://www.saedsayad.com/images/ANN_Unit_step.png\"><img src=\"https://drive.google.com/uc?export=view&id=1XDNopwwsVTCLnC4ioN8tv1xbfLYlF9-e\"></a>\n",
        "\n",
        "Simplest \"nonlinearity\" - later we will encounter a plethora of others\n",
        "\n",
        "<a href=\"http://slideplayer.com/slide/5214241/16/images/5/Perceptron:+Linear+threshold+unit.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1Da8YDVCuy4wDyrLTBEcCySn8qBdOs3Yx\"></a>\n",
        "\n",
        "**Attention**\n",
        "\n",
        "- $x_{1..n}$ are the input values, \"input activations\"\n",
        "- $x_{0}$ is also present! -- This is the  \"bias unit\", or \"bias term\"\n",
        "<a href=\"https://raw.githubusercontent.com/qingkaikong/blog/master/39_ANN_part2_step_by_step/figures/figure1_perceptron_structure.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1fGwrmYGp8D6mwI1ZgbVb5R33IqLS1P2z\"></a>\n",
        "- Shouldn't be confused with concept of \"bias\" met when discussing overfitting, although  semantics is similar: a general \"prejudice\" which determines the behavior of the system\n",
        "\n",
        "### \"Biological inspiration\"\n",
        "\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1D75C1LThbDYqs4gYUz5kC8cZJprQHBPB\"><img src=\"https://drive.google.com/uc?export=view&id=1EQa7ODziQoOVoLi1nZZ2yyJfOxBgrqr8\" width=55%></a>\n",
        "\n",
        "\n",
        "[source](https://www.facebook.com/photo.php?fbid=2063218160389423&set=gm.2075284019202050&type=3&theater)\n",
        "\n",
        "## Capable of modeling logical operations\n",
        "- Logic considered pinnacle of cognitive activities\n",
        "- \"It can learn, it models logic, what else would be needed?\"\n",
        "- Perceptron's problems with modeling certain logical functions had huge effect in history of AI. \n",
        "\n",
        "##  Artificial neuron -- mathematical model\n",
        "\n",
        "(the mathematical discussion follows mainly that of [Hal Daum√© III](http://ciml.info/dl/v0_99/ciml-v0_99-ch04.pdf))\n",
        "\n",
        "### Activation function\n",
        "\n",
        "with \n",
        "- $\\mathbf x = \\langle x_1,...,x_D \\rangle $ incoming activations, \n",
        "- $\\mathbf w = \\langle w_1,...,w_D \\rangle$ weights, and\n",
        "- $b$ bias\n",
        "\n",
        "the outgoing activation is\n",
        "\n",
        "$a(x_1,...,x_D) = \\sum_{d=1}^D w_d x_d +b$ where\n",
        "\n",
        "If $a(\\mathbf{x}) \\geq 0$ then the input is classified as a positive if $a(\\mathbf{x}) < 0$ then as a negative instance.\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL1GBtviBVfY"
      },
      "source": [
        "<a id=\"learning\"></a>\n",
        "# Learning\n",
        "\n",
        "## Inspiration: Hebbian learning\n",
        "\n",
        "[Hebbian learning rule](https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Hebbian_Learning)\n",
        "- One of the oldest learning rules\n",
        "- If there is a high correlation between the outputs of two neurons at the two ends of a synapse (\"they fire together\") then the strength of the synapse should be increased (\"what fires together wires together\"). \n",
        "\n",
        "$ w_{ij}[n+1]=w_{ij}[n]+\\eta x_{i}[n]x_{j}[n]$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $n$ is the current time step.\n",
        "- $x_{i},x_{j}$ are the activations of the two neurons\n",
        "- $w$ is the strength of the synapse (later: \"weight\")\n",
        "- $\\eta$ is the learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU335vTrBVfZ"
      },
      "source": [
        "## The perceptron algorithm\n",
        "\n",
        "1. for all $d\\in 1..D$: $w_d \\leftarrow 0$ (initialize weights)\n",
        "2. $b \\leftarrow 0$ (initialize bias) \n",
        "3. $\\mathit{EpochCount}$ times: for all $(\\mathbf{x}, y)$ training examples\n",
        "   - Calculate the $a= \\mathbf w \\mathbf x + b$ activation\n",
        "   - If $ya \\leq 0$ (wrong or 0 prediction):\n",
        "       - $\\mathbf{w} \\leftarrow \\mathbf{w} + y\\mathbf{x}$\n",
        "       - $b \\leftarrow b + y$\n",
        "       \n",
        "(In more complex formulations there is also a learning rate parameter (most frequently signified by $\\eta$). Using this parameter the learning step would be:\n",
        "\n",
        "- $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y\\mathbf{x}$\n",
        "- $b \\leftarrow b$ + $\\eta y$\n",
        "\n",
        "accordingly, the above simpler but perfectly functional(!) version uses a learning rate  $\\eta=1$.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixt483z_BVfa"
      },
      "source": [
        "### Why is this a useful update rule?\n",
        "\n",
        "Later we will see that the algorithm is guaranteed to a find a separator if the data set is separable. Regardless, it is simple to see that the update step changes the perceptron's output on the incorrectly classified example in the right direction, since the difference between the outputs before and after update:\n",
        "\n",
        "$$[(\\mathbf w + y\\mathbf x)\\mathbf x + b + y] - [\\mathbf w x + b] = y\\mathbf x^2 + y = y (\\mathbf x^2 + 1)$$\n",
        "\n",
        "that is, the output increases at least by 1 if the misclassified example was positive and decreases at least by 1 if it was negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fvz23WJtPC9"
      },
      "source": [
        "   \n",
        "##  Complexity contrast with other learning algorithms\n",
        "\n",
        "### Perceptron algorithm\n",
        "\n",
        "Update was possible simply by rotating potentiometers (in the case of binary vectors by a single step forward or backward).\n",
        "#### vs, for instance, Newton's method\n",
        "\n",
        "Other methods for reducing the error rate can be way more complex. For instance, Newton's method, which approximates the minimum of the error function by using Newton's method to find its critical point:\n",
        "\n",
        "<a href=\"https://www.researchgate.net/profile/Daniel_Marcsa2/publication/266091369/figure/fig5/AS:476476194725892@1490612185738/The-geometrical-construction-of-Newton-Raphson-method.png\"><img src=\"https://drive.google.com/uc?export=view&id=1CRdZS-0tQuE3SEo7Yn5oeeX6IJluShYx\" width=40%></a>\n",
        "\n",
        "In the (typical) multidimensional case,\n",
        "\n",
        "- this requires the computation of the Hessian matrix, which consists of all second order partial derivatives \n",
        "- moreover, the Hessian needs to be inverted\n",
        "\n",
        "## EpochCount and order of processing\n",
        "\n",
        "- If the order of training examples is wrong then the perceptron learns only from a few examples\n",
        "- The order is so significant that a random shuffle of the original training data typically results in a 20% faster convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "FQC_X3Zdk0gk"
      },
      "source": [
        "## Geometrical interpretation\n",
        "\n",
        "The **decision boundary** of a perceptron with $\\mathbf w$ weights and  $b$ bias is the set of possible inputs for which the activation is 0, that is, the set\n",
        "\n",
        "$$\\left\\{\\mathbf x : \\sum_{d=1}^D w_d x_d + b = 0\\right\\} = \\{\\mathbf x: \\mathbf w \\mathbf x + b = 0 \\}$$ \n",
        "\n",
        "If $b = 0$ then the decision boundary is $\\{\\mathbf x: \\mathbf w \\mathbf x = 0 \\}$, which is the set of vectors that are perpendicular to $\\mathbf  w$, therefore the boundary is a  *hyperplane* which is perpendicular to  $\\mathbf w$ and crosses the $\\mathbf 0$ vector.\n",
        "\n",
        "<a href=\"https://ds055uzetaobb.cloudfront.net/image_optimizer/947723b3ba09371025dac3dab038f6b79a9ea2d3.png\"><img src=\"https://drive.google.com/uc?export=view&id=1hC2kVGM-yuYOw5PNW7mH4GyPRIE32sMC\"  height=\"400\"></a>\n",
        "\n",
        "In addition, if $\\mathbf w$ is a unit vector (we can assume that, since the decision boundary is determined solely by its direction), then the $\\mathbf w \\mathbf x$ activation will simply be the _signed projection_ of $\\mathbf x$  to $\\mathbf w$. On one side of the hyper plane the projection will be positive while on the other side negative, so the plane _separates_ the inputs which are predicted to be positive and negative.\n",
        "\n",
        "<a href=\"http://www.cs.cornell.edu/courses/cs4780/2015fa/web/lecturenotes/images/perceptron/perceptron_img1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Bc0zOu1qD6c5Gey8hnjaO3gHRhQ2VRFc\"  height=\"400\" ></a>\n",
        "\n",
        "The role of the $b$ bias is to *move* to separator hyperplane in parallel with  $\\mathbf w$ by exactly  $-b$ units.\n",
        "\n",
        "## The hypercone of the good solutions\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1KIZ9QUaLL2SisNrzwegoa9WA5B2e31IL\"><img src=\"https://drive.google.com/uc?export=view&id=1HVtk-4g5BSegzt3AqFKMHrB8vwjFaA0t\" height=\"400\"></a>\n",
        "\n",
        "(Source: Hinton - Neural networks for machine learning)\n",
        "\n",
        "## Margin\n",
        "\n",
        "If the positive and negative examples of a $\\mathbf D$ data set are separated by the  hyperplane determined by a $\\mathbf w$ unit weight vector and $b$ bias then the minimum of the activations on $\\mathbf D$ is the separator's _margin_:\n",
        "\n",
        "$$\\mathrm{Margin}(\\mathbf D, \\mathbf w, b) = \\min_{(\\mathbf x, y) \\in \\mathbf D} y(\\mathbf w \\mathbf x + b)$$\n",
        "\n",
        "This is simply the minimum of the distances from the separator hyperplane. The margin of a data set can also be defined, this is the largest possible margin:\n",
        "\n",
        "$$\\mathrm{Margin}(\\mathbf D) = \\sup_{\\mathbf w, b}\\mathrm{Margin}(\\mathbf D, \\mathbf w, b)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "Fwee-aijbDhS"
      },
      "source": [
        "##  Convergence theorem\n",
        "Let's assume that for a $\\mathbf D$ data set in which $\\forall \\mathbf x_i: \\|\\mathbf x_i\\|\\leq 1$ there exists a $\\mathbf{w^*}$ optimal separator with the maximal $\\gamma$ margin, and the algorithm is performed with the a $\\mathbf w_0,...,\\mathbf w_i,\\dots$ update steps (for simplicity we assume that the bias is $0$ and the $\\mathbf w^*$ is chosen to be a unit vector -- none of these assumptions is essential for the result). In that case the algorithm finds a separator in a finite $k$ number of update steps, and, moreover, $k$ is guaranteed  to satisfy\n",
        "\n",
        "$$ k \\leq \\frac{1}{\\gamma^2}$$\n",
        "\n",
        "The key idea is to prove that the angle between  $\\mathbf {w}^*$ and $\\mathbf {w}_i$ decreases to the degree needed for the linear separation in a finite number of update steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJQ8iCqRtDkn"
      },
      "source": [
        "## Advantages and disadvantages of perceptron algorithm\n",
        "__Advantages__:\n",
        "- online --  processes one example at a time and possibly improves the model on the basis of this example. $\\Rightarrow$ Capable of continuously processing new incoming examples.\n",
        "- fast and simple\n",
        "- convergence theorem\n",
        "\n",
        "__Disadvantages__\n",
        "- in contrast to SVM, there is no guarantee that the resulting separator is optimal.\n",
        "- error-driven: it can change a 99.99% precision model because of a single error\n",
        "- convergence is guaranteed only when there does exist a separator! And that-- as we will see -- is not always the case..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28clyyLVtPDB"
      },
      "source": [
        "<a id=\"advancedperceptrons\"></a>\n",
        "# Advanced perceptrons\n",
        "\n",
        "##  The problem\n",
        "\n",
        "Later examples have too large influence on the learned weights -- a last update can change a weigth vector that worked well for all other examples (!).\n",
        "\n",
        "## Solutions\n",
        "\n",
        "### Voting\n",
        "\n",
        "The weights and bias are stored at every update, together with number of correct predictions since the last update. The learning process is unchanged, but in the prediction stage the system generates a prediction with all stored weights + bias values and the result is computed as the weighted sum of all of these predictions,  where the weights are the stored \"survival times\". Problem: Requires a huge amount of memory.\n",
        "\n",
        "### Averaged perceptron\n",
        "\n",
        "Similar, but more practical alternative: prediction is performed with the weighted average of the weights and biases that were generated during the learning process -- weights are, again, the \"survival times\".  In contrast to voting, here it is enough to maintain a rolling weighted average during the learning phase, so the additional memory consumption is negligible.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### example with just one weight to show that keeping rolling weighted averaging is enough:\n",
        "\n",
        "a = [1, -1, 2]      # weight of perceptrons\n",
        "b = [10, 5, 100]    # survival times (weights of the weights)\n",
        "\n",
        "c = [a[i]*b[i] for i in range(len(a))]  # pointwise multiplication\n",
        "wa1 = sum(c)/len(c)     # mean\n",
        "print(wa1, \"\\t= t(i) weighted average of weights\" )\n",
        "\n",
        "a2 = a + [3]        # a new perceptron weight\n",
        "b2 = b + [200]      # its survival time\n",
        "\n",
        "c2 = [a2[i]*b2[i] for i in range(len(a2))]\n",
        "wa2 = sum(c2)/len(c2)\n",
        "print(wa2, \"\\t= t(i+1) weighted average of weights using the stored weights and survival times\")\n",
        "\n",
        "wa2b = (wa1*(len(c2)-1) + (3*200))/len(c2)\n",
        "print(wa2b, \"\\t= t(i+1) weighted average of weights using previous weighted average of weights\")\n"
      ],
      "metadata": {
        "id": "arK8HpWxUIOz",
        "outputId": "4bea3918-6924-412a-dd9f-52718bc1a0e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68.33333333333333 \t= t(i) weighted average of weights\n",
            "201.25 \t= t(i+1) weighted average of weights using the stored weights and survival times\n",
            "201.25 \t= t(i+1) weighted average of weights using previous weighted average of weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDwOqt297dRR"
      },
      "source": [
        "# Limitations\n",
        "\n",
        "### Minsky & Papert (1969): \"Perceptrons\"\n",
        "\n",
        "<a href=\"http://slideplayer.com/slide/775779/3/images/41/Minsky+&+Papert+(1969).jpg\"><img src =\"https://drive.google.com/uc?export=view&id=15clKwCvInuqSkbhH_f5GYhVphbtTBPqK\" width=600 heigth=600></a>\n",
        "\n",
        "#### A very general perceptron definition\n",
        "multiple layers and nonlinearity are possible\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1ipkPyUFpS8bfTrXNIy6CqMOOAzm14Z9j\"><img src=\"https://drive.google.com/uc?export=view&id=1y9a4JaeJoTbf6v6IMNQx8e7c1D1mdHlv\" width=600 heigth=600></a>\n",
        "\n",
        "#### Criticism\n",
        "\n",
        "> Perceptrons have been widely publicized as \"pattern recognition\" or \"learning\" machines and as such have been discussed in a large number of books, journal articles, and voluminous \"reports.\" Most of this writing (some exceptions are mentioned in our bibliography) is without scientific value and we will not usually refer by name to the works we criticize. The sciences of computation and cybernetics began, and it seems quite rightly so, with a certain flourish of romanticism. They were laden with attractive and exciting new ideas which have already\n",
        "borne rich fruit. Heavy demands of rigor and caution could have held this development to a much slower pace; only the future could tell which directions were to be the best.\n",
        "\n",
        "##### \"The Seductive Powers of Perceptrons\"\n",
        "\n",
        "> Thus \"programming\" takes on a pleasingly homogeneous form. Moreover since \"programs\" are representable in a\n",
        "[multi]-dimensional space, they inherit a metric which makes it easy to imagine a kind of automatic programming which people have been tempted to call learning', by attaching feedback devices to the parameter controls they propose to \"program\" the machine by providing it with a sequence of input patterns and an \"error signal\" which will cause the coefficients to change in the right direction when the machine makes an inappropriate decision.\n",
        "\n",
        "#### Goal: precise theory + what can they be used for? \n",
        "- Perceptrons are \"massively parallel\" machines -- these architectures were not so well understood as the classic sequential ones.\n",
        "- Although they write about multilayer perceptrons too, their most important _theorems_ concern single layer linear perceptrons.\n",
        "- Focus: perceptrons as visual pattern recognizers (this was their main application area at the time)\n",
        "- Negative results: some predicates, e.g., parity, connectedness etc. cannot be represented by certain types of perceptrons.  Their most well known result of this type is the XOR operation:\n",
        " \n",
        "## XOR Problem\n",
        "- It was important to model logical operators\n",
        "- \"Feeding\" 0,1 input the output should be the corresponding operation result\n",
        "- A perceptron can model a logical operation only if it is linearly separable\n",
        "\n",
        "### The truth table of XOR\n",
        "\n",
        "<a href=\"https://www.dyclassroom.com/image/topic/logic-gate/xor-xnor/xor-table.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Sqe9kQz1vNImvBzlFtVEQpazd6FQt6Lg\" width=300 heigth=300></a>\n",
        "\n",
        "### Problem\n",
        "$x1$ and $x2$ represents $A$ and $B$\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1m59mOWDu7yShgMpSgw2yC8YDVzi1-fzs\"><img src=\"https://drive.google.com/uc?export=view&id=1Ci6_UtG_Lr2UnwQQv6r86382TnSfGTf5\" style=\"width: 80%;\"></a>\n",
        " \n",
        "### The XOR proof\n",
        "Let us assume, toward a contradiction, that a $w_1,w_2, b$ perceptron computes XOR. Then\n",
        "\n",
        "1. $w_1 + b > 0$\n",
        "2. $w_2 + b > 0$\n",
        "3. $w_1 + w_2 + b \\leq 0$\n",
        "4. $b \\leq 0 $\n",
        "\n",
        "But adding (1) and (2) we have: $w_1 + w_2 + 2b > 0$ \n",
        "\n",
        "And adding (3) and (4) we get: $w_1 + w_2 + 2b \\leq 0$, which is a contradiction.\n",
        "\n"
      ]
    }
  ]
}