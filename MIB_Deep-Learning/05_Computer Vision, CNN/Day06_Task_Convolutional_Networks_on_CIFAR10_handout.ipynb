{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSHhdguTHm0N"
      },
      "source": [
        "# Task: CIFAR-10 classification\n",
        "\n",
        "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "\n",
        "> \"consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        ">The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\"\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1wlfkvZgS0oBDwxKicWmYgtsJmP3IcMdj\">\n",
        "\n",
        "### Categories:\n",
        "\n",
        "- airplane \t\t\t\t\t\t\t\t\t\t\n",
        "- automobile \t\t\t\t\t\t\t\t\t\t\n",
        "- bird \t\t\t\t\t\t\t\t\t\t\n",
        "- cat \t\t\t\t\t\t\t\t\t\t\n",
        "- deer \t\t\t\t\t\t\t\t\t\t\n",
        "- dog \t\t\t\t\t\t\t\t\t\t\n",
        "- frog \t\t\t\t\t\t\t\t\t\t\n",
        "- horse \t\t\t\t\t\t\t\t\t\t\n",
        "- ship \t\t\t\t\t\t\t\t\t\t\n",
        "- truck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIX2ehpiHm0S"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:01.205778Z",
          "start_time": "2019-05-01T08:39:00.058811Z"
        },
        "collapsed": true,
        "id": "sTT_MyHltNm4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adadelta, Adam, SGD\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "# from tensorboardcolab import TensorBoardColab \n",
        "\n",
        "# Fix seeds for (hopefully) reproducible results\n",
        "from numpy.random import seed\n",
        "seed(14)\n",
        "tf.random.set_seed(19)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d9_yHMEtNnG"
      },
      "source": [
        "Download the data if necessary and load it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.264800Z",
          "start_time": "2019-05-01T08:39:01.207234Z"
        },
        "collapsed": true,
        "id": "jFmPQL0xM_4p"
      },
      "outputs": [],
      "source": [
        "train, test = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_images, train_labels = train\n",
        "\n",
        "valid_test_images, valid_test_labels = test\n",
        "\n",
        "train_images = train_images / 255.\n",
        "\n",
        "valid_test_images = valid_test_images / 255.\n",
        "\n",
        "valid_images = valid_test_images[:5000]\n",
        "valid_labels = valid_test_labels[:5000]\n",
        "test_images = valid_test_images[5000:]\n",
        "test_labels = valid_test_labels[5000:]\n",
        "\n",
        "print(train_images.shape, valid_images.shape, test_images.shape)\n",
        "print(train_labels.shape, valid_labels.shape, test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxyeFqibHm05"
      },
      "source": [
        "# Model\n",
        "\n",
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.268218Z",
          "start_time": "2019-05-01T08:39:36.266265Z"
        },
        "collapsed": true,
        "id": "S075_4a6tNna"
      },
      "outputs": [],
      "source": [
        "n_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.277888Z",
          "start_time": "2019-05-01T08:39:36.269933Z"
        },
        "collapsed": true,
        "id": "mFRBJvJC8BGe"
      },
      "outputs": [],
      "source": [
        "# TASK - Hyperparameters\n",
        "# Fill in the initial values!\n",
        "# Later, experiment!\n",
        "#############################\n",
        "\n",
        "\n",
        "# dropout - Something between 0.0 < dropout_rate < 1.0, think in \"tens of percentages\" as default\n",
        "# dropout rate for conv layers\n",
        "dropout_rate_1 = \n",
        "# dropout rate for fully connected layers\n",
        "dropout_rate_2 = \n",
        "\n",
        "# Choose an appropriate batch size for the training!\n",
        "batch_size = \n",
        "\n",
        "# Choose an appropriate number of epochs\n",
        "epoch_count = \n",
        "\n",
        "# These are the default parameters, you can experiment with learning rates, schedules, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACdo3SEitNnl"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.404889Z",
          "start_time": "2019-05-01T08:39:36.282271Z"
        },
        "collapsed": true,
        "id": "o8OgyCFaHm1H"
      },
      "outputs": [],
      "source": [
        "# adapted from https://github.com/jtopor/CUNY-MSDA-661/blob/master/CIFAR-CNN/TF-Layers-CIFAR-GITHUB-v3.py\n",
        "\n",
        "\n",
        "tf.compat.v1.reset_default_graph() # It's good practice to clean and reset everything\n",
        "clear_session()          # even using Keras\n",
        "\n",
        "\n",
        "# WE USE FUNCTIONAL API!\n",
        "# (Could be different, but not now...)\n",
        "\n",
        "\n",
        "\n",
        "# Model\n",
        "#######\n",
        "\n",
        "# Define the input!\n",
        "# Remember, we have pictures with 32x32 pixels and 3 color channels\n",
        "# Disregard batch size, Keras will do that for us.\n",
        "x = \n",
        "\n",
        "# Convolutional Layer #1: (batch_size, 32, 32, 3) -> (batch_size, 32, 32, 64)\n",
        "# Define a \"normal\" convolutional layer for images (not a single sequence, so ?D)\n",
        "# There should be 64 convolutional units\n",
        "# The kernel should be 5 in width and heigth\n",
        "# There should be padding so that the input and output dimensions would be equivalent\n",
        "# The non-linearity should be ReLU\n",
        "conv1 = \n",
        " \n",
        "# Pooling Layer #1: (batch_size, 32, 32, 64) -> (batch_size, 16, 16, 64)\n",
        "# Define a maximum based pooling layer with appropriate dimensions\n",
        "# The pooling size should be 2,2 and stride 2\n",
        "pool1 = \n",
        "\n",
        "# Define a dropout layer with using the first dropout rate parameter\n",
        "dropout1 = \n",
        "\n",
        "# Convolutional Layer #2: (batch_size, 16, 16, 64) -> (batch_size, 16, 16, 64)\n",
        "# Repeat the prior conv layer\n",
        "# Watch for the right input\n",
        "conv2 = \n",
        "  \n",
        "# Pooling Layer #2: (batch_size, 16, 16, 64) -> (batch_size, 8, 8, 64)\n",
        "# Repeat the prior pooling layer\n",
        "# Watch for the right input\n",
        "pool2 = \n",
        "\n",
        "# Define a dropout layer with using the FIRST dropout rate parameter\n",
        "dropout2 = \n",
        "\n",
        "# Convert tensors into vectors: (batch_size, 8, 8, 64) -> (batch_size, 4096)\n",
        "# Use a single KERAS function, NO numpy or reshape magic!\n",
        "# Hint: the result is not 2D but \"flat\"\n",
        "pool2_flat = \n",
        "\n",
        "# Fully connected Layer #1: (batch_size, 4096)-> (batch_size, 512)\n",
        "# Define a fully connected layer with 512 nodes and ReLU\n",
        "dense1 = \n",
        "\n",
        "# Define a dropout layer with using the SECOND dropout rate parameter\n",
        "dropout3 = \n",
        "\n",
        "# Dense Layer #1: (batch_size, 512)-> (batch_size, 256)\n",
        "# Define a fully connected layer with 256 nodes and ReLU\n",
        "dense2 = \n",
        "\n",
        "# Define a dropout layer with using the SECOND dropout rate parameter\n",
        "dropout4 = \n",
        "\n",
        "# Logits layer: (batch_size, 256) -> (batch_size, 10)\n",
        "# Define a fully connected layer with ??? nodes\n",
        "# Think about it, what shape should the output be?\n",
        "# What activation?\n",
        "# Think about it: we are in a classification problem!\n",
        "predictions = \n",
        "\n",
        "# Full model\n",
        "# Instantiate (initialize) the model with inputs and outputs\n",
        "model = \n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGKQ-8bmtNn1"
      },
      "source": [
        "## Loss, optimization and compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.473837Z",
          "start_time": "2019-05-01T08:39:36.407595Z"
        },
        "collapsed": true,
        "id": "j1UmvSzLHm1R"
      },
      "outputs": [],
      "source": [
        "# Loss \n",
        "\n",
        "loss = sparse_categorical_crossentropy # we use this cross entropy variant as the input is not \n",
        "                                       # one-hot encoded\n",
        "\n",
        "# Optimizer\n",
        "# Choose an optimizer - adaptive ones work well here\n",
        "optimizer = \n",
        " \n",
        "# Compilation\n",
        "#############\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkPe16fDtNoJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:51:59.029061Z",
          "start_time": "2019-05-01T08:39:36.475186Z"
        },
        "collapsed": true,
        "id": "zZ1d14lFtNoM"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = model.fit(x=train_images, y=train_labels,\n",
        "                    validation_data=(valid_images, valid_labels),\n",
        "                    epochs=epoch_count,\n",
        "                    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:56:28.958057Z",
          "start_time": "2019-05-01T08:56:28.713031Z"
        },
        "collapsed": true,
        "id": "6GTGeChttNoi"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def display_history(history):\n",
        "    \"\"\"Summarize history for accuracy and loss.\n",
        "    \"\"\"\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "display_history(history);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:52:07.228831Z",
          "start_time": "2019-05-01T08:52:07.222761Z"
        },
        "collapsed": true,
        "id": "ixlyf1Fg8BGl"
      },
      "outputs": [],
      "source": [
        "assert max(history.history['val_accuracy']) > 0.75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7nD81aQtNo3"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:52:10.803767Z",
          "start_time": "2019-05-01T08:52:10.663477Z"
        },
        "collapsed": true,
        "id": "XcFR3MkStNo5"
      },
      "outputs": [],
      "source": [
        "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}