{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"motivation\"></a>\n",
        "# Motivation: Problems of computer vision \n",
        "\n",
        "Computer vision (since the earliest perceptrons) has been traditionally a crucial field of AI research.\n",
        "One of the reasons is that the perceptual \"circuitry\" of vision in humans got considerable attention in many fields (like cognitive science) and is directly relevant for the \"biological inspiration\" of AI methods.\n",
        "On the other hand it is a challenging field, model performance was way lower than human baseline till most recently (think 2014-15), though it is a common ability of many animals.\n",
        "Finally the field is interesting, since we have a strong understanding of physical processes forming it's baseline and would \"expect it to be simple\". It is not."
      ],
      "metadata": {
        "id": "Pupc4iyGEt3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invariances in the field of vision\n",
        "\n",
        "\"Pictures come from objects, not random pixels\"\n",
        "\n",
        "<a href=\"https://cms-assets.tutsplus.com/uploads/users/108/posts/19997/image/color-fundamentals-value-1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1dnawrdlFRXc-NF5pU0VdkEuYZ9wJKouv\" width=400 heigth=400></a>\n",
        "\n",
        "The basic mechanism of light reflection and absorption are responsible for the detected \"pixel distribution\" on a picture, so we can assume that the empirically observed distribution is coming from a directly not observed, but very well defined \"generative distribution\" of physical objects. While \"practicing\" vision, we reconstruct the domain of objects from the visible data of pixels. Our expectation would be the same for AI systems. \n",
        "\n",
        "<a href=\"http://www.mstworkbooks.co.za/natural-sciences/gr8/images/gr8ec04-gd-0052.png\"><img src=\"https://drive.google.com/uc?export=view&id=1S-KuCXHzkrJ3c1nxVEhcHvebdKZ9CnlB\" width=400 heigth=400></a>\n",
        "\n",
        "Colour vision, especially how the abstract categories of \"colour\" arise from the raw perceptions is non-trivial. (We perceive the same object as \"red\", even though the lighting conditions influence the perception strongly, and we categorise different things as pink or red according to culture, or even see things differently [based on our culture](https://en.wikipedia.org/wiki/Linguistic_relativity_and_the_color_naming_debate). More [here](https://books.google.hu/books?id=O9SIAgAAQBAJ&dq=Color+vision:+A+case+study+in+the+Foundations+of+Cognitive+Science))\n",
        "\n",
        "From these it follows, that there are many \"invariances\" in the visual field, like rotational, translation, lighting,... which the model should handle well. \n",
        "\n",
        "Another \"adjacent\" field in computer science concerns itself with the realistic generation of pictures (or sequences, as animations) from a known set of objects. This is how computer graphics and special effects are made with the inverse procedure, \"ray tracing\".\n",
        "<a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Ray_trace_diagram.svg/600px-Ray_trace_diagram.svg.png\"><img src=\"https://drive.google.com/uc?export=view&id=1u1xs_R3i_W8wNBb82yoyJcDUY4r6xJq9\" width=400 heigth=400></a>\n",
        "\n",
        "It is important to note that the development of Graphics Processing Units was a direct result of the need for better computer graphics, thus it is by no chance, that the first applications of GPUs to deep learning came from the field of image recognition. The hardware was already somewhat specialized for the task. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0UWGCf_MEy_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compositionality\n",
        "\n",
        "\"Objects are made of parts, not just blobs\"\n",
        "\n",
        "<a href=\"https://kevinkaixu.net/projects/scores/teaser.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1hCBcAw0SgIJ4R12M3HqyBPOkKZPzCrKv\" width=600 heigth=600></a>\n",
        "\n",
        "It is also crucially important that \"objects\" themselves (though sometimes vaguely defined in themselves) are results of a compositional structure, that is they many times are composed of \"parts\" (themselves objects). This presupposes that a good model can handle a hierarchic composition of objects.\n",
        "\n",
        "(The recent advances of machine learning made a \"full circle\", and deep learning methods are aiding the composition of computer graphics, like in: [\"SCORES: Shape Composition with Recursive Substructure Priors\"](https://kevinkaixu.net/projects/scores.html).)\n",
        "\n",
        "Further complicating the problem, objects in the visual field can be described by edges, shapes, textures, so their description is non-trivial.\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1zgVDLXoJCPTIS1JAwbwlGR5MnxDsvNBt\"><img src=\"https://drive.google.com/uc?export=view&id=1ex4Bqp1TxtbiqEQ59bnKn16yMewSpa7D\"  width=600 heigth=600></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "vpMWF13ME2Fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why traditional methods suffered?\n",
        "\n",
        "\"Could you please build up a list of cat parts with visual specification, please?\"\n",
        "\n",
        "Image input is notoriously high in dimensionality of measurements, though we annoyingly know that is is very well compressable data, just we don't know how.\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1VJZaGkHUVAZXpS-vRMRgEmzIQPia5JWV\"><img src=\"https://drive.google.com/uc?export=view&id=1A_Tzd1mMKNMy2YwdKoRFmtiIPbrWzeQ5\"  width=600 heigth=600></a>\n",
        "\n",
        "\n",
        "I can describe a scene as \"there are two cardinal red spheres of 1cm diameter in an empty white space\", and I have described the scene with a very good approximation, though it can have literally myriad instantiations under viewing angles, lighting conditions,...\n",
        "\n",
        "<a href=\"https://cdn8.bigcommerce.com/s-e2p82/images/stencil/500x659/products/2992/7729/Q010145__80365.1458131489.jpg?c=2\"><img src=\"https://drive.google.com/uc?export=view&id=1oQjfhv2ifX42zraCCWTllUYkGo3VPRzo\" width=400 height=400></a>\n",
        "\n",
        "As stated before: Compressability and \"understandability\", \"discernability\" has it's deep connections. "
      ],
      "metadata": {
        "id": "Lc7bZIDCE5An"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPY08-rco59K"
      },
      "source": [
        "\n",
        "## Crucial importance of end-to-end learning\n",
        "\n",
        "In the visual field it is all the more obvious that the historical progress did not come from the better or stronger classifiers - an SVM will do fine - but the elaboration of features, that is, the representation of the input, is essential. Years upon years of human engineering went into feature extractors (see some at [wikipedia](https://en.wikipedia.org/wiki/Feature_detection_(computer_vision)#Feature_extraction) though there are many many more), and the big breakthrough in 2012 for deep learning came from the fact, that a neural net with no engineered features, working on raw pixels (though with a clever architecture) significantly beat the state of the art.\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1v8yieDdObp74czCbGMg1J50xPYL20XhZ\"><img src=\"https://drive.google.com/uc?export=view&id=1D82DR5IvGIW0tGESnFCFcGuJvrPpx54D\"  width=600 heigth=600></a>\n",
        "\n",
        "[source](https://www.youtube.com/watch?v=o8otywnWwKc&t=301s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aside: why not use feedforward NN?"
      ],
      "metadata": {
        "id": "wfv_shH5cG59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "hKuVMTbJcKrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## an MNIST example\n",
        "if True:\n",
        "  from keras.datasets import mnist\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  print(\"Image shape:\", x_train[0].shape)\n",
        "  \n",
        "  i = 4\n",
        "  img = x_train[i]\n",
        "  lab = str(y_train[i])\n",
        "\n",
        "  def show_grayscale_img(img, lab=None):\n",
        "    plt.imshow(255 - img, cmap=plt.get_cmap('gray'), vmin=0, vmax=255)\n",
        "    if lab:\n",
        "      plt.title(lab)\n",
        "    plt.show()\n",
        "\n",
        "  show_grayscale_img(img, lab=lab)\n",
        "\n",
        "  fl = img.flatten().reshape(1,-1)\n",
        "  fl = np.concatenate([fl]*10, axis=0)\n",
        "  plt.figure(figsize = (15,1))\n",
        "  show_grayscale_img(fl, lab=f\"{lab} flattened\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "YkwabhT8dwYK",
        "outputId": "fa7b6da3-7c5a-4135-cbb7-8967d4c7fe07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (28, 28)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data() \n",
        "\n",
        "label_str = [\"airplane\",\n",
        "\"automobile\",\n",
        "\"bird\",\n",
        "\"cat\",\n",
        "\"deer\",\n",
        "\"dog\",\n",
        "\"frog\",\n",
        "\"horse\",\n",
        "\"ship\",\n",
        "\"truck\",]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN8RR0bOcXeT",
        "outputId": "7d96c252-8c91-451b-92a5-bc619b8c3b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 664 ms, sys: 169 ms, total: 833 ms\n",
            "Wall time: 849 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flattening a greyscale image:\n",
        "\n",
        "<img src=\"https://peltarion.com/static/flatten_pa1.png\" width=400>"
      ],
      "metadata": {
        "id": "lismvko0f9t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 4\n",
        "img = x_train[i]\n",
        "lab = label_str[y_train[i][0]]\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "\n",
        "def show_grayscale_img(img, lab=None):\n",
        "  plt.imshow(img, cmap=plt.get_cmap('gray'), vmin=0, vmax=255)\n",
        "  if lab:\n",
        "    plt.title(lab)\n",
        "  plt.show()\n",
        "\n",
        "img_gray = rgb2gray(img)\n",
        "\n",
        "print(\"Image shape:\", img_gray.shape)\n",
        "\n",
        "show_grayscale_img(img_gray, lab=lab)\n",
        "\n",
        "fl = img_gray.flatten().reshape(1,-1)\n",
        "fl = np.concatenate([fl]*25, axis=0)\n",
        "print(\"\\nFlattened image shape:\", (fl.shape[1],))\n",
        "plt.figure(figsize = (15,1))\n",
        "show_grayscale_img(fl, lab=f\"{lab} flattened\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "A7tlJyoDcg6b",
        "outputId": "e3778a73-0d5a-46d8-94b1-ea5f665a19ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Flattened image shape: (1024,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flattening a colour image:\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSXi0jUu7PCM3cPGW086GyMVLtNQ7QvywUFkg&usqp=CAU\" width=700>"
      ],
      "metadata": {
        "id": "EhT89SCWfqjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Image shape:\", img.shape,)\n",
        "plt.imshow(img)\n",
        "plt.title(lab)\n",
        "plt.show()\n",
        "\n",
        "fl = img.flatten().reshape(1,-1)\n",
        "fl = np.concatenate([fl]*50, axis=0)\n",
        "print(\"\\nFlattened image shape:\", (fl.shape[1],))\n",
        "plt.figure(figsize = (15,1))\n",
        "plt.imshow(fl)\n",
        "plt.title(f\"{lab} flattened\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "R990hWA6ed5b",
        "outputId": "cf1eabfb-9418-406f-cd1e-1a8bef9c948d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de5DkV3Xfv6ef89zZnX3OPrQvZLkWg4S8JQshZITAJQguIZeNwRWZikmWpKxKqJBKySIJMuUk4AQwNgnUyghkjCWeKgmDY4TKLsVIEVq9Vqtdweqxq9XszOxrZufdz5M/ujc12tzvndl59Kx1v5+qqZm5p+/vd/p2n/5132+fc8zdIYR4/ZNZbgeEEK1BwS5EIijYhUgEBbsQiaBgFyIRFOxCJIKCXSwqZvb3ZvbPie0SMxs3s+xstxWLj4L9dcjFGkTu/oq7d7l7bbl9SREFuxCJoGC/iDGz28zsRTMbM7ODZnZzc/wOM/vLGbfbZmZuZjkz+88A3g7gi823zF9s3uYaM3vczM42f18zY/7fm9kfmdkjzTnfN7PVZvYNMxtt3n7bjNvTYzXZaWY/bc6938x6z/eT3N/fM7NDZjZsZn9rZlsXaSkFFOwXOy+iEbg9AP4QwF+aWV9sgrt/AsD/BnBr8y3zrc1g+wGAPwWwGsDnAPzAzFbPmPpBALcA2ARgJ4BHAXwVQC+AQwA+CQBzPNbvAvg9AH0Aqs3bRjGzmwDcDuA3AKxt3od7Zpsn5o6C/SLG3b/t7sfdve7u3wRwGMBV8zjUPwFw2N2/7u5Vd78HwPMAfn3Gbb7q7i+6+1kAfwPgRXf/sbtXAXwbwFsu4Fhfd/cD7j4B4D8C+MC5TbkI/xLAf3X3Q81z/hcAV+jqvngo2C9izOx3zexpMxsxsxEAvwRgzTwOtRHA0fPGjqJxFT/H0Iy/pwL/d13AsY6dZ8tjdr+3AvjCjPt6BoCdd1yxABTsFynNK9qdAG4FsNrdVwI4gEYATADomHHzDedNPz+V8TgawTSTSwD0z8O1uRxry3m2CoBTsxz3GICPuvvKGT/t7v7IPHwUARTsFy+daATtSQAws3+GxpUdAJ4GcF1Tt+4B8AfnzR0CsGPG/z8E8Atm9jvNTbzfBrALwF/Pw6+5HOufmtkuM+sA8CkA35mD3PZlAH9gZm8EADPrMbPfmod/gqBgv0hx94MAPovGRtkQgDcB+EnT9iCAbwLYD+AJ/P9B+wUAv9nc1f5Tdz8N4H0APg7gNIB/D+B97j7b1Tbk11yO9XUAXwMwCKANwL+ew3HvA/AZAPea2Sga72Lec6H+CY6peIUQaaAruxCJoGAXIhEU7EIkgoJdiEQIfkd5qVjRs9LXrmPf9uQbhWbh16RMxugcj7yOxbYkDfyYRibyGbOczWL+z+uIMLrhGjlX5IDR7dv4Hb/wky0Bi322uPvzOxubFT9V2Hr6xHGMjQ4HH5kFBbuZ3YiGzJMF8Ofu/unY7deu68On/+SuoK1er9N57cVicLzQ1kbn1LPhOQBQdf5CkAP/VmeWKMV57nr02eE57keFvbIg/iTI1IjV83ROtcKPWMtE5PF5BHtM/YkqQ5Fz1esR/8nE6ItpxI/Y87RWm1/mLjtbNbpWYT8+9W8/QOfM+21887vO/wMNLXQXgA+Z2a75Hk8IsbQs5DP7VQBecPeX3L0M4F4ANy2OW0KIxWYhwb4Jr014eBWBpAUz22Nm+8xs3+jZ4QWcTgixEJZ8N97d97r7bnffvaJn1VKfTghBWEiw9+O12U2bMb8sKiFEC1jIbvzjAC41s+1oBPkHAfzObJPqZFc1V+S7xeV6eJdz4uwYnZPv5Nu32Xw7tcH5vDrZ2a1Gds5r0xVqmz47RW2FNq4m1MB3hMenxoPjGePH6+rsoTaPnKse2X02IivOdxc8ssTR3Xj2mMU2/mM77jEfY7vxbD0AoE5WpT5PVYAx72B396qZ3Qrgb9GQ3u5y9+fmezwhxNKyIJ3d3X+IRn6zEOIiR1+XFSIRFOxCJIKCXYhEULALkQgtzXqr1WsYnQhLQ5UKl6hOnTwdHH+1/wSdk23rpLaubv7lnmKGS1RMlStXue/1SpXaJsfCawEA7XnuBzJcdhkrh+XIcplLPzu2X0ptb9jJy7a3xxKRiDQUlYwiyS4eMdZjuhzLC5pvQs48iUlvGXLf6hHZcz7oyi5EIijYhUgEBbsQiaBgFyIRFOxCJEJLd+PHJybwyP95lNj4znQG4SSZqRLfNZ2uhXfwASBf4LZsnb/+1ciG6rTzHfdaZKe4s8B3s9vDLcwBAG1FXjqrlikHxycmuGKwb/9T1Hbi1HFq27F9O7WtWRPu49je0REcBwCPlZeKJJnUSYkmADD2eLa6Fl4suYYlDc0jESY2R1d2IRJBwS5EIijYhUgEBbsQiaBgFyIRFOxCJEJrE2FqdYyMh+uueaT2m5FshlyB163riEhX2Qy3FVCgtmmE5Z9q5DVzbHKC2qYmuK1oXF7rcp4kkyV3LV/kdfemx6ep7cVjvIbo0YFBalu5IlzXbsvmzXTO2jWr+fFW8eSlXCbSxYfIcvNNdmENdwBe726287HuLvEadBfuv67sQiSCgl2IRFCwC5EICnYhEkHBLkQiKNiFSISWSm91d0yVwzJDPh9zhWQF1Xgml4PbLBtp0xNRNMqVsERVibje3dFFbWOjk9Q2WuatoUqRDKpCISwddhf4Hctmudw4US3xeZEMwdKps8HxkRGe3djZxeXBvr6N1LZz+w5q6yqEZcoiWScgXg+xEikL5+ASYCwzj8lyMXWQSYCxWn0LCnYzOwJgDEANQNXddy/keEKIpWMxruzXu/upRTiOEGIJ0Wd2IRJhocHuAH5kZk+Y2Z7QDcxsj5ntM7N95Wn+OVQIsbQs9G38te7eb2brADxoZs+7+8Mzb+DuewHsBYCeNRtaWwtICPH/WNCV3d37m79PALgPwFWL4ZQQYvGZ95XdzDoBZNx9rPn3rwH4VGxO3R1TpbB8Varw1x3WOqct0n4o9hYikmAXbSXEbBORYplt7fxkxXykcGSFz5su8Y9DVSNZXpH7VYhkjcUvB/yYuVz4mDE/xib5Op49fIjaTp3m+8PdbeHsu82bePbdqkiGXSGSPRjrX1Wv8qKkVaLKxbIpax6Wj5dKelsP4L5mIOYA/JW7/68FHE8IsYTMO9jd/SUAly+iL0KIJUTSmxCJoGAXIhEU7EIkgoJdiERoadabu6NMsn+sxrOCWF+reiaiocUoRgoDZvnrXz0Tlk9ykVWsRLLXCjkuHXa186ysyTIvEFlF2MdIWzyUqtxYjBTnzEayvJxcRyr1iARFCnoCQCbDH5fBMyeo7Xgp3NfvhaOv0Dlr14b71AHAxo1bqK2rq5va2ooRmZhInxWPSG+k910tUohSV3YhEkHBLkQiKNiFSAQFuxCJoGAXIhFauxsPoBqpxcWokR3c6fExOicX2SKvRTbxc5kytbEEmnw+lnwQWeJILblYMbyuSNurKnn5jpSLQyXiR7XG1yNj/KBOsjtqkR33WjZWdI2bYrXazMJrVY0Ukxs9PkxtRweOUFuxwHfcOzo6qI0ldMXq5OXz4ftVLvG6hrqyC5EICnYhEkHBLkQiKNiFSAQFuxCJoGAXIhFanghTqoSlHFZnDgDq5Mv9rG0OAFQjddqmIvJEPiJrZYnUVMzxOU5qwgGAeaRdUEQO8zrXoVgexGSNJ6CUwc+VidSnK0ceszzRKT3Dz1XJ8PsVk9cy2UgNPQsnDUXyaqL1C+sRDbM8xWvojU5EtEMmb5b48Vi8TE2O0jm6sguRCAp2IRJBwS5EIijYhUgEBbsQiaBgFyIRWiq91et1TE6HpZBcTAupEzcj8tTUxBC1FQpcXOldz9sCtRP1JBORtbKRWnKeqVDb2eFw7TQAmBrn8srW7ZcFx8cqnXTO8PBZaisWebZWhcioAGAkTa0e09D4Mkbn1SKHLCC8xplspBZepPVWLZY+GMsCLE1QW33kWHD8dP9L/FykPl0lIv/NemU3s7vM7ISZHZgx1mtmD5rZ4eZv3hxLCHFRMJe38V8DcON5Y7cBeMjdLwXwUPN/IcRFzKzB3uy3fua84ZsA3N38+24A719kv4QQi8x8P7Ovd/eB5t+DaHR0DWJmewDsAYBcG//cKIRYWha8G++NL6jTLRJ33+vuu919d7ZQXOjphBDzZL7BPmRmfQDQ/M1bcgghLgrm+zb+AQAfBvDp5u/75zLJ4ahVieQRkU9WFduD4ys6uSw01RG5a8Ylo/w4z5ZrI9Uc161bR+dMt/MihOUql97a2/h9y3aE1wMAOlasCI6v7OyjczasKVFbLPtuOiKHTZJ5gye5JFqZGKG2vPO1ylV5O6xsPfxYVyqRYqVZvvZ18MezHmmVhSl+vtHjR4LjpWG+VuPj4cesSgp9AnOT3u4B8CiAy8zsVTP7CBpB/m4zOwzgXc3/hRAXMbNe2d39Q8R0wyL7IoRYQvR1WSESQcEuRCIo2IVIBAW7EInQ0qw3uAPVsBTS09FNp60kMlr/wCt0zlTkCzylSJaaDR6ltu2rwxLbui2b6Jznjx+nNq/z7KqOCS4B9nRy+efZY88Ex7s28KyrriIvmPnyzw9SW62T5z+tvPTN4XNtfAOdM3H0ELVlI5l+K5xnek2Oh+W8yTH+1ZBCvovaRqd5ccv2lWupbXU7f6zHSWYeIj0JjWWJRgqc6souRCIo2IVIBAW7EImgYBciERTsQiSCgl2IRGi59JaphWWGDV1c7hgaDssklW6uTeS6uZSXMS6fVCvD1Lb1yjcGx4cjvdLKqyLZa8aXP7OCy2sjozyDamw6LNnVJ3lGWWmaS5E9ET+OjXPJa+JkuGDm1pUr6ZyNl4XlOgAYOcgz2yb6uVw6PBS2jU7wgp41kt0IAGen+HOufRWX3rq3cFuV9GebnuLZiKwHn0X0Ol3ZhUgEBbsQiaBgFyIRFOxCJIKCXYhEaOlufC6bRe+K8C75mi6+ez5yJlyLq7eNJ3AU83xXslrhu8/rdobbJwHAjr4twfHnXuFtelYWefunaqR90roNfNc6s4YrFxO58Ot3ppv7MXxykNq2ruPtsCYL3P/hWjjx5szwSTon03cJtW3edTW19b/6PLVNT00Gx/NZ/vzwSD+pbJ3XwiuN8OSak+AKSnUy7GMmy6/FNdKKLIau7EIkgoJdiERQsAuRCAp2IRJBwS5EIijYhUiElkpvhXwWWzf0Bm2/8Z530nlHX9oWHB+b5okYpWkuC1VLXHrbtpHLP14PSzK+ZgOdczYir01Mcv83r+EtparOE2/GJ8IJI97Ga/J1Oa8ll61zjWd9D29DNXEiLLGN94dlJgColPj96lzPJcCNb3w7tdUrZ4PjJ46/SOdMjnOZDJH1WNHJE6xy4DUFnURhZZKfy0nCi0dacs2l/dNdZnbCzA7MGLvDzPrN7Onmz3tnO44QYnmZy9v4rwG4MTD+eXe/ovnzw8V1Swix2Mwa7O7+MIAzLfBFCLGELGSD7lYz2998m08/9JnZHjPbZ2b7SqSwghBi6ZlvsH8JwE4AVwAYAPBZdkN33+vuu919d7GNb+gIIZaWeQW7uw+5e83d6wDuBHDV4rolhFhs5iW9mVmfuw80/70ZwIHY7c+RNceKbFgaeuuVXPK66o3h9kpjk7xGV8X561ilyuWJ6iT/qDE1HT7f9jJv/zRZ4vLJeKTFUz7PH5rhUd4KqW17OLttqsTXyleuobb+wQFqO/wyb7+1a1VYOnzlZGT7p86lq1obz4rs2noltb1957bg+JljXHr72ZNPUNuJwZ9RW6fx+oUo8fZb0zVST67OpchcPjynTGo8AnMIdjO7B8A7AKwxs1cBfBLAO8zsCgAO4AiAj852HCHE8jJrsLv7hwLDX1kCX4QQS4i+LitEIijYhUgEBbsQiaBgFyIRWpr1Vq9WMX4mLE+8+jJX7zZv2h4c39S3ns7JdXCpph5puzR66hS1jYyEfV/du5rOmZjiUsjkVCQjbpxLNWPjPdR22c4d4eNNRKSfKS4Brm3n2XL5Er9vv/wr1wTHz0zyOUcGwxlqAFDO8DZUtSneGgqkJdPGN4efUwCw9s3vprbqcLj4KQCcOfQYtb184HFqO/Xiz4PjmQJ/zDK5sCxnkWKqurILkQgKdiESQcEuRCIo2IVIBAW7EImgYBciEVoqvWUzWaxs7wzaxk7zfmMDJPtnzQber6sny+9aZzfvo4YeLtllLSwbdUfS9HsiPew8M78+cIcO8t5ma9eGpaaODp5VOBmR+S7fxjP6fnU3zzabIpmFk1wZwqVbeIbg0GkuDx4f5Jl0gy8fC46/EunnNh2RbdtX8sKXK38pVL2twRWXvZXaNr28Pzi+/xFe7e3k4MvBcTde0FNXdiESQcEuRCIo2IVIBAW7EImgYBciEVq6G5/PZtHXG07isDJPkDgzdCI4/sz+F+icpw7wWmHrN22htrf/6nXUtmlt2PfpYb4Dms1Ftuoju/G5HH9oLtnI2zW1t+WD48UCf11fUeigNnRzHys17scYSQCaqnEF5dDhI9Q2XAq3kwKAK3eEFQgAGF8XXseXB7j6c+goVzueeYk/58aKXOVZs4Kv8a71YcVj93U8IeepRx8Mjh99IZI8Qy1CiNcVCnYhEkHBLkQiKNiFSAQFuxCJoGAXIhHMnScEAICZbQHwFwDWo9EBZq+7f8HMegF8E8A2NLrCfMDdI/1vgFXdXf6O3W8K2t50SbhdEAD0rA5LK088xyWS5yMyztuuv4HaquDr8es3XBscX9XG57S186SKXJ7LMVPTXM5bu5qvVUcxnGhUjrR/imHZSButyLXC8uGacYePvkrn/PF/+zy1nTrBk11+5erw4wIA7/utW4LjXuJ16w48/lNqO17l0uFzI7xdUz3La/n51Ehw/NJITPQffjI4/shDD+DsmVNBJ+dyZa8C+Li77wJwNYDfN7NdAG4D8JC7Xwrgoeb/QoiLlFmD3d0H3P3J5t9jAA4B2ATgJgB3N292N4D3L5WTQoiFc0Gf2c1sG4C3AHgMwPoZnVwH0XibL4S4SJlzsJtZF4DvAviYu7+mZ7A3PvgHP7ia2R4z22dm+0oV/pVYIcTSMqdgN7M8GoH+DXf/XnN4yMz6mvY+AMEvsLv7Xnff7e67i/nw97aFEEvPrMFuZoZGi+ZD7v65GaYHAHy4+feHAdy/+O4JIRaLuWS9vQ3ALQCeNbOnm2O3A/g0gG+Z2UcAHAXwgdkOVKnVcXIkLCk9n+dZTdkTp4PjrwwMBMcB4Lob3kFtt/+HT1Dbn33xf1LbD77/QHD8Fzfx9k/5QpbaOrtXUFutxuux9fb0Utva3vDWSSyLrlDgmW2ZSKus8RovKFfOha8jX/ryV+mcg88/S23FPPfxvge+TW2bLyNS76W/QOe0F3mrqRXO7/PGLmpClawHAEyQTEAvc7l066ZwTcF9kXWaNdjd/R8AMHGRC9ZCiIsKfYNOiERQsAuRCAp2IRJBwS5EIijYhUiElhacLBSL2LTtDUFbDWN0XqUSzlAqdHKto28Lb1vkxrPUtmzk7X1+fP93g+Njg7zwYkc7z3YqtkeKUVIBBCjm+JeTujrCa9LRzjPsChG5pq3AffQ2ft9OToUfz+cOHaRz3vUuLu5cfsXl1Hbnn3M579GH/yY4vmMDLw5Z6OBy6alBXqjymcM/p7Z8J1/H9SvCvtSmuPzaTgqI8meNruxCJIOCXYhEULALkQgKdiESQcEuRCIo2IVIhJZKbw5HFWE5oVbnclihGJaNOnnSGEbHecHGoRM8w+7UGV4z89XBcPadV3lRjrYil1wqFS6txMqAFvP8YesshmW5bI7LSe1tPMurrY1LdvUsF3peOTkUNjif8/6bb6a2a665htqOHeNFLO974PvB8aee2Urn1KbL1DY8dJbayqf7qS1X44VHJ6vjwfGXho/ROR3FsFxaKk3RObqyC5EICnYhEkHBLkQiKNiFSAQFuxCJ0NLd+Gq1hlMj4R3tSpW348llwq9JXuW72U/tP0Btb7r8lyPzeB001u6onOM77uUK3wUfGDhFbdOR9kSFSD25PDldLEEiX+CJNfnIzn/Nebuj8enwrnDvGt5eYM1qXstvbHSU2jb0baC2M8Nh5eVHP/ohnTM9PkFtp0+Hd84BYML4tTMXSYjKEoVi1fpw2zMAWLc+fJ+rkdqFurILkQgKdiESQcEuRCIo2IVIBAW7EImgYBciEWaV3sxsC4C/QKMlswPY6+5fMLM7APwLAOe0jdvdnesZaNR+q1lYrrEsr4M2PhlOapka5zLI4MmwxAcAf/JnX6S2oy8c5X6Uw7LGC/08scYjCT6xFk+VGpe1rMbbAmXJ67dFxDeL1Dpz4+2OYnIePHy/2zu576dP88esGGlRNXqWy3KlUtj/I0d48oxFJN0Kf1jgkaShWGITqwHYWeQ1Ficnwj7WI8+3uejsVQAfd/cnzawbwBNm9mDT9nl3/+9zOIYQYpmZS6+3AQADzb/HzOwQAF66VQhxUXJBn9nNbBuAtwB4rDl0q5ntN7O7zIzXUxZCLDtzDnYz6wLwXQAfc/dRAF8CsBPAFWhc+T9L5u0xs31mtq9a5kUehBBLy5yC3czyaAT6N9z9ewDg7kPuXnP3OoA7AVwVmuvue919t7vvzkW+gy2EWFpmDXYzMwBfAXDI3T83Y7xvxs1uBsAzT4QQy85cduPfBuAWAM+a2dPNsdsBfMjMrkBDVTgC4KOzniyXQ+/qXmLl2WFTJAupFGn/lIlkII0Mj1Db6rXrqK2nN5yFVI3IHXXn9cyqFS5D1apc8orVrqtXwr7EZL5SiftYJxIaACCS9ZYh15GRSPbaTx75CbVdf/311PbcwUPUxu52OfKYZSPPxXrkeRWTS2ulyEfYctiXY0d5DbpsMVzTrhL5qDyX3fh/QFhSjWrqQoiLC32DTohEULALkQgKdiESQcEuRCIo2IVIBPOYtLLI9PT2+LU3XBu01SPZRKRjFLIRMSEXKcposbscyXhiGUWZLJdqqmXehqpe45JXLSLj1COLxR7OaoVLeeMTPHuwVOLyYKUS8Z+sY+x4He28cOe27dupbd8TT1LbyGi4cGcsCzAWE7WILdLZCrBojmCQTIY/r9o6whl20+MjqNWqwZPpyi5EIijYhUgEBbsQiaBgFyIRFOxCJIKCXYhEaGmvN4PBLCwn5PP8dceyRLaocTkjn4/kzscSuSISSZFJbJE5hcgKG9qoLSaV1WI6JZGGYvLg6jUsExGoRPzwSNYbkw7rdS5tTkxwmXJwaIjatm3jstzYRDgLbHIq3IuuAX+CVKOyXEQSjTxm7LHJkB6HDVv4OXdieozPoRYhxOsKBbsQiaBgFyIRFOxCJIKCXYhEULALkQgtld4cBvewzOD1SC8ykqEUSySKZYZFZbkcl6iMnDATcyRyvGxEWslHCiJWKryoIC0sGXEx1o8ua3ytqjUuyzGlLx+5z+3dK6lt0yW811usv9kU6c8XkxRjzx3Lcv9j2XKxY2bJYsWLhIazB8+eOUXn6MouRCIo2IVIBAW7EImgYBciERTsQiTCrLvxZtYG4GEAxebtv+PunzSz7QDuBbAawBMAbnGP9DpCY9e3PB3eYWQ73QDANkBjO7vR3c9YfbrI7rmTBIl6JHHCIu2CMpGd7nw7t3mW78YXI7vFnPnVY6vGWlSVw0+FeiRZJHa8yXIs6YbvWk9Xw2sVe76BJV4B8Mi5YskuhQJXE2L1EhkdpAZdNHlmDsctAXinu1+ORnvmG83sagCfAfB5d38DgGEAH7lQh4UQrWPWYPcG58qP5ps/DuCdAL7THL8bwPuXxEMhxKIw1/7s2WYH1xMAHgTwIoARdz/3vutVAJuWxkUhxGIwp2B395q7XwFgM4CrAPziXE9gZnvMbJ+Z7WOf44QQS88F7ea4+wiAvwPwVgArzezczsJmAP1kzl533+3uu/ORTQohxNIya7Cb2VozW9n8ux3AuwEcQiPof7N5sw8DuH+pnBRCLJy57Pn3AbjbGsXjMgC+5e5/bWYHAdxrZn8E4CkAX5nLCZ32yOFyB2slBOMySLFYpLZ4Igm35QthOSwm8+XAJbRaJBmjGquTF0u4IDIgq1kGxGUoiyXrFCNJPvnwu7jYuWISWmyNK0ReA4BMPbzG9ci5qhFbNtLjqR6RDmOP2XxasHGJjfs3a7C7+34AbwmMv4TG53chxD8C9A06IRJBwS5EIijYhUgEBbsQiaBgFyIRbD7b/vM+mdlJAEeb/64BwAtmtQ758Vrkx2v5x+bHVndfGzK0NNhfc2Kzfe6+e1lOLj/kR4J+6G28EImgYBciEZYz2Pcu47lnIj9ei/x4La8bP5btM7sQorXobbwQiaBgFyIRliXYzexGM/uZmb1gZrcthw9NP46Y2bNm9rSZ7Wvhee8ysxNmdmDGWK+ZPWhmh5u/Vy2TH3eYWX9zTZ42s/e2wI8tZvZ3ZnbQzJ4zs3/THG/pmkT8aOmamFmbmf3UzJ5p+vGHzfHtZvZYM26+aWYXVg3G3Vv6AyCLRg27HQAKAJ4BsKvVfjR9OQJgzTKc9zoAVwI4MGPsjwHc1vz7NgCfWSY/7gDw71q8Hn0Armz+3Q3g5wB2tXpNIn60dE3QSErvav6dB/AYgKsBfAvAB5vjXwbwry7kuMtxZb8KwAvu/pI36szfC+CmZfBj2XD3hwGcOW/4JjSq9AItqtZL/Gg57j7g7k82/x5DoxLSJrR4TSJ+tBRvsOgVnZcj2DcBODbj/+WsTOsAfmRmT5jZnmXy4Rzr3X2g+fcggPXL6MutZra/+TZ/yT9OzMTMtqFRLOUxLOOanOcH0OI1WYqKzqlv0F3r7lcCeA+A3zez65bbIaDxyg5E2swsLV8CsBONhiADAD7bqhObWReA7wL4mLuPzrS1ck0CfrR8TXwBFZ0ZyxHs/QC2zPifVqZdaty9v/n7BID7sLxltobMrA8Amr9PLIcT7j7UfKLVAdyJFq2JmeXRCLBvuPv3msMtX5OQH8u1Js1zX3BFZ8ZyBPvjAC5t7iwWAHwQwAOtdsLMOs2s+9zfAH4NwIH4rCXlATSq9ALLWK33XOsJWMwAAAC7SURBVHA1uRktWBNrVKH8CoBD7v65GaaWrgnzo9VrsmQVnVu1w3jebuN70djpfBHAJ5bJhx1oKAHPAHiulX4AuAeNt4MVND57fQSNBpkPATgM4McAepfJj68DeBbAfjSCra8FflyLxlv0/QCebv68t9VrEvGjpWsC4M1oVGzej8YLy3+a8Zz9KYAXAHwbQPFCjquvywqRCKlv0AmRDAp2IRJBwS5EIijYhUgEBbsQiaBgFyIRFOxCJML/BZuVsP/teM3aAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Flattened image shape: (3072,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x72 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2cAAAA9CAYAAAApthFpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZTklEQVR4nO3dfZBkV3nf8e9z7+3u6XnZmd3Z1UqrFZIQSLIwsSBY2EBkKsS8mZRMChNBGSg7DthGjklwJbzYMRA7wSlDFYSUCJQhCsZIApsyxvhFlHHhxJZAAknoFe3Ket3Vvu/M7Mz0y73nyR/ndE/PaHZ2drUvo53fp6p3b9+Xc8859+lz7ul7+465OyIiIiIiInJmZWc6AyIiIiIiIqLBmYiIiIiIyJqgwZmIiIiIiMgaoMGZiIiIiIjIGqDBmYiIiIiIyBqgwZmIiIiIiMgaoMGZiIg8a5jZ35rZLx1l2XPM7IiZ5cdadxX7eaOZPZ7Se5GZPWJm/+KZ5P1MeTbnXURkvdHgTEREjsszGfScSu7+mLuPunt1EpL7feC6lN73j2dDM3Mze97A+1ea2RMnIU8iInKW0+BMRETk6S4E7j3TmRARkfVFgzMRkXXIzN5nZjvNbMbM7jOzNw4s+5CZ/eHA+4vS1aDCzH4X+GfAp9Itf59K67zMzL5rZlPp/5cNbP+3ZvY7Zvb3aZs/M7NJM/uimU2n9S8aWP+oaSWXmNl30rZ/amablubzKGX+RTO738wOmdlfmdmFy6zTMLMjQA7cZWY7l1nnKjP7BzM7bGa7zexTZlZPy76dVrsrlfUdwF8A29L7I2a2zcyygWNwwMxuXqYc7zCzx8xsv5l9cGD/R902LX+bmT2aln0QERF51tDgTERkfdpJHGSNAx8G/tDMzjvWRu7+QeDvWLjl77o0MPhz4JPAJPBx4M/NbHJg02uBtwHnA5cA/wB8HtgE3A/8NsAq03o78IvAeUCZ1l2RmV0DfAD4V8CWVIYvLVO+truPprc/5u6XLJNcBfx7YDPwk8CrgF9N2189sO2ou98AvA7Yld6Puvsu4NeAnwV+CtgGHAL+55L9vAK4LKX/n83sR9L8o25rZlcA1xPrehuxDrcfq35ERGRt0OBMRGQdcvcvu/sudw/ufhPwEHDVCSb3M8BD7v4Fdy/d/UvAA8C/HFjn8+6+092niFeSdrr7N929BL4MvOg40vqCu9/j7rPAbwFv7j0EZAW/DPw3d78/7fO/Alcud/XsWNz9Dne/NeXvEeB/EQdKx+OXgQ+6+xPu3gY+BLxpyVW/D7v7vLvfBdwF/Ngqtn0T8HV3/3Za9ltAON4yiojImbHsrR8iInJ2M7O3A/8BuCjNGiVeCToR24BHl8x7lHiVrGfPwPT8Mu97V6tWk9bjS5bVOHbeLwQ+YWYfG5hnKd2l+1uRmV1KvKL3EmCY2JfecTxppPx81cwGB04VsHXg/VMD03Ms1NFK225joH7cfdbMDhxn3kRE5AzRlTMRkXUmXS36LHAdMOnuE8A9xMEKwCxx0NFz7pIkfMn7XcQBw6DnAE+eQPZWk9YFS5Z1gf3HSPdx4F3uPjHwarr7359AHq8nXs17vrtvIN4uaSusv7S+evl53ZL8DLn7aupspW13M1A/ZjZMvLVRRESeBTQ4ExFZf0aIA4Z9AGb2C8CPDiy/E7g6/d2wceD9S7bfAzx34P03gEvN7K3poSH/GrgC+PoJ5G01af28mV2RBh4fAb6yisfnfxp4v5m9AMDMxs3s504gfwBjwDRwxMwuB35lyfKl9bMHmEx1OZif3+3dVmlmW9Lv4lZjpW2/ArzBzF6RHlLyEdTXi4g8a6jBFhFZZ9z9PuBjxIdy7AFeCPy/geW3ADcBdxNv11s6yPoE8TdOh8zsk+5+AHgD8F7gAPAfgTe4+7GuZi2Xt9Wk9QXgfxNv+xsC/t0q0v0q8HvAjWY2TbxS+LrjzV/yG8BbgRniFcibliz/EHBDeprjm939AeLDRx5O87YR6/BrwF+b2QxwK/DSVe7/qNu6+73Au4E/Il5FOwTob6yJiDxLmPtyd1uIiIiIiIjI6aQrZyIiIiIiImvAMxqcmdlrzexBM9thZu87WZkSERERERFZb074tsb0N2V+CPw08X727wJvSb9lEBERERERkePwTK6cXQXscPeH3b0D3Ais9klTIiIiIiIiMuCZDM7OZ/EfAn2CxX8kVERERERERFapONU7MLN3Au8EyIdq/3T0wgkKC2TmWPq7nNb/B/DFf61zYa2BeW4DyyG4wcB6A0lhOEUWyNLSgFF5huEEj9O1LJBZWJRub3tPqbkPTGPgMa2yyghlBqVB7gvDXQfCQHoByKE+1GUo7y6zr5h/d0v5pv++1amRtWwh3aWFBDxbmGcBPAevB4bqJTWrFpWlV4eL6tmNVrcgm188Xu9l0zym29vX4Pz+vLpT1CpqWUVmTrxjdvF+ghudssBms4VCLlOewX0tOaBxMgevOY16lyKLxy6k49JLsgwZTBdxXq9Yg3XEwnR/X9ni/fTznQM1p1YrKbLQL0vv/2q6hlVpO0v1Yyltf3p99dc5il59ZkXAbKEGw2xB3mXZ4w4DafriIvSLNFi2gQPj2UL53SDrQlYuXqdf98bTEu/HgQ2s6kcvo/ni7frzKycUhucD+3SwCrzXWoWB+QPleFodp/TydjxeVSOP5cwXjjmWjm0vvRQroVjIIw5ZFfPQT7cXM2mfedexVhdCRRgdItQMq8CC99OohmxRmS0s1HvecUJuVI1Y71alz1zpWLr1PBSGBchaXSjTfurWjztPZa8ai/Maj6PjedzeKqdqGKEej3PedrJ2BVUFIUBR4LU8lScQxoYoGxbTSXHrOYQaZJ1Udelz04+ZLB0zg1CP9VnMQTFbxn2E9MHICwgVBKcab1JtqghVBlUqUJEOctf69YXF/fbrsrdfi2V3g9qck83HesIM8hxCwKsKH21SbGszUZun9Dx+fskoQ0bp8X93I7jFtj3EOs57+6xi+ash8NyxrlGbdbJuKldVQRVgqEFra8Y5IzOMZC3qBCqM2VCnsEDA6HhBOxQEN0rP6ZQ5oZVDHg+mlan9zD2+AMqMYh6KIyWUJRQFrc0FQ8MdRos24/kcdYNO6rOK2DIy5zVaXqNpHUrPaXtBJxRUqczdKsc7WT92vAAyh8qozULeqmhN5hRDJVXIoJXhhTM+Mk8j68bPAU5mgZxALSXU63O7njNTNZnt1MmyQFXmWNtivVqMEc+gmE/Hsh7ruzbnlCle85GS3EJq86GZd8ks9uW9pqb0jKnpEerTTnfEqG3o0KlyaOWxbANtYN5a+OxjYFu6dFoFWcdiu9BrX/NArYj9mlls87tlTm2/kbVKMGhtqZEPVVTtvF+Hvb7YyvS5TjFsm7o4hu0r4meq16emz9bge6ugPhPojmZk3ZhGdyJAmZHPx7YpFAttVjoUsS5GnbwWyPdkZGXAM+uX1TOL7X1msc0vHQuOb0uZ31NQNSy2T+3YFnlh/bYmqxYa2qpuVE1oHAxYGeJnrtuNJ06WCpL1ChjbmHK7M5R3ae1rxlg2oFumijMockIjdQQe2y3cITOs3U1tCBCc/s9zbHGHY0VO67wGjaEO5aE6tf1zYIZ7wCyDPIOiIBQZoW79Y4Y7WRXTxj21M95/H18BD+kc1oxqokk5EcgPZ+SH5hbnI7OYtyyjGqkvnI8Qj/OiPmug3wHvt9nW2zcQ6jlZu8Q76WAP/jzJwCzr9w/0khqI836dLverpoF21arU/wSwEPp56qdjRqjlhMbAed7gIVh6ErK0nAN1QK8eAk+z6HzCFmYuPRdZ9nxjsDzL5YOB+mHxskX7OUrZfOn83j4H9rVsPQNzB57Y7+5bllv2TAZnTwIXDLzfnuYtzqP7Z4DPAIxfttWv/uzPMdmYZaRok5mTEwdqWcp9SJ0ikDrMPHUtC9pV0Z9XudGqaou266XVCQVDeZfNjVnqWdmfd7jTpMgqWlWN6c4Qm4dm2VBrLUo3w+n6wv6DG2XICB478ODGfFnj4Oww03tHKQ4VlBsqrFnFz0k3w9oLn8B8NqMaDVx42VNcOr63n05PN+T9E4PgRmZOGXI6IeeBx86l+eBQ/wShd8JDaog9g6oJVcOxyshbsVEOF89z2bY9bB2a6ZcF0qAy7SfWeUYn5Dz45FaG727G45Y6jFBL+e9A7UicXzVTx2mx0XaD7pjTOb/D1q1TbBudop5X/WMyeAw7Vc6jBzZRu3UsnoCn4A213sloTLeYjY1/OWyUzbhO3kknhXksX3d7m0u272NyaJYyZE+Lg32zI1S3bMZKCI24Xa88tSP0G8janOMG5XA8AeidfIXeAAHoTDjlOR22nXeILc1ZAObLGgGjXRYcuuU8Goc9nbTGk2zPY+dv5cLxKlrxJNyLgX2kQVx/cBGgOwqt7V2GJ+cYqnfJLPYL87duZmS3k5Ux/2UznrTHDWMHDQsn9/3PYdpH/+Sg12gExzOjGoppVU2nqkNzjzG8N3YIvYFIKKxfhyF10lbFvPTjslgoDx7LuLTx7B3nop3KkZZnlVPMBtoTOe3xmH6oGVnHaUw585vjASvmHC9SeQYGTb2yZ73+3aExXTHyjzMAHHnuBrrDRnfU4qAiDcTaG42qHrfL27HuOxMe0y4hK43GIahPLdRF3omV2xtEjuzu0HhoDz49w+xPXc7clpzarFObC2RdJ+sEDl7eIG97PBlNaZRDMT/jj3RpbcyZvjCjcRgaU4G8A0MHumSdgAWntblO1nFG7nuKcPAw86+4nNnzChpTgVAzQhGPw+FLMupTCydPzf2BrHTaG3Jq84HaTMXhS+rMne80nzImdnYZ3nkIm5rB5+Zh62a6545T++GT+JFZ5q5+AYefVzC8J1AOGaEGnQ1G6xxneFcctHXG4r6b+2LMVPU4YAkFHNme0drsbPm+s/H2fdhcCz9yBIJjkxvx6Rm81WbmVT/K1M/PMDvVhJl4glxt6mK5k+2rk88Z5YhDBiOPxUFE3o7HZHhfrIOpizOqITjnjpLRu3YRDhzEGg1sfAyfnSccnqL70hcy+eFHuGbLnezpjtMKNaaqJnvbYxxoj3BwfphOlTPbqjO3f5hsPqc4Ymx4OMZcYzrQGs+YuhS6GyuajxdsvaPL0K4ZbK4Nh6fxI7Nw6UU88J5Rfu2qv+Flww+xvZhnJmTc1rqITfkR5kKDx7ub2DF3DvNVjYPtER4/PMHcAxNUowFvVtT21WLbNF6Rj8UTsXCgzuSdGVtuOwh79sM5kzz4S5u47EWP8YrJnbx+7G4uLCoeLXM6ZExmbWoG32ufy0Ptrbxw6An2lmPsbG/lsflNzHQb7J8fZdeBcardTRoHMzyDzsZAGKkoDhdsvS0w9tAUD719go2XH2Rqpkn20DDtLRU/85K7eN7wHgDGshZj+TwjWZvz8ykyc+ZCjcNhmKfKcb558AruePICmo0Ohw6M0tzZYOyxGCdzW+Px3XxXwHNj+qI4GDnney0OPb/B3DZj/Kq9TAzNM9GYJ7jxI2NPMZq3mKsa5Ba/hD3QHeHrf/lSLvhmmz0/PsT5r32Uxw5upNwxRuNgHFn02opND1aEwshKp6oZzX+7i4d3nMvowwWtTU7VdCicbKLD1skphmtdGnnJfFnj8X0b2f75Gs0f7oUs44HrzmXDJYeZemSC2rRhIfbF3TGncdAYOuBxoDlkDL31KTpVTu2zk4Sa0RmJfUJVM7pjUA7H/txrTv1Qxvl/N8+ulzcZfcIp5gNPXdPBD9bZdHdGY9qZ25zRnoznA8O7Fxr+fVd3mdh8hI3/Y4TGvjmqkTpZNxCKjGq4oGpkdEcyqrrR3F9StCrK3zwY4+wTW5m6uKBqwoZHAkMHurQ3Fv1+oDEV2yWAmQsKDr0wcMmNbWqH5uOXO0/sgU4X6jWwDBtqQJHDfItwzkYOfbTk0o17efD6FzB56944eNmzL2a80YDNG5m7cAOeG1knUJvukrVKwnCN2s7dsa0CvNMhpEGK5Tn9zhLIJzdy/3+6iEuveIJ9Nz2Hc274PmQZ3ulgzSbZ6Ahh6yY6k02OnF+P5wOzsb2sT5Xkc12sigNOa3WxThc6XbwsYzna7diO1QqmXvNP2H/NPJN/1mT8y99byEdmZI0G1GvYyDBTP76NbjPr91mexQF2b1AQ+8U0KAtOMRcoZrrk812sU4I7rQvGGXr4AOHxXXFwWi18G2eZYY0GrZdfwZFtMdAHv5TsDfyzXv9WLfTVVsV+w6r4hWP9cEnWDeStkuxIK37pFAJWBTzPIM9pXTDOzHPq/X7ci4Vzkt65Rm/w3+ubCenLioEvQz1PXxZ2vH8+1N+2ituXDYtfJqQvMMohW3T/X6gt3g5imbPOwP49fZkaFpa7xS8YenW+UJnxM9n/wiR9WeNm/XObwfn9zdJ5SdaN++yfk/S+wE+D29tveO+jHMUzua3xu8DzzexiM6sD1xL/KKaIiIiIiIgcp2MOzszsAjP7lpndZ2b3mtmvp0W/CWwA7gdmgDvd/d5TmFcREREREZGz1mpuayyB97r798xsDLjDzG5Jyz7q7r9/6rInIiIiIiKyPhxzcObuu4HdaXrGzO5HT2UUERERERE5qY7rN2dmdhHwIuC2NOs6M7vbzD5nZhtPct5ERERERETWjVUPzsxsFPhj4D3uPg1cD1wCXEm8svaxo2z3TjO73cxu70zNn4Qsi6w/gw8QEhEREZGz06oGZ2ZWIw7MvujufwLg7nvcvXL3AHwWuGq5bd39M+7+End/SX28ebLyLbKuZMv9/Q4REREROaus5mmNBvwBcL+7f3xg/nkDq70RuOfkZ09ERERERGR9WM3TGl8OvA34gZndmeZ9AHiLmV1J/HNqjwDvOiU5FBERERERWQdW87TG/8uiv33d942Tnx0REREREZH16bie1igiIiIiIiKnhgZnIiIiIiIia4C5n75ndJvZDPDgaduhPNtsBvaf6UzImqYYkZUoPmQlig9ZieJDVnKy4+NCd9+y3ILVPBDkZHrQ3V9ymvcpzxJmdrviQ1aiGJGVKD5kJYoPWYniQ1ZyOuNDtzWKiIiIiIisARqciYiIiIiIrAGne3D2mdO8P3l2UXzIsShGZCWKD1mJ4kNWoviQlZy2+DitDwQRERERERGR5em2RhERERERkTXgtA3OzOy1Zvagme0ws/edrv3K2mJmj5jZD8zsTjO7Pc3bZGa3mNlD6f+Nab6Z2SdTzNxtZi8+s7mXk83MPmdme83snoF5xx0PZvaOtP5DZvaOM1EWOfmOEh8fMrMnUxtyp5m9fmDZ+1N8PGhmrxmYr/7nLGRmF5jZt8zsPjO718x+Pc1XGyIrxYfaEAHAzIbM7DtmdleKkQ+n+Reb2W3peN9kZvU0v5He70jLLxpIa9nYOSHufspfQA7sBJ4L1IG7gCtOx771Wlsv4BFg85J5/x14X5p+H/B7afr1wF8ABvwEcNuZzr9eJz0ergZeDNxzovEAbAIeTv9vTNMbz3TZ9Dpl8fEh4DeWWfeK1Lc0gItTn5Or/zl7X8B5wIvT9BjwwxQHakP0Wik+1Ibo1TvmBoym6RpwW2obbgauTfM/DfxKmv5V4NNp+lrgppVi50TzdbqunF0F7HD3h929A9wIXHOa9i1r3zXADWn6BuBnB+b/H49uBSbM7LwzkUE5Ndz928DBJbOPNx5eA9zi7gfd/RBwC/DaU597OdWOEh9Hcw1wo7u33f0fgR3Evkf9z1nK3Xe7+/fS9AxwP3A+akOEFePjaNSGrDOpLTiS3tbSy4F/DnwlzV/ahvTalq8ArzIz4+ixc0JO1+DsfODxgfdPsPIHRM5eDvy1md1hZu9M87a6++40/RSwNU0rbtan440Hxcn6c126Le1zvVvWUHysa+n2ohcRv/lWGyKLLIkPUBsiiZnlZnYnsJf4xcxO4LC7l2mVwePdj4W0fAqY5CTHiB4IIqfbK9z9xcDrgHeb2dWDCz1eH9YjRAVQPMiyrgcuAa4EdgMfO7PZkTPNzEaBPwbe4+7Tg8vUhsgy8aE2RPrcvXL3K4HtxKtdl5/hLJ22wdmTwAUD77enebLOuPuT6f+9wFeJH4Q9vdsV0/970+qKm/XpeONBcbKOuPue1JkG4LMs3Dqi+FiHzKxGPPH+orv/SZqtNkSA5eNDbYgsx90PA98CfpJ4y3ORFg0e734spOXjwAFOcoycrsHZd4Hnp6ef1Ik/ovvaadq3rBFmNmJmY71p4NXAPcRY6D0d6x3An6bprwFvT0/Y+glgauBWFTl7HW88/BXwajPbmG5PeXWaJ2ehJb87fSOxDYEYH9emp2ldDDwf+A7qf85a6bcefwDc7+4fH1ikNkSOGh9qQ6THzLaY2USabgI/Tfxt4reAN6XVlrYhvbblTcDfpKvzR4udE1Ice5Vnzt1LM7uO2NjlwOfc/d7TsW9ZU7YCX43tJQXwR+7+l2b2XeBmM/s3wKPAm9P63yA+XWsHMAf8wunPspxKZvYl4JXAZjN7Avht4KMcRzy4+0Ez+y/EDhTgI+6+2odIyBp2lPh4pZldSbxV7RHgXQDufq+Z3QzcB5TAu929Sumo/zk7vRx4G/CD9JsRgA+gNkSio8XHW9SGSHIecIOZ5cQLVje7+9fN7D7gRjP7HeD7xEE+6f8vmNkO4sOqroWVY+dEWHoEpIiIiIiIiJxBeiCIiIiIiIjIGqDBmYiIiIiIyBqgwZmIiIiIiMgaoMGZiIiIiIjIGqDBmYiIiIiIyBqgwZmIiIiIiMgaoMGZiIiIiIjIGqDBmYiIiIiIyBrw/wEj/uUtcGrC2AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some important issues for feedfwd networks in the image domain:\n",
        "\n",
        "* At the very least, local information gets lost.\n",
        "* Extremely high dimensional input -> extremely large number of weights to train!"
      ],
      "metadata": {
        "id": "fBxtEUISgPBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"architecture\"></a>\n",
        "# Convolutional Neural Network (CNN) architecture\n",
        "\n",
        "Breakthrough solution for image recognition the introduction of **Convolutional Neural Network** architecture: \n",
        "- De-facto standard now\n",
        "- Numerous applications outside of it\n",
        "\n"
      ],
      "metadata": {
        "id": "6PINFCbAFjCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some sung and unsung heroes \n"
      ],
      "metadata": {
        "id": "GTgKY5-9FkhF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBNny4BQo59R"
      },
      "source": [
        "\n",
        "### Fukushima\n",
        "\n",
        "Worked on **neocognitron model** in the 1980s, which predates CNN-s.\n",
        "- Hierarchical, multilayered artificial neural network. \n",
        "- Used for handwritten character recognition and other pattern recognition tasks.\n",
        "- Served as inspiration for convolutional neural networks.\n",
        "\n",
        "<a href=\"http://personalpage.flsi.or.jp/fukushima/files/FukushimaImg.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1KlhW8c2YjNfalQFZd5pR_1ZtOytRgq1R\" width=300 heigth=300></a>\n",
        "\n",
        "\n",
        "\n",
        "Inspired by model proposed by Hubel & Wiesel in 1959. \n",
        "- Found two types of cells in the visual primary cortex:\n",
        "- Simple cell (S-cell) and complex cell (C-cell)\n",
        "- **Simple cell**: Takes input from complex cells. Responds primarily to **oriented edges and gratings** (bars of particular orientations) in small regions of the visual field\n",
        "- **Complex cell**: Responds (like simple cell) to **oriented edges and gratings**; however **has a degree of spatial invariance**. This means that its receptive field cannot be mapped into fixed excitatory and inhibitory zones. Rather, it will respond to patterns of light in a certain orientation within a large receptive field, regardless of the exact location. Some complex cells respond optimally only to movement in a certain direction. Insensitive to exact location of the edge in the field.\n",
        "- **Cascading model of these two types of cells** for use in pattern recognition tasks, one in a sense a *local edge detector* and one a more *global* one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-11T13:19:57.958225Z",
          "start_time": "2019-10-11T13:19:57.926389Z"
        },
        "hideCode": true,
        "hidePrompt": true,
        "id": "6N2ENSEho59T",
        "outputId": "d0891cbe-9afc-410d-ca19-e31bac3eb66a"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Cw5PKV9Rj3o\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Cw5PKV9Rj3o\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neocognitron is natural extension of cascading models. \n",
        "- Consists of multiple types of cells: S-cells, C-cells. \n",
        "- **Local features** are extracted by S-cells (convolutional layer), and these features' **deformation, such as local shifts**, are tolerated by C-cells (downsampling layer). \n",
        "- **Local features** in the input are **integrated gradually** and classified in the **higher layers**\" [Wikipedia](https://en.wikipedia.org/wiki/Neocognitron)\n",
        "\n",
        "<a href=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/581528b2215e017eba96ef4ee16d33a74645755f/3-Figure1-1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1mOh77E9W3xjDSkgf3n9bUOvZj9ntUsIv\" width=400 heigth=400></a>\n",
        "\n",
        "Many of the features of later CNNs present in Fukushima's work, but ideas were based on learning rules of classical era, predating backprop.\n"
      ],
      "metadata": {
        "id": "XIEWueVRE-P_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Yann LeCun\n",
        "\n",
        "It is common to attribute the elaboration of Convolutional Neural Nets - as well as many advances till this day - to [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun), a student of Hinton, who worked extensively on image recognition tasks, especially in hadwritten digit recognition for numeric cheques. He is also the creator our \"workhorse\", the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.\n",
        "\n",
        "<a href=\"https://qph.fs.quoracdn.net/main-thumb-48611829-200-wjnraisajwlkqlmolpgmqnkfnxvuezwr.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=15YmSA67B0Ri2fE0uN5HaQcHofRO8eOgG\" width=400 heigth=400></a>\n",
        "\n",
        "Till this day he is one of the most inlfuential figureheads of the deep learning revolution, he's talks and publications are well worth following.\n"
      ],
      "metadata": {
        "id": "J7zXVUHKFgiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic building blocks of CNNs\n",
        "\n",
        "Basic building blocks of a CNN:\n",
        "\n",
        "- **Convolutional layers** - _edge detectors_; analogous to simple cells\n",
        "- **Subsampling layers** (\"Pool\" or \"Pooling\") - _going beyond exact location_/ reducing information (analogous to complex cells),\n",
        "- **Fully connected layers** - combining the information.\n",
        "- A **convolutional layer** contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a **filter**. Units can **share filters**. **Downsampling layers** contain units whose receptive fields **cover patches of previous convolutional layers**. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes **even when the objects are shifted**.\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1Yw1NECPVDmrO2a3JoPwJVvnsNU8BU1Ul\"><img src=\"https://drive.google.com/uc?export=view&id=1BS6SG7AuFU5kXQoMtYEAk4G1X2Dq65kP\" height=\"500\"></a>\n",
        "\n",
        "A good summary introduction of CNN elements can be found [here](https://www.wikiwand.com/en/Convolutional_neural_network) and [here](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/).\n",
        "\n",
        "### Basic ideas\n",
        "\n",
        "Remember idea of _\"Embed and cut\"_ from representation learning? Convolutional and pooling layers essentially are building up **a pipeline for capturing a hierarchy of more and more abstract features of the image** until the \"embedding space\" provides enough information for final classifier - the fully connected layers. \n",
        "\n",
        "<a href=\"https://www.researchgate.net/profile/Siva_Chaitanya_Mynepalli/publication/281607765/figure/fig1/AS:284643598323714@1444875730488/Learning-hierarchy-of-visual-features-in-CNN-architecture.png\"><img src=\"https://drive.google.com/uc?export=view&id=1WypG0MbnF80Fy2t5gPZZVk8k1DBGbTaM\" width=600 heigth=600></a>\n",
        "\n",
        "[source](https://www.researchgate.net/publication/281607765_Hierarchical_Deep_Learning_Architecture_For_10K_Objects_Classification)\n",
        "\n",
        "<a href=\"https://devblogs.nvidia.com/wp-content/uploads/2015/11/hierarchical_features.png\"><img src=\"https://drive.google.com/uc?export=view&id=1-o9nsggls89UjPLPaF0-cQG1krYlMXYy\" width=600 heigth=600></a>\n",
        "\n",
        "We could not have defined these features so exactly by hand!\n"
      ],
      "metadata": {
        "id": "d9Oz2g-4GKrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hierarchy of \"detectors\" or \"filters\"\n",
        "\n",
        "Alternatively think of architecture as hierarchy of \"filters\" that \"scans\" through input field and gives most \"signal\" when the input part is overlaid with specific pattern it is looking for.\n",
        "\n",
        "<a href=\"http://mathworld.wolfram.com/images/gifs/convgaus.gif\"><a href=\"https://drive.google.com/uc?export=view&id=1jBJQH4LnMEL3K91ETQo4JhJcuI2La-q7\"></a><a href=\"https://drive.google.com/uc?export=view&id=1jBJQH4LnMEL3K91ETQo4JhJcuI2La-q7\"></a><img src=\"https://drive.google.com/uc?export=view&id=1jBJQH4LnMEL3K91ETQo4JhJcuI2La-q7\" height=\"300\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "0XV_GdS7NxR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Engineered prior for invariances\n",
        "\n",
        "Usage of filters and convolution (see below):\n",
        "- Realize a kind of invariance, since **the filters detect the appropriate pattern irrespective of its location in the picture**. \n",
        "- Combined with **successive abstraction of hierarchy** - can effectively solve problems of **location invariance**.\n"
      ],
      "metadata": {
        "id": "Hk6zdUIVN0oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weight sharing and subsampling: parameter decrease\n",
        "\n",
        "Application of **\"subsampling\" layers** together with **weight sharing** in case of convolution allows for radical decrease of number of weights used for a model, making training time feasible for _huge_ neural networks and work on _high resolution_ (high dimensionality) input data. \n",
        "\n",
        "(By the way, as weight sharing **decreases the number of parameters** it can also be considered as an implicit \"regularization\".)\n"
      ],
      "metadata": {
        "id": "Pbz7l-A-N0fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolution \n",
        "\n",
        "#### Mathematical definition\n",
        "General definition:\n",
        "\n",
        "$$y(t) = x(t) * h(t) = \\int_{-\\infty}^{\\infty}x(\\tau)h(t - \\tau)d\\tau$$\n",
        "\n",
        "- f and g are both functions which operate on the same domain of inputs\n",
        "- x(t) is the input, h(t) is the impulse response\n",
        "- In the context of probability, you might frame these as density functions; in audio processing, they might be waveforms; \n",
        "- When you convolve two functions together, the convolution itself acts as a function, insofar as we can calculate the value of the convolution at a particular point t, just as we can with a function.\n",
        "\n",
        "- To calculate a convolution between f and g, at point t, we take the integral over all values tau between negative and positive infinity, and, at each point, multiply the value of f(x) at position tau by the value of h(x) at ttau, that is, the difference between the point for which the convolution is being calculated and the tau at a given point in the integral.\n",
        "\n",
        "- We have $h(t - \\tau)$, as one function is reversed and \"moves through the other\"\n",
        "\n",
        "<img src=\"http://mathworld.wolfram.com/images/gifs/convgaus.gif\" height=\"300\">\n",
        "\n"
      ],
      "metadata": {
        "id": "yRrUCxYnN-tR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Convolution, cross correlation and auto-correlation\n",
        "Some features of convolution are similar to cross-correlation: for real-valued functions, of a continuous or discrete variable, it differs from cross-correlation only in that either f (x) or g(x) is reflected about the y-axis; thus it is a cross-correlation of f (x) and g(x), or f (x) and g(x)\n",
        "\n",
        "\n",
        "__Convolution:__\n",
        "\n",
        "Continuous function:\n",
        "$$y(t) = x(t) * h(t) = \\int_{-\\infty}^{\\infty}x(\\tau)h(t - \\tau)d\\tau$$\n",
        "\n",
        "Discrete function (1d):\n",
        "$${\\displaystyle (f*g)[n]=\\sum _{m=-\\infty }^{\\infty }f[m]g[n-m]}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "__Cross-correlation:__\n",
        "\n",
        "Continuous function:\n",
        "$${\\displaystyle (f\\star g)(\\tau )\\ \\triangleq \\int _{-\\infty }^{\\infty }{\\overline {f(t)}}g(t+\\tau )\\,dt}$$\t\n",
        "\n",
        "\n",
        "where ${\\displaystyle {\\overline {f(t)}}}$ denotes the complex conjugate of $f(t)$, and $\\tau$  is the displacement, also known as lag (a feature in $f$ at $t$ occurs in $g$ at $t+\\tau$.\n",
        "\n",
        "Discrete function (1d):\n",
        "\n",
        "$${\\displaystyle (f\\star g)[n]\\ \\triangleq \\sum _{m=-\\infty }^{\\infty }{\\overline {f[m]}}g[m+n]}$$\t\n",
        "\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1ScFaF4uPzOB80ibtFtV1GXAItG-xeStY\"><img src=\"https://drive.google.com/uc?export=view&id=1t0zHH0sA-Bh0pOTJuKuCNTArIXoKzZPP\" width=50%></a>\n",
        "\n",
        "\n",
        "Visual comparison of convolution, cross-correlation, and autocorrelation. For the operations involving function f, and assuming the height of f is 1.0, the value of the result at 5 different points is indicated by the shaded area below each point. Also, the symmetry of $f$ is the reason $ f*g$ and $f\\star g$ are identical in this example.\n",
        "\n",
        "\n",
        "If $X$ and $Y$ are two independent random variables with probability density functions $f$ and $g$, respectively, then the probability density of the difference $Y-X$ is formally given by the cross-correlation (in the signal-processing sense) $f\\star g$; however this terminology is not used in probability and statistics. In contrast, the convolution $f*g$ (equivalent to the cross-correlation of $f(t)$ and  $g(-t)$ gives the probability density function of the sum $X+Y$.\n",
        "\n",
        "Note that in statistics cross-correlation is always normalized\n",
        "\n"
      ],
      "metadata": {
        "id": "uKovhBxRODwt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW9wxkfso59U"
      },
      "source": [
        "#### 2D example\n",
        "\n",
        "Generally the idea of convolution is as follows:\n",
        "\n",
        "<a href=\"https://qph.fs.quoracdn.net/main-qimg-578748437404fe6733bc7823755e813c-c\"><img src=\"https://drive.google.com/uc?export=view&id=1OyQ61_iIvOw648jyyHA_ShXahpRleXko\" width=600 heigth=600></a>\n",
        "\n",
        "**Or more in detail:**\n",
        "\n",
        "This is **the pixel intensity representation** of the input image:\n",
        "\n",
        "<a href=\"https://ujwlkarn.files.wordpress.com/2016/07/screen-shot-2016-07-24-at-11-25-13-pm.png?w=127&h=115\"><img src=\"https://drive.google.com/uc?export=view&id=1s_1b2OfApLRgOM5DAG2zI6VFwIIy8v4V\" height=\"200\"></a>\n",
        "\n",
        "This is **the weight matrix of the neuron, that is the \"filter\"**:\n",
        "<a href=\"https://ujwlkarn.files.wordpress.com/2016/07/screen-shot-2016-07-24-at-11-25-24-pm.png?w=74&h=64\"><img src=\"https://drive.google.com/uc?export=view&id=13H-YblvYFu7vk247tpltvuUKfYG78L-h\" height=\"100\"></a>\n",
        "\n",
        "**Convolution is the \"shifting\" of the \"filters\" throughout the input**: \n",
        "\n",
        "<a href=\"https://ujwlkarn.files.wordpress.com/2016/07/convolution_schematic.gif?w=268&h=196\"><img src=\"https://drive.google.com/uc?export=view&id=1T41iR_7bmAADcvR10t5nO4KSaOiXWfgy\" height=\"300\"></a>\n",
        "\n",
        "[source](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n",
        "\n",
        "The convolution of filters on an original image:\n",
        "\n",
        "<a href=\"https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif?w=748\"><img src=\"https://drive.google.com/uc?export=view&id=1iw-zbgmqv2UietTjIUlsbtqQqg3x8sea\" height=\"300\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8S8LszBo59Y"
      },
      "source": [
        "#### Another intuition for convolutions (and cross-correlation): dot product\n",
        "\n",
        "Notice that in the cases we considered we actually calculate _dot products_ between our two inputs: one shifted around and one kept fix (in the 2d case between the shifted filter and the input) that is, we calculate $\\mathbf x \\cdot \\mathbf y$, which is nothing else than \n",
        "$$\\|\\mathbf x\\|  \\|\\mathbf y\\| \\cos\\alpha$$ where $\\alpha$ is the angle between $\\mathbf x$ and $\\mathbf y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j68w3PVXo59a"
      },
      "source": [
        "#### More detailed example of 1D convolutional neural network\n",
        "[\"Source\"](https://colah.github.io/posts/2014-07-Understanding-Convolutions/) \n",
        "\n",
        "So, how does convolution relate to convolutional neural networks?\n",
        "\n",
        "Consider a 1-dimensional convolutional layer with inputs $\\{x_n\\}$ and outputs $\\{y_n\\}$:\n",
        "\n",
        "\n",
        "<a href=\"https://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY.png\"><img src=\"https://drive.google.com/uc?export=view&id=1APOXs74gq0GrD1bmcR8pizjCjVSrLFxX\" width=400 heigth=400></a>\n",
        "\n",
        "As we observed, we can describe the outputs in terms of the inputs:\n",
        "\n",
        "$$y_n = A(x_{n}, x_{n+1}, ...)$$\n",
        "\n",
        "Generally, $A$ would be multiple neurons. But suppose it is a single neuron for a moment.\n",
        "\n",
        "Recall that a typical neuron in a neural network is described by:\n",
        "\n",
        "$$\\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~...~ + b)$$\n",
        "\n",
        "Where $x_0$, $x_1$,... are the inputs. The weights, $w_0$, $w_1$, ... describe how the neuron connets to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior. Saying that multiple neurons are identical is the same thing as saying that the weights are the same.\n",
        "\n",
        "Its this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.\n",
        "\n",
        "Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, $W$:\n",
        "\n",
        "$$y = \\sigma(Wx + b)$$\n",
        "\n",
        "For example, we get:\n",
        "\n",
        "$$y_0 = \\sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 ...)$$\n",
        "\n",
        "$$y_1 = \\sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 ...)$$\n",
        "\n",
        "Each row of the matrix describes the weights connecting a neuron to its inputs.\n",
        "\n",
        "Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.\n",
        "\n",
        "<a href=\"https://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY-W.png\"><img src=\"https://drive.google.com/uc?export=view&id=1mewFQgdq4v_OTJGSYoo8H4kQylFwJ7ok\" width=400 heigth=400></a>\n",
        "\n",
        "Which corresponds to the equations:\n",
        "\n",
        "$$y_0 = \\sigma(W_0x_0 + W_1x_1 -b)$$\n",
        "$$y_1 = \\sigma(W_0x_1 + W_1x_2 -b)$$\n",
        "\n",
        "So while, normally, a weight matrix connects every input to every neuron with different weights:\n",
        "\n",
        "$$W = \\left[\\begin{array}{ccccc} \n",
        "W_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\\\\n",
        "W_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\\\\n",
        "W_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\\\\n",
        "W_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\\\\n",
        "...     &   ...   &   ...   &  ...    & ...\\\\\n",
        "\\end{array}\\right]$$\n",
        "\n",
        "The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons dont connect to many possible inputs, theres lots of zeros.\n",
        "\n",
        "$$ W = \\left[\\begin{array}{ccccc} \n",
        "w_0 & w_1 &  0  &  0  & ...\\\\\n",
        " 0  & w_0 & w_1 &  0  & ...\\\\\n",
        " 0  &  0  & w_0 & w_1 & ...\\\\\n",
        " 0  &  0  &  0  & w_0 & ...\\\\\n",
        "... & ... & ... & ... & ...\\\\\n",
        "\\end{array}\\right]$$\n",
        "\n",
        "\n",
        "Multiplying by the above matrix is the same thing as convolving with $[...0, w_1, w_0, 0...]$ The function sliding to different positions corresponds to having neurons at those positions.\n",
        "\n",
        "What about two-dimensional convolutional layers?\n",
        "\n",
        "<a href=\"https://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv2-5x5-Conv2-XY.png\"><img src=\"https://drive.google.com/uc?export=view&id=1XDZLeJwT5996Mdx2VKnEF6brjqQ_qn24\" width=400 heigth=400></a>\n",
        "\n",
        "The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.\n",
        "\n",
        "Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stride and padding\n",
        "\n",
        "\"Step size\" (number of pixels of picture, in our case) of filters, the so called _stride_ crucial parameter.\n",
        "\n",
        "- **Pixels at the edges** of the picture taking part in fewer convolutions: nothing to be convolved on one of their sides. \n",
        "- Effect can be **mitigated by _\"padding\"_ input with virtual pixels**. \n",
        "- Most widespread padding is **_zero-padding_**: fill up the space with zeros, but other solutions like \"warping\" \"clamping\" and \"mirroring\" also exist. (Further reading [here](https://www.cs.toronto.edu/~urtasun/courses/CV/lecture02.pdf))\n",
        "\n",
        "<a href=\"https://adeshpande3.github.io/assets/Pad.png\"><img src=\"https://drive.google.com/uc?export=view&id=1unDQ1OgVvKqo9GgFcBH5TiNJ7MYa5XSW\" height=\"300\"></a>\n",
        "\n",
        "With careful choice of padding and stride **we can make the successive layers of the network constant in width**:\n",
        "\n",
        "**The new representation, the _\"activation map\"_** calculated as follows:\n",
        "\n",
        "Let the picture be of size $W\\times W$, the size of the filter (_\"receptive field\"_) $F\\times F$, stride $S$ and padding $P$. The resulting activation map is then:\n",
        "$O=(W-F+2P)/S+1$. \n",
        "\n",
        "\n",
        "For example on a 7x7 picture we convolve a 3x3 filter with stride 1 and 0 padding, we get a $5\\times 5$- output. If the stride is 2 pixels, get $3\\times 3$ as output. "
      ],
      "metadata": {
        "id": "P9hzXsrgOM1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weight sharing\n",
        "\n",
        "- Can also look at the weights in above example, as $5\\times 5$ neuron forming the new representation.\n",
        "- Major difference from fully connected layers, since in case of CNNs **all neurons have the same weights**. \n",
        "- Called _weight sharing_.\n",
        "- Different neurons receive inputs from different parts of the picture. \n"
      ],
      "metadata": {
        "id": "LKsAlPk_OPAs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJsxgWTyo59b"
      },
      "source": [
        "#### Depth in the case of 2d convolutions: working with multiple channels\n",
        "\n",
        "- The 2d convolution operation can be extended to handle multiple two-dimensional channels.\n",
        "- At the end always get a scalar value. \n",
        "- Including the color channels (RGB), the picture can be described as a $W\\times W \\times D$ 3D tensor\n",
        "- **\"Depth\" equals number of channels**\n",
        "- If we follow reasoning, we can stack layers of activation maps on top of each-other\n",
        "- We again get multi-channel input at the next layer, like:\n",
        "\n",
        "\n",
        "<a href=\"http://deeplearning.net/tutorial/_images/cnn_explained.png\"><img src=\"https://drive.google.com/uc?export=view&id=1qAnq_tnXW6-J5Q8MIOqkT5608Uya_RL0\" height=\"300\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2oieymKo59b"
      },
      "source": [
        "#### 3 (and more) dimensional convolutions\n",
        "The convolution concept has natural extensions to 3 and more dimensions: in these cases the high-dimensional filters cover a small fraction of the (high-dimensional) input volume and are moved along all dimensions to provide (high-dimensional) feature maps. Similarly to the 2d case, the processing typically involves multi-channel inputs, and, consequently the typical inputs and outputs for $n$-dimensional convolutional layers are $n+1$-dimensional tensors.\n",
        "\n",
        "<a href=\"https://miro.medium.com/max/1610/1*wUVVgZnzBwYKgQyTBK_5sg.png\"><img src=\"https://drive.google.com/uc?export=view&id=1WrPjv-zUkiCjN_cRtOEcqYn_xH6l1zlH\" width=50%></a>\n",
        "\n",
        "3d convolutions can be used for\n",
        "+ video analysis (time is an additional dimension) (see e.g. [3D Convolutional Neural Networks for Human Action Recognition](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.442.8617&rep=rep1&type=pdf))\n",
        "+ 3d \"still image\" analysis (see e.g. [Convolutional Neural Network for 3D Object\n",
        "Recognition using Volumetric representation](http://www.mva-org.jp/Proceedings/2017USB/papers/04-22.pdf)), there is even a [\"3d MNIST\" data set](https://www.kaggle.com/daavoo/3d-mnist/data#full_dataset_vectors.h5).\n",
        "\n",
        "Finally, 4d convolutions can be used, e.g, for analyzing 3d videos (see e.g., [4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks](https://arxiv.org/pdf/1904.08755.pdf))."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Connection between hyperparameters\n",
        "\n",
        "Input size: $W_1 \\times H_1 \\times D_1$  (width, length, depth - we were using rectangular inputs above, but that is not necessary)\n",
        "\n",
        "Hyperparameters:\n",
        " - Number of filters: K\n",
        " - Filter size: F\n",
        " - Stride: S\n",
        " - Padding: P\n",
        " \n",
        " Output: $W_2\\times H_2 \\times D_2$,\n",
        "  where \n",
        "  \n",
        "  - $W_2 = (W_1-F-2P)/S+1$\n",
        "  - $H_2 = (H_1-F-2P)/S+1$\n",
        "  - $D_2 = K$\n",
        "  \n",
        "  \n",
        "  Total parameters for a layer: $(F\\times F\\times D_1 +1)*K$, where 1 is representing the bias. \n",
        "  \n",
        "  (In case of fully connected layers it is $W_1 \\times H_1 \\times D_1 \\times W_2 \\times H_2$ parameters, which is by order of magnitude larger...)\n",
        "  \n",
        " \n",
        "#### Comparison to fully connected layer\n",
        "\n",
        "<html>\n",
        "<head>\n",
        "<style>\n",
        "table {\n",
        "    font-family: arial, sans-serif;\n",
        "    border-collapse: collapse;\n",
        "    width: 100%;\n",
        "}\n",
        "\n",
        "td, th {\n",
        "    border: 1px solid #dddddd;\n",
        "    text-align: left;\n",
        "    padding: 8px;\n",
        "}\n",
        "\n",
        "tr:nth-child(even) {\n",
        "    background-color: #dddddd;\n",
        "}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<h2>Connection between sub parts</h2>\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>MLP</th>\n",
        "    <th>Convolutional network</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><a href=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1kM0eYXqJMMeVPxe6IWWqZEnkMy7pTuTa\" height=\"300\"></a></td>\n",
        "    <td><a href=\"http://cs231n.github.io/assets/cnn/cnn.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1YVFC030nMTmt5aj_ScIih7-iSnbAaRZH\" height=\"300\"></a></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "Left side: traditional 3 layer network. \n",
        "Right side: ConvNet, where we arrange inputs in 3 dimensions:\n",
        "- Output of filters for each part of picture single scalar, values form an activation map for the given filter over the whole image. \n",
        "- Maps stacked on top of each-other to get to 3D representation, forming the input for the next layers. \n",
        "- In case of pictures, RGB input can be considered 3D by default. [Source](http://cs231n.github.io/convolutional-networks/)\n"
      ],
      "metadata": {
        "id": "CfaS1SkjOUrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convolution summary: filters, stride, padding and channels\n",
        "\n",
        "**Some typical choices** for filter, stride, padding:\n",
        "\n",
        "* Typical filter (kernel) sizes: 2x2, **3x3**, 5x5 (-> look for truly local relationships)\n",
        "  * Note: for RGB (3-channel) images, filter sizes are: ...x3\n",
        "* Typical stride: 1 (-> overlap)\n",
        "* Typical padding: 0-padding\n",
        "\n",
        "**Filters (kernels) and channels:**\n",
        "* Input image:\n",
        "  * grayscale image = WxH = 1 channel (2D)\n",
        "  * RGB image = WxHx3 = 3 channels (3D)\n",
        "* 1 filter -> 1 map = same WxH dimension as input image, but just 1 \"channel\".\n",
        "  * The filters are the weights! (3x3 filter -> 9 weights.)\n",
        "* $n$ filters (= $n$ \"features\" we look for) -> $n$ maps -> **$n$ new \"channels\" for next convolutional layer**.\n",
        "  * For CNN, this $n$ is the unit number to specify!\n",
        "  * One filter is \"copy-and-pasted\" multiple times over the image (with shared weights!!!) to emulate convolution (instead of a \"for\"-cycle). But we don't have to worry about that / specify it. <- Calculated from filter, stride, padding."
      ],
      "metadata": {
        "id": "rIa9aifkiYEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pooling\n",
        "\n",
        "- Follow logic of \"detectors\" in case of convolutional filters\n",
        "- Accept premise that there is a compositional hierarchy of object parts\n",
        "\n",
        "$\\Rightarrow$ Most relevant information **whether filter detected certain pattern somewhere in the region** of the input picture. \n",
        "\n",
        "**Exact location is not important**, only the fact that there is \"strong\" (confident) signal.\n",
        "\n",
        "Following this logic additional _pooling layer_ introduced in CNNs:\n",
        "- Downsampling strategy, **reducing the dimensionality of the inputs** for a next convolutional layer, \n",
        "- **Aiding the \"invariant behavior\"** of the network as well as a hierarchic form of compressing.\n",
        "\n",
        "Different pooling strategies exist (like \"average pooling\"), but most widespread form is **_max pooling_**:\n",
        "\n",
        "- The pooling layer does **no learning**, has no weights. \n",
        "- For each $k\\times k$  input a single value which is the $maximum$ of activation in that region.\n",
        "- In case of input with dimensions $N\\times N$ and $k$ stride it will  output a $ \\frac{N}k \\times\\frac{N}k$ layer, as each $k\\times k$ has an output of only a scalar because of the application of the $max$ function.\n",
        "\n",
        "<a href=\"https://adeshpande3.github.io/assets/MaxPool.png\"><img src=\"https://drive.google.com/uc?export=view&id=15jl2DTDmapH4wdng3CaiTTL0DJwdcFxS\" width=400 heigth=400></a>\n",
        "\n",
        "Some more justification for this procedure can be that in a certain local region the presence of a feature is unlikely be \"double\", unless the picture is somehow blurry, so we only need a decision of maximal presence.  Additional resources [here](http://yann.lecun.com/exdb/publis/pdf/boureau-icml-10.pdf).\n"
      ],
      "metadata": {
        "id": "Xj_sKMRQOg6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQkjjsi0hvt8KfTaNyzeOnHJAe-B_D8cjJoQ6hHmEr7Gip686uToGLMr3fxbJoG6o4iWw&usqp=CAU\" width=700>"
      ],
      "metadata": {
        "id": "fvDGarSDgtS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fully-connected layers\n",
        "\n",
        "Remember: after the series of conv-pool modules, we still need fully-connected (\"FC\") layers, and the usual output layer!\n",
        "\n",
        "(Often these few FC layers require the most weights in the network to train!)\n",
        "\n",
        "(In modern networks, batchnorm and dropout layers are also included.)"
      ],
      "metadata": {
        "id": "Fn9TcjaDm5BD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egQH_Ihdo59c"
      },
      "source": [
        "### Sidenote: some disadvantages of CNN\n",
        "\n",
        "Recently the researchers at Uber's AI lab started to investigate the failings of ConvNets, especially their **inability to solve tasks requiring exact coordinates**, for example dealing with single pixel based tasks (like given a coordinate pair, the network should output a singel pixel of white at that coordinate). The basic convolutional architectures fail miserably, thus implying that **we laid too much emphasis on invariances**, and with it, crippled the ability of the networks to use accurate coordinates.\n",
        "\n",
        "<a href=\"https://eng.uber.com/wp-content/uploads/2018/07/image15.png\"><img src=\"https://drive.google.com/uc?export=view&id=1KskYAWOPHmfu8Y1aDS99MZz5yWdkmOXL\" width=300 heigth=300></a>\n",
        "\n",
        "The researchers propose a simple extension of connectivity, thus a new architecture they call [\"CoordConv\"](https://eng.uber.com/coordconv/) to solve this isssue.\n",
        "\n",
        "<a href=\"https://eng.uber.com/wp-content/uploads/2018/07/image8-768x321.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1jTSwprNTUqMbnbycAmkK1T1VY-L1KKM6\" width=400 heigth=400></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iulmk1K8o59c"
      },
      "source": [
        "<a id=\"specarch\"></a>\n",
        "# Specific architecture examples\n",
        "\n",
        "After the initial success of ConvNet a _huge_ amount of development went into refining techniques and architectures that fueled the rapid development of the visual processing field, which surpassed (on certain tasks!!!) human performance.\n",
        "\n",
        "<a href=\"https://pbs.twimg.com/media/DDwtxn6U0AADbz0.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1AnIfao6NY3anOIoKwAPbEKGYnHBytwea\" heigth=400></a>\n",
        "\n",
        "One obvious source of development was, that with the \"tricks\" discussed before, as well as the increase of computation power the number of layers inside modern image recognition models also rises rapidly.\n",
        "\n",
        "<a href=\"https://www.researchgate.net/profile/Kien_Nguyen26/publication/321896881/figure/fig1/AS:573085821489153@1513645715549/The-evolution-of-the-winning-entries-on-the-ImageNet-Large-Scale-Visual-Recognition.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Q4saXOkq7Q7kshqAwcVQ36aIlArzV2OW\" height=400></a>\n",
        "\n",
        "[Source](https://www.researchgate.net/figure/The-evolution-of-the-winning-entries-on-the-ImageNet-Large-Scale-Visual-Recognition_fig1_321896881)\n",
        "\n",
        "But some structural modifications were also necessary for these results to become achiveable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4qJAh_LzdcP"
      },
      "source": [
        "## LeNet\n",
        "\n",
        "[Original paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
        "\n",
        "This is the original ConvNet by LeCun et al. with the first implementation of the conv, pool and FC layers used for character recognition.\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1Yw1NECPVDmrO2a3JoPwJVvnsNU8BU1Ul\"><img src=\"https://drive.google.com/uc?export=view&id=1BS6SG7AuFU5kXQoMtYEAk4G1X2Dq65kP\" height=\"500\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMFPmvcGo59d"
      },
      "source": [
        "Some notable curiosities from today's perspective:\n",
        "\n",
        "+ Neurons were not always connected to all input channels\n",
        "+ The weights of the penultimate FC layer (F6) were initialized on the basis of manually created 7x12 images of \"ideal digits\", and the \"Gaussian connections\" actually measured the Eucledian distance between these images and the current activation..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11d4hcgAtKNZ"
      },
      "source": [
        "---------------------------------\n",
        "## AlexNet\n",
        "\n",
        "Papers:\n",
        "\n",
        "- [Here](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
        "- [and here](https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html)\n",
        "\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1600/1*qyc21qM0oxWEuRaj-XJKcw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1JVfFD1XnLzMZ3wTxgVhMCd0GzQwd6Vc4\" height=\"300\"></a>\n",
        "\n",
        "\n",
        "Winner of the \"ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) 2012\", 37.5% top-1 and 17.0% top-5 error rate, 650000 neuron, 60 million parmeters.\n",
        "\n",
        "### Main ideas:\n",
        "- **RELU in all layers**\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=14p7awodIcVsqEj-N2buGty74uz2doXuA\"><img src=\"https://drive.google.com/uc?export=view&id=1Q6PVEcGpdW0tpKov3fJnwA1qaCx564H3\" height=\"500\"></a>\n",
        "\n",
        "- **Parallel processing on two GPUs**\n",
        "\n",
        "The processing is done in two separate channels. This \"innovation\" came to be because of the fact that GPU memory was extremely limited back then, and the only way to train a bigger model was to \"divide it by half\". None the less a mechanic had to be implemented that ensured communication between the two channels at some layers.\n",
        "\n",
        "- **Local response normalization**\n",
        "\n",
        "As a linear form of \"inhibition\", we divide the activation of a neuron at a position with a scalar that depends on the activations of other neurons in the same position. It has some faint connections to batch norm, and resulted in 1.2-1.4% gain in accuracy. This is also called \"brightness normalization\" and fell out of favour recently.\n",
        "\n",
        "- **Overlapping pooling**\n",
        "\n",
        "Stride is smaller than the pooling filter. This caused 0.3-0.4% increase in accuracy.\n",
        "\n",
        "- **Techniques against overfitting**\n",
        "\n",
        "    - Data augmentation\n",
        "    - DropOut (only on the last FC layers - it doubled the number of training iterations none the less for convergence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6R0L3U0tQqB"
      },
      "source": [
        "-----------------------------\n",
        "## VGG (Simonyan and Zisserman, Visual Geometry Group, University of Oxford)\n",
        "\n",
        "Sources:\n",
        "- https://arxiv.org/abs/1409.1556\n",
        "- http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
        "- https://github.com/machrisaa/tensorflow-vgg\n",
        "- https://github.com/tensorflow/models/blob/master/research/slim/nets/vgg.py\n",
        "- https://gist.github.com/omoindrot/dedc857cdc0e680dfb1be99762990c9c\n",
        "\n",
        "<a href=\"http://www.hirokatsukataoka.net/research/cnnfeatureevaluation/cnnarchitecture.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=18zYv9l6Kyd3HqirrZGPvnMfUZknLFCnp\" width=600 heigth=600></a>\n",
        "\n",
        "Second place in 2014 ImageNet. It had 16 convolutional and 3 FC layers 3x3 convolutional and 2x2 pooling filters. The model's appeal was its simplicity, though the FC layers at the end made it quite demanding in computational terms. Decreasing FC layers was later found to mitigate this problem without meaningful loss of performance.\n",
        "\n",
        "Main ideas:\n",
        "\n",
        "- more convolutional layers\n",
        "- only small, but overlapping filters (3x3, stride=1)\n",
        "- no local response normalization\n",
        "- Sampling from multi-scale pictures\n",
        "- usage of 1x1 convolutional layers\n",
        "- ensemble of multiple models (2 made a large improvement, no need for large ensembles)\n",
        "\n",
        "VGG is still a widely used architecture - especially in transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XK31BL7tNrD"
      },
      "source": [
        "---------------------------------------------\n",
        "## GoogLeNet-Inception models\n",
        "\n",
        "Winner of ILSVRC 2014 in detection and classification also, model of the Google team.\n",
        "\n",
        "**Sources:**\n",
        "- https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf\n",
        "- https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/googlenet.html\n",
        "\n",
        "\n",
        "**Design objective:**\n",
        "Lower level of energy and memory consumption\n",
        "\n",
        "**Problems:**\n",
        "1. More layers mean higher precision, but too many parameters (overfitting, training time, computational power,...)\n",
        "2. ConvNet libraries (as well as general numeric libraries and hardware) is designed to deal with dense matrices, but computer vision representations work with sparse distributions\n",
        "\n",
        "**Ideas:**\n",
        "\n",
        "1. idea, based on: M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, abs/1312.4400, 2013. Using **1x1 convolutions, increasing depth but not increasing dimensions**. Concatenating this structure repeatedly resulted in GoogLeNet\n",
        "\n",
        "2. idea: Inspired by biology, trying to reach scalar-invariance by applying **variable size filters in parallel.** (This was also present in the classical model of Serre, HMAX just with non-learned filters.) Optimally we would work with many filters that are only sparsely active.\n",
        "\n",
        "Inception model combines the two ideas, it does not enforce sparsity explicitly, but approximates it instead.\n",
        "\n",
        "The building blocks, \"inception modules\" are composed of different size pooling and convolutional filters, their output is concatenated to become the input for the next layer. The ratio of different size filters changes as we get further from the original input, since we can expect to see lower and lower levels of spatial correlations in the higher levels of representation. The number of parameters is controlled by 1x1 dimension reduction convolutions:\n",
        "\n",
        "<a href=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/inception_1x1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1TlXaTrrr5B5sTspLUavQ1YExfJCH7pqm\" width=1000></a>\n",
        "\n",
        "The structure of GoogLeNet:\n",
        "\n",
        "<a href=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/GoogleNet.png\"><img src=\"https://drive.google.com/uc?export=view&id=1PBbKbWtZus7RFqpZQvZ9gi3JCy6q77U-\" width=1000 ></a>\n",
        "\n",
        "Because of memory constraints, in the first two layers they used \"classical\" conv and pool layers, then multiple inception modules were chained after each-other. Finally they used _avg pooling_ (instead of FC layers), dropoutot and softmax. If we consider a module one \"layer\", the number of operators with parameters was 22, all in all the number of layers was 100\n",
        "\n",
        "3. idea: Put **external classifiers on some of the intermediate layers**, this contributes to the error minimized during training. This ensures that **strong gradient flow is maintained in bottom layers** also. (The motivation came from the fact that not so deep models work also, so there is a gradient signal that can be extracted from them.) During testing, the auxillary classifiers are not used.\n",
        "\n",
        "4. idea: The usage of **photometric distortion** help the model becoming invariant against the properties of the photos taking (camera, detector type,...) [soruce](https://arxiv.org/abs/1312.5402)\n",
        "\n",
        "5. idea: They taught 7 (6) identical models on subsets of the training data, and finally made an **ensemble** of them. (Ensembling always helps a bit. :-( )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8btKTu-5CnGD"
      },
      "source": [
        "---------------------------------------------\n",
        "## Highway Networks\n",
        "\n",
        "**Soruces:**\n",
        "- https://arxiv.org/abs/1505.00387\n",
        "- https://web.archive.org/web/20190528021739/http://people.idsia.ch:80/~rupesh/very_deep_learning/\n",
        "- https://arxiv.org/abs/1507.06228\n",
        "- https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa\n",
        "- https://github.com/wujysh/highway-networks-tensorflow\n",
        "\n",
        "\n",
        "**Problem:**\n",
        "\n",
        "The teaching of very deep networks is difficult (unsurprisingly) \n",
        "\n",
        "**Idea:**\n",
        "\n",
        "The flow of information could be enhanced by passing through **the original as well as the transformed input**, thus the signal is essentially not weakened during the process. (And by the way, gradients can flow backwards more easily!). This general idea is also called a skip connection, although highway networks are just one particular way of implementing skip connections.\n",
        "\n",
        "$y=H(\\mathbf{x}, \\mathbf{W_H})\\cdot T(\\mathbf{x}, \\mathbf{W_T})+ \\mathbf{x}\\cdot C(\\mathbf{x}, \\mathbf{W_C})$, \n",
        "\n",
        "where T is the \"transform\" gate, C is the \"carry over\" gate. \n",
        "\n",
        "Most simple form is when \n",
        "\n",
        "$C()=1-T()$, \n",
        "\n",
        "thus \n",
        "\n",
        "$y=H(\\mathbf{x}, \\mathbf{W_H})\\cdot T(\\mathbf{x}, \\mathbf{W_T})+ \\mathbf{x}\\cdot (1-T(\\mathbf{x}, \\mathbf{W_T}))$\n",
        "\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=17asvsfH4S6O2TVmE8dLQBF7uhJmalCNt\"><img src=\"https://drive.google.com/uc?export=view&id=1-N3GS0iZG4oBhIJUeoADgEsHXRuQtWe9\" width=500 height=500></a>\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1ab6Z6Sr8_tc30s6ZtvaBy-f4ycdyXWOI\"><img src=\"https://drive.google.com/uc?export=view&id=1z9JmaYDBPxEVkhaEyhFbhFR55l6ddxsc\" width=500 height=500></a>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93JL4amjuX5W"
      },
      "source": [
        "------------------------------------------\n",
        "## ResNet, Wide ResNet, ResNeXt\n",
        "\n",
        "\n",
        "**Sources:**\n",
        "- https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet\n",
        "- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/resnet.py\n",
        "- https://arxiv.org/abs/1512.03385\n",
        "- https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf\n",
        "- https://arxiv.org/pdf/1602.07261.pdf   combining Inception and ResNet leads to quicker training and higher accuracy\n",
        "- https://medium.com/@waya.ai/deep-residual-learning-9610bb62c355\n",
        "\n",
        "\n",
        "Originally a result of Microsoft Research Asia, the lead author, Professor He is now at Facebook AI...\n",
        "\n",
        "ResNets\t@\tILSVRC\t&\tCOCO\t2015\twinner in all categories!\n",
        "- ImageNet\tClassification:\tUltra-deep\t152-layer nets\t\n",
        "- ImageNet\tDetection: 16% better\tthan\t2nd\n",
        "- ImageNet\tLocalization: 27% better\tthan\t2nd\n",
        "- COCO\tDetection: 11% better\tthan\t2nd\n",
        "- COCO\tSegmentation: 12% better\tthan\t2nd\n",
        "\n",
        "<a href=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/ImagenetTable.png\"><img src=\"https://drive.google.com/uc?export=view&id=14ySyXWjsbWTKmOC5_8Lo6VX56XvZGkoh\" width=500></a>\n",
        "\n",
        "\n",
        "### ResNet vs Highway networks\n",
        "\n",
        "Motivation:\n",
        "\n",
        "Depth has a limit: \"degradation problem\"\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1g9laFRfDIdW0Bl9PC58et2z9DqyB5DNf\"><img src=\"https://drive.google.com/uc?export=view&id=16nhnu5Z-nfKzhx8WR_xWTyFQsSrDRr8I\" width=800 ></a>\n",
        "\n",
        "\n",
        "You need more and more tricks to keep up the gradient flow in case of deeper networks.\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=141aDrpQAtXdN8HLKg3eopbO3RB7YSXeo\"><img src=\"https://drive.google.com/uc?export=view&id=1NUEvCVv3uJ8IRq_OEH6MjbjkNAXUKM3M\" width=600 ></a>\n",
        "\n",
        "\n",
        "ResNets and Highway networks time to realise the same goal, so similar concept, different realization:\n",
        "\n",
        "<a href=\"https://www.researchgate.net/publication/311842587/figure/fig1/AS:442294890438658@1482462727725/Illustrating-our-usage-of-blocks-and-stages-in-Highway-and-Residual-networks-Note-also.png\"><img src=\"https://drive.google.com/uc?export=view&id=1HWX0Jq4d9kMiPaz1ZyEiwx0iKTlkow49\" width=600 ></a>\n",
        "\n",
        "[Source](https://www.researchgate.net/publication/311842587_Highway_and_Residual_Networks_learn_Unrolled_Iterative_Estimation)\n",
        "\n",
        "Both solutions try to implement a \"shortcut\" in depth, though in case of Highway networks it is done through a \"gating function\". The gates are dependent on the input and other parameters. If the value of the gate converges to 0, it does not learn a residual mapping (so it is in essence a variable amount of \"residuality\"), while ResNet always operates on the basis of residuals. It is not clear that a Highway net can always capiatalize on the increased depth.\n",
        "\n",
        "The big idea of ResNets is to learn the simplest of \"transformations\", namely _identity mapping_, that is we introduce \"skip connections\", so that the old mapping is the \"residual\" part of the new mapping. (It had some precursors in [VLAD (Vector of Locally Aggregated Descriptors](http://www.vlfeat.org/api/vlad-fundamentals.html).)\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=17vkfFKkbNBxtc4v_tqM9GFRonE-hbPNw\"><img src=\"https://drive.google.com/uc?export=view&id=12Aoxyj1PiCqoxcP48UH_jEe37SoMo9Ot\" width=600 ></a>\n",
        "\n",
        "Introduction of identity transformation helps optimization.\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1NWGv-2LtzBHZODghTTJsVlccye3CaEpT\"><img src=\"https://drive.google.com/uc?export=view&id=19bjWrVN3nK3L8W6Yuu0HkvRiVTeSGo8c\" width=600 ></a>\n",
        "\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1MLyeQ6fpeQbLn71jmuclDn0ECO5i2utC\"><img src=\"https://drive.google.com/uc?export=view&id=1ei1f6Ss85qKwvw8_Ww8MGF55G7DT_PCK\" width=600 ></a>\n",
        "\n",
        "\n",
        "Building blocks of original ResNet, \"improved\" and \"generalized to more layers\":\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1dWnfQSp0JBR1jLQ-iPXzBlB9V-vQ1FqU\"><img src=\"https://drive.google.com/uc?export=view&id=1-rDLrJnJuTAJCOWD_DTwiB3raEsBS79d\" width=800 ></a>\n",
        "\n",
        "The difference lies in the transformations used. In smaller networks they use non-linearities (ReLU or ReLU+Batchnorm), and in bigger networks identity.\n",
        "\n",
        "The whole network is made up of such units, but before the classification there are no FC layers, only average pooling.\n",
        "\n",
        "\n",
        "The bottleneck structure for reducing parameters is useful in big tasks:\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=19W-y5pAJCNTPO0iRzb5tRfguUj3P1Jq_\"><img src=\"https://drive.google.com/uc?export=view&id=1NSD0G8n3FQpHa2KoSktXcdvwgXlVWAw0\" width=800 ></a>\n",
        "\n",
        "\n",
        "Two ResNet components are:\n",
        "- basic:two convolutional layers, 3x3 filters, batch normalization and ReLU\n",
        "- bottleneck: dimensionality reduction and 1x1 expanding convolutional layer and a 3x3 conv layer inbetween\n",
        "\n",
        "\n",
        "**Results:**\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1Wp9eO5k0KV8BPRhdqx89I2CtCkAlb6KM\"><img src=\"https://drive.google.com/uc?export=view&id=13NwIaUb2JHspoaMoMK4J_pFKggBm7RTk\" width=800 ></a>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bupmpKsGG4AL"
      },
      "source": [
        "## Wide ResNet\n",
        "\n",
        "[**Source**](https://arxiv.org/abs/1605.07146)\n",
        "\n",
        "The introduction of identity mapping was the key in ResNets but with enough layers, the so called \"diminishing feature reuse\" problem arises, that is, the information is preferredly flowing through the skip connections and nothing is learned on the other branch.\n",
        "\n",
        "\n",
        "**Ideas:**\n",
        "- Changing the order of operators: BN->ReLU->conv, results in faster training\n",
        "- Since the bottleneck layer is \"narrow\", and the goal is to build wide nets, they have left it out\n",
        "- Increase of representation power\n",
        "    - more layers\n",
        "    - more conv layers in one ResNet module\n",
        "    - change of receptive fields of filters (smaller filters are generally better\n",
        "    - Widening: more filters inside the conv layer of the module\n",
        "- Widening (more filters) allowed for better GPU utilization, drastic speed increase in comparison to classic ResNet\n",
        "- Batch Norm instead of DropOut in convolutional layers\n",
        "\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1txjchc8PNT9_UGxNYMdAQaG39Pe7e3UO\"><img src=\"https://drive.google.com/uc?export=view&id=1mJ8SF3NxyTwIzZkK1awSu8wQKhyRLEkV\" width=500 height=500></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_EOtKEtuzXZ"
      },
      "source": [
        "------------------------------------------------------\n",
        "## DenseNet\n",
        "\n",
        "**Sources:** \n",
        "- https://arxiv.org/abs/1608.06993\n",
        "- https://github.com/liuzhuang13/DenseNet\n",
        "- https://github.com/YixuanLi/densenet-tensorflow\n",
        "\n",
        "**Problem:** \n",
        "- Many other models were experimenting with \"shortening the paths\", but there were many variants for it\n",
        "\n",
        "**Idea:**\n",
        "- What if we enable connection between all layers, the diverse filters together increase representation potential?\n",
        "- Few filters per layer, further compression between \"dense\" blocks\n",
        "\n",
        "\n",
        "<a href=\"https://cloud.githubusercontent.com/assets/8370623/17981494/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1N8XaHD8JKjxsJsGByuKNMu-7K0anJnmM\" width=500 height=500></a>\n",
        "\n",
        "<a href=\"https://cloud.githubusercontent.com/assets/8370623/17981496/fa648b32-6ad1-11e6-9625-02fdd72fdcd3.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1cvhze7FOILIr3ZxOvBB89m18kwgZ1Y-o\" width=700 height=500></a>\n",
        "\n",
        "Access to earlier feature maps is implemented using concatenation:\n",
        "\n",
        "<a href=\"https://miro.medium.com/max/600/1*9ysRPSExk0KvXR0AhNnlAA.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1bYdt8_ldmZaA15C4xZrFWxoNrTL6-YOj\"></a>\n",
        "\n",
        "(Image source: [Review: DenseNet  Dense Convolutional Network (Image Classification)](https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803))\n",
        "\n",
        "**Comparison with other models:**\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1av2mGDY1aH0GAq7bpyjwdA9sN7d2C09u\"><img src=\"https://drive.google.com/uc?export=view&id=1Hn-M7ZHk23Qwi-Ap616mmLxjA1frzEaU\" width=500 height=500></a>\n",
        "\n",
        "**Further reading**\n",
        "\n",
        "A nice, readable overview can be found here: [Review: DenseNet  Dense Convolutional Network (Image Classification)](https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K17cXf-o59g"
      },
      "source": [
        "<a id=\"visualization\"></a>\n",
        "# CNN Visualization\n",
        "\n",
        "As the convolutional networks got deeper and deeper, it became an important question to investigate what these networks learn, how do the successive layers of representation look like in practice, or is there any point in adding layers after all.\n",
        "\n",
        "For this purpose it became a growing field of experimentation to try to visualize the inner workings of CNNs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUWPK8OFhPoh"
      },
      "source": [
        "-----------------------------------------\n",
        "## ZFNet - the first tries\n",
        "\n",
        "[Source](https://arxiv.org/abs/1311.2901)\n",
        "\n",
        "Zeiler and Fergus created this winning model in 2013 by the in-depth analysis of AlexNet (and thus employing smaller filters and smaller strides in the first layers), but their most important contribution was the introduction of a \"deconvnet\" for visualization of intermediate representations (Zeiler, M., Taylor, G., and Fergus, R. Adaptive deconvolutional networks for mid and high level feature learning. In ICCV, 2011.)\n",
        "\n",
        "### The main idea: Deconvolution\n",
        "\n",
        "Deconvolution operator - or maybe better to say \"transposed convolution\" - operator is used to lossfully reconstruct the original representation before a convolutional layer.\n",
        "\n",
        "<a href=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_transposed.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1ogWtuHi_PZqt4Tpcl8SgpmEPAN4zuE8m\" width=400 heigth=400></a>\n",
        "\n",
        "- **Unpooling:** In the convnet, the max pooling operation is non-invertible, however we can obtain an approximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. \n",
        "- **Rectification:** The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity.\n",
        "- **Filtering**: The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally\n",
        "\n",
        "\n",
        "\n",
        "### Deconvnet structure for visualization:\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1BprM4pxtQHLABOjPiegMeZuucBhZGjEa\"><img src=\"https://drive.google.com/uc?export=view&id=12u2LJ4JXLadYv3mi8_Mpo0iof04Z89C6\" width=300 height=300></a>\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1600/1*KyfQTpv1hYDg8ABXNt0FVg.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1EzZ4b7NRmnGDuIhpq3L4rPbbKXn18tT4\" width=400 heigth=400></a>\n",
        "\n",
        "\n",
        "Projections of activations layer by layer into pixel space:\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1aB5vreWKi3eFekLywq9hZF8Zbgs_frtE\"><img src=\"https://drive.google.com/uc?export=view&id=13J1Di2offEFoTwClp4ND6uLUFvCL1Bc4\" width=700 height=700></a>\n",
        "\n",
        "The projections from each layer show the hierarchical nature of the features in the network. Layer 2 responds to corners and other edge/color conjunctions.Layer 3 has more complex invariances, capturing similar textures (e.g. mesh patterns (Row 1, Col 1); text(R2,C4)). Layer 4 shows significant variation, but is more class-specific: dog faces (R1,C1); birds legs (R4,C2). Layer 5 shows entire objects with significant pose variation, e.g. keyboards (R1,C11) and dogs (R4)\n",
        "\n",
        "A nice summary of the ZFnet paper can be found [here](https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLS9vX43o59g"
      },
      "source": [
        "\n",
        "## Methods for visualization\n",
        "\n",
        "Based on [\"Visualizing what ConvNets learn\"](https://cs231n.github.io/understanding-cnn/)\n",
        "\n",
        "### Visualization of activations in early layers\n",
        "**Layer Activations**. The most straight-forward visualization technique is to show the activations of the network during the forward pass. For ReLU networks, the activations usually start out looking relatively blobby and dense, but as the training progresses the activations usually become more sparse and localized. One dangerous pitfall that can be easily noticed with this visualization is that some activation maps may be all zero for many different inputs, which can indicate dead filters, and can be a symptom of high learning rates.\n",
        "\n",
        "\n",
        "<a href=\"https://cs231n.github.io/assets/cnnvis/act2.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1645uvkMxnyAnW2vAEssurBpgkJ6aO1ws\" width=400 heigth=400></a>\n",
        "\n",
        "<a href=\"https://cs231n.github.io/assets/cnnvis/act1.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1ri1CpB-U8t9sj4tD9n4A23OCZj9aACr0\" width=400 heigth=400></a>\n",
        "\n",
        "Typical-looking activations on the first CONV layer (left), and the 5th CONV layer (right) of a trained AlexNet looking at a picture of a cat. Every box shows an activation map corresponding to some filter. Notice that the activations are sparse (most values are zero, in this visualization shown in black) and mostly local.\n",
        "\n",
        "### Visualization of weights\n",
        "\n",
        "**Conv/FC Filters**. The second common strategy is to visualize the weights. These are usually most interpretable on the first CONV layer which is looking directly at the raw pixel data, but it is possible to also show the filter weights deeper in the network. The weights are useful to visualize because well-trained networks usually display nice and smooth filters without any noisy patterns. Noisy patterns can be an indicator of a network that hasnt been trained for long enough, or possibly a very low regularization strength that may have led to overfitting.\n",
        "\n",
        "<a href=\"https://cs231n.github.io/assets/cnnvis/filt1.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1rRiMyxsRf-YfTo86KddYWvLc-dfE4vsi\" width=400 heigth=400></a>\n",
        "\n",
        "### Using prototypical images\n",
        "\n",
        "We can scan through the training set, and choose for every unit the picture that activates it the most, thus getting a \"prototypical\" image. \n",
        "\n",
        "<a href=\"https://cs231n.github.io/assets/cnnvis/pool5max.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1HzMgVbTAai6kBICV7r_tsdCNKsMi5iol\" width=600 heigth=600></a>\n",
        "\n",
        "\"Maximally activating images for some POOL5 (5th pool layer) neurons of an AlexNet. The activation values and the receptive field of the particular neuron are shown in white. (In particular, note that the POOL5 neurons are a function of a relatively large portion of the input image!) It can be seen that some neurons are responsive to upper bodies, text, or specular highlights.\"\n",
        "\n",
        "Interesting parallelism is, that in certain recurrrent nets (see next class) some tendency was captured, which utilizes one specific neuron to signal eg. the sentiment of a text. Thus the visualization of activations can be indeed quite insightful!\n",
        "\n",
        "<a href=\"https://openai.com/content/images/2017/04/low_res_maybe_faster.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1gLByF5-cc9mC-sASJwzD9Pbsde3wviIl\" width=600 heigth=600></a>\n",
        "\n",
        "Source: [Unsupervised Sentiment Neuron](https://blog.openai.com/unsupervised-sentiment-neuron/)\n",
        "\n",
        "### Mapping the full space with the training images\n",
        "\n",
        "We can use the CNN's inner representation vectors and put them into a common space, do a dimensionality reduction (typically t-SNE), and then place the original images at the resulting locations, thus getting a feeling about how the inner space of the CNN is structured.\n",
        "\n",
        "<a href=\"https://cs231n.github.io/assets/cnnvis/tsne.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1D3tm47hxMRiA3Q2N7tNt8bq2jSm_S0D-\" width=900 heigth=900></a>\n",
        "\n",
        "### Covering some parts of the input\n",
        "\n",
        "We can cover parts of the picture and \"convolve\" the covering area in such a way as to get a probaility value for a certain class (eg that if I cover this part, is this picture still classified as dog?).\n",
        "This can give insight about the key features used by the recognition of that given class.\n",
        "\n",
        "<a href=\"https://cs231n.github.io/assets/cnnvis/occlude.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1c-3rX8pgPYGaq3aurwyklIuPErEb7hHL\" width=400 heigth=400></a>\n",
        "\n",
        "\n",
        "_It is this are of research that led to the development of adversarial examples._\n",
        "\n",
        "### Perturbing a single pixel (closely related to covering some parts of the input\n",
        "\n",
        "Up until their logical extreme, the [\"one pixel attack\"](https://arxiv.org/abs/1710.08864).\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1kpCiineXcyOwEUOHa1tCQT6WDzbDH8PS\"><img src=\"https://drive.google.com/uc?export=view&id=17AqrL8I2nO09GyX5LyGHqyL6ZwMNs85v\" width=55%></a>\n",
        "\n",
        "- Pixel pertubations optimized with differential evolution (DE)\n",
        "- Population based optimization algorithm for solving complex multi-modal optimization problems. DE belongs to the general class of evolutionary algorithms (EA). Moreover, it has mechanisms in the population selection phase that keep the diversity such that in practice it is expected to efficiently find higher quality solutions than gradient-based solutions or even other kindsof EAs. In specific, during each iteration another set of candidate solutions (children) is generated according to the current population (parents). Then the children are compared with their corresponding parents, surviving if they are more fitted (possess higher fitness value) than their parents. In such a way, only comparing the parent and his child, the goal of keeping diversity and improving fitness values can be simultaneously achieved. DE does not use the gradient information for optimizing and therefore does not require the objective function to be differentiable or previously known. Thus, it can be utilized on a wider range of optimization problems compared to gradient based methods (e.g., non-differentiable, dynamic, noisy, among others). The use of DE for generating adversarial images have the following main advantages:\n",
        "- Higher probability of Finding Global Optima - DE is a meta-heuristic which is relatively less subject to local minima than gradient descent or greedy search algorithms (this is in part due to diversity keeping mechanisms and the use of a set of candidate solutions). \n",
        "- Require Less Information from Target System - DE does not require the optimization problem to be differentiable as is required by classical optimization methods such as gradient descent and quasi-newton methods. This is critical in the case of generating adversarial images since 1) There are networks that are not differentiable. 2) Calculating gradient requires much more information about the target system which can be hardly realistic in many cases.\n",
        "- Simplicity - The approach proposed here is independent of the classifier used. For the attack to take place it is sufficient to know the probability labels. There are many DE variations/improvements such as selfadaptive, multi-objective, among others. The current work can be further improved by taking these variations/improvements into account.\n",
        "\n",
        "And the famous reprogramming paper: [ADVERSARIAL REPROGRAMMING OF\n",
        "NEURAL NETWORKS](https://arxiv.org/pdf/1806.11146.pdf)\n",
        "\n",
        "<a href=\"https://venturebeat.com/wp-content/uploads/2018/07/Capture-boring.png?fit=400%2C312&strip=all\"><img src=\"https://drive.google.com/uc?export=view&id=1kMbFFS_Qw5q_5zQu765UqBQ6opCom8jb\" width=60%></a>\n",
        "\n",
        "Some more interesting visualization methods can be found [here](https://distill.pub/2019/activation-atlas/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EicTa8Jlo59h"
      },
      "source": [
        "# Representation learning revisited\n",
        "\n",
        "The canonical example we gave for the effectiveness of deep networks is a hierarchical application of some \"kernels\", some folding of the input space that enables good separation.\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1tQu8JagtQKjd7xVbB5uDBA0CebjQcZ2B\"><img src=\"https://drive.google.com/uc?export=view&id=116xHphwqrSbyXpKX6Acz_Krvl5taLcE1\" width=50%></a>\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1q6TEXhcZ0hU9nv4CycGcNJyUb9RqC_Xy\"><img src=\"https://drive.google.com/uc?export=view&id=1Kxm5kL-d9OFYHHZqeguv_9pedUxRc6ps\" width=50%></a>\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1UFV35b84geZTymaTKQBpLXva8efafloW\"><img src=\"https://drive.google.com/uc?export=view&id=1nhHmfEy8z59jvuIOMw6gRjmWiwkfHh3b\" width=50%></a>\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1jAyFn9iKhjSADG-YViN73goVic1x8iu5\"><img src=\"https://drive.google.com/uc?export=view&id=1iP3yPJkH9hDUbM7OsXN3ta_pW5NTO1md\" width=50%></a>\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1XSrsBdnan08LVjcVjiJwRwn6_u3HyzvA\"><img src=\"https://drive.google.com/uc?export=view&id=1d2racDbobxx3T7eREAKBhU3ssmGUkPt8\" width=50%></a>\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1Aqx6qLy9pVt1-p2IG_CM0SKLnh7-Y2cI\"><img src=\"https://drive.google.com/uc?export=view&id=1evDeC6rJV8iyqpUOpD2E0o5N1G740L9q\" width=50%></a>\n",
        "\n",
        "\n",
        "But what does this mean in practice? \n",
        "\n",
        "\n",
        "## Connection with \"representation\" techniques\n",
        "\n",
        "<a href=\"https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1IwmD6DM9gmIW6rG5UTE0HvCvVfJ_EG9e\" width=75%></a>\n",
        "\n",
        "For classical \"representation learning\" techniques we have observed, that they try to investigate a transformation of the original data space, so that it becomes more \"informative\".\n",
        "\n",
        "The baseline approach is **principal components analysis**, during which the search for linear transformations (rotations) of the representation space is guided by the **variance pattern** of the data, coming up with \"axes\" that explain the most of the original variance of the data. The drawback of this method is, that it uses **linear transformations** and **does not utilize category information** of the classification problem.\n",
        "\n",
        "Going one step further **linear discriminant analysis** tries to mitigate this second drawback by coming up with transformations that are **explicitly useful for classification**, though the constraint - as name implies - of linear separation remains.\n",
        "\n",
        "On the other hand more recent **non-linear** embedding methods as [**t-SNE**](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) (t distributed Stochastic Neighborhood Embedding) and more recently [UMAP](https://arxiv.org/abs/1802.03426) successfully relax the constraint of linear embedding and try to preserve the neighborhood structure of complex manifolds after projecting them to lower dimensionality.  \n",
        "\n",
        "<a href=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_t_sne_perplexity_001.png\"><img src=\"https://drive.google.com/uc?export=view&id=1EuZxQwCbRn04ze58HibQkaDhoRQpAP6d\" width=75%></a>\n",
        "\n",
        "(By the way, all the above visualizations of neural weights or activations were t-SNE projected to be visible in 2-3D, that is it's original use-case.)\n",
        "\n",
        "But amongs some other drawbacks (computational and generalization based) these techniques are not learning **target specific** representations of the data.\n",
        "\n",
        "\n",
        "We can state, that the best \"embedding\" of a set of data is through it's **salient features with respect to a given problem**.\n",
        "\n",
        "And we can demonstrate, that convnets are exactly doing this:\n",
        "\n",
        "## They DO learn features hierarchies!\n",
        "\n",
        "How do we know? - We, use the visualization techniques to track it!\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1kH7OYoulhmwPbGeIU_gpBNKKfBWn-s0Y\"><img src=\"https://drive.google.com/uc?export=view&id=18cTn5y-8q7mr3BQr0O9FbWVswGN4uGnh\"></a>\n",
        "\n",
        "\n",
        "## Generality of learned features\n",
        "\n",
        "If we examine more in detail the learned representations based on given tasks, we can see, that:\n",
        "\n",
        "1. There are distinct patterns learned in the case of separate classes\n",
        "2. The low level features are not that distinct\n",
        "3. If we learn on a broad set of classes, maybe our features will be more general?\n",
        "\n",
        "<a href=\"https://deliveryimages.acm.org/10.1145/2010000/2001295/figs/f4.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1i2KNeTYlvqr--HJNdoKVdsYverTOuBAa\" width=85%></a>\n",
        "\n"
      ]
    }
  ]
}