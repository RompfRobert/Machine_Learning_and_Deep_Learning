{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSHhdguTHm0N"
      },
      "source": [
        "# Task: CIFAR-10 classification\n",
        "\n",
        "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "\n",
        "> \"consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        ">The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\"\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1wlfkvZgS0oBDwxKicWmYgtsJmP3IcMdj\">\n",
        "\n",
        "### Categories:\n",
        "\n",
        "- airplane \t\t\t\t\t\t\t\t\t\t\n",
        "- automobile \t\t\t\t\t\t\t\t\t\t\n",
        "- bird \t\t\t\t\t\t\t\t\t\t\n",
        "- cat \t\t\t\t\t\t\t\t\t\t\n",
        "- deer \t\t\t\t\t\t\t\t\t\t\n",
        "- dog \t\t\t\t\t\t\t\t\t\t\n",
        "- frog \t\t\t\t\t\t\t\t\t\t\n",
        "- horse \t\t\t\t\t\t\t\t\t\t\n",
        "- ship \t\t\t\t\t\t\t\t\t\t\n",
        "- truck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIX2ehpiHm0S"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:01.205778Z",
          "start_time": "2019-05-01T08:39:00.058811Z"
        },
        "collapsed": true,
        "id": "sTT_MyHltNm4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adadelta, Adam, SGD\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "# from tensorboardcolab import TensorBoardColab \n",
        "\n",
        "# Fix seeds for (hopefully) reproducible results\n",
        "from numpy.random import seed\n",
        "seed(14)\n",
        "tf.random.set_seed(19)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d9_yHMEtNnG"
      },
      "source": [
        "Download the data if necessary and load it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.264800Z",
          "start_time": "2019-05-01T08:39:01.207234Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "jFmPQL0xM_4p",
        "outputId": "1bf07532-560f-466b-b717-d9763d754e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 32s 0us/step\n",
            "(50000, 32, 32, 3) (5000, 32, 32, 3) (5000, 32, 32, 3)\n",
            "(50000, 1) (5000, 1) (5000, 1)\n"
          ]
        }
      ],
      "source": [
        "train, test = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_images, train_labels = train\n",
        "\n",
        "valid_test_images, valid_test_labels = test\n",
        "\n",
        "train_images = train_images / 255.\n",
        "\n",
        "valid_test_images = valid_test_images / 255.\n",
        "\n",
        "valid_images = valid_test_images[:5000]\n",
        "valid_labels = valid_test_labels[:5000]\n",
        "test_images = valid_test_images[5000:]\n",
        "test_labels = valid_test_labels[5000:]\n",
        "\n",
        "print(train_images.shape, valid_images.shape, test_images.shape)\n",
        "print(train_labels.shape, valid_labels.shape, test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxyeFqibHm05"
      },
      "source": [
        "# Model\n",
        "\n",
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.268218Z",
          "start_time": "2019-05-01T08:39:36.266265Z"
        },
        "collapsed": true,
        "id": "S075_4a6tNna"
      },
      "outputs": [],
      "source": [
        "n_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.277888Z",
          "start_time": "2019-05-01T08:39:36.269933Z"
        },
        "collapsed": true,
        "id": "CAMIScCf8BRZ"
      },
      "outputs": [],
      "source": [
        "# TASK - Hyperparameters\n",
        "# Fill in the initial values!\n",
        "# Later, experiment!\n",
        "#############################\n",
        "\n",
        "\n",
        "# dropout - Something between 0.0 < dropout_rate < 1.0, think in \"tens of percentages\" as default\n",
        "# dropout rate for conv layers\n",
        "dropout_rate_1 = 0.4 \n",
        "# dropout rate for fully connected layers\n",
        "dropout_rate_2 = 0.35\n",
        "\n",
        "# Choose an appropriate batch size for the training!\n",
        "batch_size = 100\n",
        "\n",
        "# Choose an appropriate number of epochs\n",
        "epoch_count = 25\n",
        "\n",
        "# These are the default parameters, you can experiment with learning rates, schedules, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACdo3SEitNnl"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.404889Z",
          "start_time": "2019-05-01T08:39:36.282271Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "o8OgyCFaHm1H",
        "outputId": "25978555-c8c8-40c0-9070-918aa2fa0416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 32, 32, 64)        4864      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 64)        102464    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 2,338,890\n",
            "Trainable params: 2,338,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# adapted from https://github.com/jtopor/CUNY-MSDA-661/blob/master/CIFAR-CNN/TF-Layers-CIFAR-GITHUB-v3.py\n",
        "\n",
        "\n",
        "tf.compat.v1.reset_default_graph() # It's good practice to clear and reset everything\n",
        "clear_session()          # even using Keras\n",
        "\n",
        "\n",
        "# WE USE FUNCTIONAL API!\n",
        "# (Could be different, but not now...)\n",
        "\n",
        "\n",
        "\n",
        "# Model\n",
        "#######\n",
        "\n",
        "# Define the input!\n",
        "# Remember, we have pictures with 32x32 pixels and 3 color channels\n",
        "# Disregard batch size, Keras will do that for us.\n",
        "x = Input(shape=(32,32,3))\n",
        "\n",
        "# Convolutional Layer #1: (batch_size, 32, 32, 3) -> (batch_size, 32, 32, 64)\n",
        "# Define a \"normal\" convolutional layer for images (not a single sequence, so ?D)\n",
        "# There should be 64 convolutional units\n",
        "# The kernel should be 5 in width and heigth\n",
        "# There should be padding so that the input and output dimensions would be equivalent\n",
        "# The non-linearity should be ReLU\n",
        "conv1 = Conv2D(filters=64,\n",
        "                kernel_size=[5, 5],\n",
        "                padding=\"same\",\n",
        "                activation=\"relu\")(x)\n",
        " \n",
        "# Pooling Layer #1: (batch_size, 32, 32, 64) -> (batch_size, 16, 16, 64)\n",
        "# Define a maximum based pooling layer with appropriate dimensions\n",
        "# The pooling size should be 2,2 and stride 2\n",
        "pool1 = MaxPool2D(pool_size=[2, 2], strides=2)(conv1)\n",
        "\n",
        "# Define a dropout layer with using the first dropout rate parameter\n",
        "dropout1 = Dropout(rate=dropout_rate_1)(pool1)\n",
        "\n",
        "# Convolutional Layer #2: (batch_size, 16, 16, 64) -> (batch_size, 16, 16, 64)\n",
        "# Repeat the prior conv layer\n",
        "# Watch for the right input\n",
        "conv2 = Conv2D(filters=64,\n",
        "               kernel_size=[5, 5],\n",
        "               padding=\"same\",\n",
        "               activation=\"relu\")(dropout1)\n",
        "  \n",
        "# Pooling Layer #2: (batch_size, 16, 16, 64) -> (batch_size, 8, 8, 64)\n",
        "# Repeat the prior pooling layer\n",
        "# Watch for the right input\n",
        "pool2 = MaxPool2D(pool_size=[2, 2], strides=2)(conv2)\n",
        "\n",
        "# Define a dropout layer with using the FIRST dropout rate parameter\n",
        "dropout2 = Dropout(rate=dropout_rate_1)(pool2)\n",
        "\n",
        "# Convert tensors into vectors: (batch_size, 8, 8, 64) -> (batch_size, 4096)\n",
        "# Use a single KERAS function, NO numpy or reshape magic!\n",
        "# Hint: the result is not 2D but \"flat\"\n",
        "pool2_flat = Flatten()(dropout2)\n",
        "\n",
        "# Fully connected Layer #1: (batch_size, 4096)-> (batch_size, 512)\n",
        "# Define a fully connected layer with 512 nodes and ReLU\n",
        "dense1 = Dense(units=512, activation=\"relu\")(pool2_flat)\n",
        "\n",
        "# Define a dropout layer with using the SECOND dropout rate parameter\n",
        "dropout3 = Dropout(rate=dropout_rate_2)(dense1)\n",
        "\n",
        "# Dense Layer #1: (batch_size, 512)-> (batch_size, 256)\n",
        "# Define a fully connected layer with 256 nodes and ReLU\n",
        "dense2 = Dense(units=256, activation=\"relu\")(dropout3)\n",
        "\n",
        "# Define a dropout layer with using the SECOND dropout rate parameter\n",
        "dropout4 = Dropout(rate=dropout_rate_2)(dense2)\n",
        "\n",
        "# Logits layer: (batch_size, 256) -> (batch_size, 10)\n",
        "# Define a fully connected layer with ??? nodes\n",
        "# Think about it, what shape should the output be?\n",
        "# What activation?\n",
        "# Think about it: we are in a classification problem!\n",
        "predictions = Dense(units = n_classes, activation='softmax')(dropout4)\n",
        "\n",
        "# Full model\n",
        "# Instantiate (initialize) the model with inputs and outputs\n",
        "model = Model(inputs=x, outputs=predictions)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGKQ-8bmtNn1"
      },
      "source": [
        "## Loss, optimization and compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.473837Z",
          "start_time": "2019-05-01T08:39:36.407595Z"
        },
        "collapsed": true,
        "id": "j1UmvSzLHm1R"
      },
      "outputs": [],
      "source": [
        "# Loss \n",
        "\n",
        "loss = sparse_categorical_crossentropy # we use this cross entropy variant as the input is not \n",
        "                                       # one-hot encoded\n",
        "\n",
        "# Optimizer\n",
        "# Choose an optimizer - adaptive ones work well here\n",
        "optimizer = Adam()\n",
        " \n",
        "# Compilation\n",
        "#############\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkPe16fDtNoJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:51:59.029061Z",
          "start_time": "2019-05-01T08:39:36.475186Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "zZ1d14lFtNoM",
        "outputId": "e6434d33-b03f-4db4-ed2f-69e785b3405b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 5000 samples\n",
            "Epoch 1/25\n",
            "50000/50000 [==============================] - 32s 636us/step - loss: 1.9014 - acc: 0.3057 - val_loss: 1.5180 - val_acc: 0.4700\n",
            "Epoch 2/25\n",
            "50000/50000 [==============================] - 29s 580us/step - loss: 1.4480 - acc: 0.4791 - val_loss: 1.2943 - val_acc: 0.5522\n",
            "Epoch 3/25\n",
            "50000/50000 [==============================] - 29s 581us/step - loss: 1.2650 - acc: 0.5522 - val_loss: 1.1649 - val_acc: 0.6012\n",
            "Epoch 4/25\n",
            "50000/50000 [==============================] - 29s 585us/step - loss: 1.1416 - acc: 0.5973 - val_loss: 0.9852 - val_acc: 0.6580\n",
            "Epoch 5/25\n",
            "50000/50000 [==============================] - 29s 581us/step - loss: 1.0509 - acc: 0.6336 - val_loss: 0.9153 - val_acc: 0.6830\n",
            "Epoch 6/25\n",
            "50000/50000 [==============================] - 29s 589us/step - loss: 0.9739 - acc: 0.6583 - val_loss: 0.8528 - val_acc: 0.7038\n",
            "Epoch 7/25\n",
            "50000/50000 [==============================] - 31s 612us/step - loss: 0.9208 - acc: 0.6809 - val_loss: 0.8612 - val_acc: 0.7056\n",
            "Epoch 8/25\n",
            "50000/50000 [==============================] - 30s 596us/step - loss: 0.8710 - acc: 0.6986 - val_loss: 0.8132 - val_acc: 0.7206\n",
            "Epoch 9/25\n",
            "50000/50000 [==============================] - 30s 597us/step - loss: 0.8318 - acc: 0.7113 - val_loss: 0.8187 - val_acc: 0.7204\n",
            "Epoch 10/25\n",
            "50000/50000 [==============================] - 30s 601us/step - loss: 0.7905 - acc: 0.7265 - val_loss: 0.7860 - val_acc: 0.7300\n",
            "Epoch 11/25\n",
            "50000/50000 [==============================] - 29s 579us/step - loss: 0.7606 - acc: 0.7369 - val_loss: 0.8563 - val_acc: 0.7096\n",
            "Epoch 12/25\n",
            "50000/50000 [==============================] - 29s 584us/step - loss: 0.7303 - acc: 0.7481 - val_loss: 0.7956 - val_acc: 0.7288\n",
            "Epoch 13/25\n",
            "50000/50000 [==============================] - 30s 604us/step - loss: 0.7047 - acc: 0.7571 - val_loss: 0.7491 - val_acc: 0.7414\n",
            "Epoch 14/25\n",
            "50000/50000 [==============================] - 31s 623us/step - loss: 0.6762 - acc: 0.7673 - val_loss: 0.7568 - val_acc: 0.7466\n",
            "Epoch 15/25\n",
            "50000/50000 [==============================] - 31s 614us/step - loss: 0.6627 - acc: 0.7725 - val_loss: 0.7227 - val_acc: 0.7574\n",
            "Epoch 16/25\n",
            "50000/50000 [==============================] - 29s 587us/step - loss: 0.6375 - acc: 0.7791 - val_loss: 0.7319 - val_acc: 0.7604\n",
            "Epoch 17/25\n",
            "50000/50000 [==============================] - 29s 588us/step - loss: 0.6202 - acc: 0.7863 - val_loss: 0.7255 - val_acc: 0.7538\n",
            "Epoch 18/25\n",
            "50000/50000 [==============================] - 29s 578us/step - loss: 0.6060 - acc: 0.7933 - val_loss: 0.7673 - val_acc: 0.7488\n",
            "Epoch 19/25\n",
            "50000/50000 [==============================] - 30s 593us/step - loss: 0.5862 - acc: 0.7996 - val_loss: 0.7315 - val_acc: 0.7628\n",
            "Epoch 20/25\n",
            "50000/50000 [==============================] - 29s 590us/step - loss: 0.5755 - acc: 0.8032 - val_loss: 0.7479 - val_acc: 0.7612\n",
            "Epoch 21/25\n",
            "50000/50000 [==============================] - 29s 588us/step - loss: 0.5543 - acc: 0.8105 - val_loss: 0.7459 - val_acc: 0.7604\n",
            "Epoch 22/25\n",
            "50000/50000 [==============================] - 29s 589us/step - loss: 0.5505 - acc: 0.8106 - val_loss: 0.7204 - val_acc: 0.7632\n",
            "Epoch 23/25\n",
            "50000/50000 [==============================] - 29s 589us/step - loss: 0.5312 - acc: 0.8185 - val_loss: 0.7445 - val_acc: 0.7640\n",
            "Epoch 24/25\n",
            "50000/50000 [==============================] - 29s 590us/step - loss: 0.5206 - acc: 0.8243 - val_loss: 0.7893 - val_acc: 0.7478\n",
            "Epoch 25/25\n",
            "50000/50000 [==============================] - 29s 588us/step - loss: 0.5169 - acc: 0.8253 - val_loss: 0.7332 - val_acc: 0.7690\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(x=train_images, y=train_labels,\n",
        "                    validation_data=(valid_images, valid_labels),\n",
        "                    epochs=epoch_count,\n",
        "                    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:51:59.236009Z",
          "start_time": "2019-05-01T08:51:59.036787Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "6GTGeChttNoi",
        "outputId": "146491c0-cc53-49a2-877d-7f50289ef5fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def display_history(history):\n",
        "    \"\"\"Summarize history for accuracy and loss.\n",
        "    \"\"\"\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "display_history(history);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:52:07.228831Z",
          "start_time": "2019-05-01T08:52:07.222761Z"
        },
        "collapsed": true,
        "id": "EaWwMwk08BRi"
      },
      "outputs": [],
      "source": [
        "assert max(history.history['val_accuracy']) > 0.75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7nD81aQtNo3"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:52:10.803767Z",
          "start_time": "2019-05-01T08:52:10.663477Z"
        },
        "collapsed": true,
        "id": "XcFR3MkStNo5"
      },
      "outputs": [],
      "source": [
        "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}