{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKNqb75Iy-Yw"
      },
      "source": [
        "# Task: sentiment classification\n",
        "\n",
        "The task is to classify one-sentence long movie reviews/opinions according to the sentiment they express. There are only two categories: positive and negative sentiment.\n",
        "\n",
        "\n",
        "> \"Data source: [UMICH SI650 - Sentiment Classification](https://www.kaggle.com/c/si650winter11/data)\n",
        "\n",
        "> Training data: 7086 lines. \n",
        "  \n",
        "> Format: 1|0 (tab) sentence\n",
        "\n",
        "> Test data: 33052 lines, each contains one sentence. \n",
        "\n",
        "> The data was originally collected from opinmind.com (which is no longer active).\"\n",
        "\n",
        "The data is in the file \"sentiment.tsv\"."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download/install necessary components/data"
      ],
      "metadata": {
        "id": "uuOoSX9nlutM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH8yC7Z1-ya5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "575fb454-a20c-4ca1-9355-91b886f7f52f"
      },
      "source": [
        "! python -m spacy download en_core_web_sm\n",
        "! pip install wordcloud\n",
        "! wget \"https://drive.google.com/uc?export=download&id=19NUVV29Pq-j2WrNBYf6WRD8or7SOUHp2\" -O sentiment.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
            "--2021-07-16 07:52:55--  https://drive.google.com/uc?export=download&id=19NUVV29Pq-j2WrNBYf6WRD8or7SOUHp2\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.218.138, 172.217.218.102, 172.217.218.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.218.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-3o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/miu7v9je13t94ufa2srb63g817vp3sub/1626421950000/10227734428265054086/*/19NUVV29Pq-j2WrNBYf6WRD8or7SOUHp2?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-07-16 07:52:56--  https://doc-0k-3o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/miu7v9je13t94ufa2srb63g817vp3sub/1626421950000/10227734428265054086/*/19NUVV29Pq-j2WrNBYf6WRD8or7SOUHp2?e=download\n",
            "Resolving doc-0k-3o-docs.googleusercontent.com (doc-0k-3o-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-0k-3o-docs.googleusercontent.com (doc-0k-3o-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 447540 (437K) [text/tab-separated-values]\n",
            "Saving to: ‘sentiment.tsv’\n",
            "\n",
            "sentiment.tsv       100%[===================>] 437.05K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-07-16 07:52:57 (145 MB/s) - ‘sentiment.tsv’ saved [447540/447540]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWjIpjZBy-Y2"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9liEvjzy-Y7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('sentiment.tsv', sep='\\t', \n",
        "                 quoting=3, # Quotes are _never_ field separators\n",
        "                 header=None)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRLzOOkny-ZO"
      },
      "source": [
        "df = df[[1,0]] # rearrange columns\n",
        "\n",
        "df.rename(columns={1:\"text\", 0:\"sentiment\"}, inplace=True) # rename columns\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-urvA8RDy-Zc"
      },
      "source": [
        "# Splitting into train, validation and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS5wjfjEy-Ze"
      },
      "source": [
        "Before doing anything else (!) we divide our data into train, validation and test parts,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcMx4I5gy-Zh"
      },
      "source": [
        "# Import the necessary function from Scikit\n",
        "from ...\n",
        "\n",
        "# Please observe, that we can only do a split into two\n",
        "# hence our best option is to call the function twice in a chain\n",
        "# Don't forget to fix the random seed also, eg to 13, since that is a lucky number! :-)\n",
        "# Try to make sure that the class proportions are the same in all three of the splits!\n",
        "df_train, df_test_valid = ...\n",
        "\n",
        "df_test, df_valid = ...\n",
        "\n",
        "assert len(df_train)==5668 and len(df_valid)==709 and len(df_test)==709\n",
        "print(len(df_train), len(df_valid), len(df_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Just to check class proportions.\n",
        "print(\"Classes (%):\")\n",
        "pd.concat([df_train[\"sentiment\"].rename(\"train\").value_counts() / len(df_train) * 100,\n",
        "          df_valid[\"sentiment\"].rename(\"valid\").value_counts() / len(df_valid) * 100,\n",
        "          df_test[\"sentiment\"].rename(\"test\").value_counts() / len(df_test) * 100,], axis=1).sort_index().round(2)"
      ],
      "metadata": {
        "id": "xhQ0LxTpnuWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCbUUTBty-Zq"
      },
      "source": [
        "# Inspecting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMZv_My5y-Zt"
      },
      "source": [
        "df_train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhMtyNyGy-Z4"
      },
      "source": [
        "We can examine the lengths of sentences as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_a-pzhcy-Z7"
      },
      "source": [
        "n_chars = df_train.text.apply(lambda x: len(x))\n",
        "\n",
        "n_chars.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UalJeEyOy-aJ"
      },
      "source": [
        "The first sentence with the maximal length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tujl0aK-y-aO"
      },
      "source": [
        "long_sentence = df_train.loc[n_chars.idxmax(), \"text\"]\n",
        "long_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr0k2LGsy-ag"
      },
      "source": [
        "# Extra task: Let's do a word cloud!\n",
        "\n",
        "Let us visualize together and separately (by category) the sentences!\n",
        "\n",
        "Tool: https://github.com/amueller/word_cloud\n",
        "\n",
        "\n",
        "Good example: https://github.com/amueller/word_cloud/blob/master/examples/simple.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_Wo1OyNdy-ak"
      },
      "source": [
        "# Helper function for displaying a word cloud\n",
        "# Input: one _UNIFIED_, space separated string!\n",
        "# Protip: https://www.tutorialspoint.com/python/string_join.htm\n",
        "def do_wordcloud(text, figsize=(15, 10)):\n",
        "    from wordcloud import WordCloud\n",
        "    \n",
        "    # Generate a word cloud image\n",
        "    wordcloud = WordCloud().generate(text)\n",
        "\n",
        "    # Display the generated image:\n",
        "    # the matplotlib way:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # lower max_font_size\n",
        "    wordcloud = WordCloud(max_font_size=40).generate(text)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "B8vuQclYy-av"
      },
      "source": [
        "### TASK !!! ####\n",
        "#Put here the world cloud!\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "S2j3sI2Dy-a0"
      },
      "source": [
        "### TASK !!! ####\n",
        "# Here only the cloud for sentences with negative sentiment!\n",
        "# Help: the shape of the DataFrame with only the negative sentences is: (2975, 2)\n",
        "# Source: https://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-GafZ4Py-a9"
      },
      "source": [
        "# Bag of words (BoW) representation of the texts\n",
        "\n",
        "We will represent each text as a (sparse) vector of lemma (word root) counts for frequent lemmas in the training data. \n",
        "\n",
        "For tokenization and lemmatization we use [spaCy](https://spacy.io/), an open source Python NLP library, which can produce a list of unique lemma ids from the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msjbzya6y-a_"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"]) \n",
        "# We only need the tokenizer, all higher functions are now unnecessary."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-wKS3rZy-bE"
      },
      "source": [
        "spaCy can produce spaCy Doc objects from texts that contain their linguistic analysis, among others lemmas and their unique spaCy string ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mNl6cRyy-bH"
      },
      "source": [
        "doc = nlp(long_sentence)\n",
        "type(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDVWmOamy-bP"
      },
      "source": [
        "print([token.lemma_ for token in doc ]) # Lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kklNRbxey-bX"
      },
      "source": [
        "print([token.lemma for token in doc]) # Connected unique ID-s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVDIxcI_y-br"
      },
      "source": [
        "Now we have to convert these lists into BoW vectors. We could \"roll our own\", but, fortunately, scikit-learn has a feature extractor doing exactly that, the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) so, for the sake of simplicity, we will use that along with spaCy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIPLl6eDy-bx"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def spacy_lemmatizer(s): \n",
        "    return [token.lemma for token in nlp(s)]\n",
        "    \n",
        "cv = CountVectorizer(analyzer=spacy_lemmatizer, #spaCy for analysis\n",
        "                     min_df= 0.001) # We ignore the lemmas with low document frequency\n",
        "cv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye0Hc8hOy-cP"
      },
      "source": [
        "sents = [\"I hate this movie.\", \"The movie is the worst I've seen.\"]\n",
        "bows = cv.fit_transform(sents)\n",
        "# A CountVectorizer produces a sparse matrix, we convert to ndarray to inspect it.\n",
        "# The rows are the sentences, the columns are the features: in our case, lemmas.\n",
        "pd.DataFrame(bows.toarray(), columns=[nlp.tokenizer.vocab.strings[k] for k in cv.get_feature_names_out()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### We will be using spacy's tokenizer, but just for comparison, \n",
        "### here is the scikit-only bow representation for the same sentences:\n",
        "for stop_words in [None, \"english\"]:\n",
        "    print(f\"Not spacy, but sklearn tokenization; stop_words: {stop_words}:\")\n",
        "    cv2 = CountVectorizer(analyzer=\"word\", #spaCy for analysis\n",
        "                        stop_words=stop_words,\n",
        "                        min_df= 0.001) # We ignore the lemmas with low document frequency\n",
        "    bows2 = cv2.fit_transform(sents)\n",
        "    # A CountVectorizer produces a sparse matrix, we convert to ndarray to inspect it.\n",
        "    # The rows are the sentences, the columns are the features: in our case, lemmas.\n",
        "    display(pd.DataFrame(bows2.toarray(), columns=[k for k in cv2.get_feature_names_out()]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "4oSM4I2--Wm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BMQTcX5y-cg"
      },
      "source": [
        "Using the CountVectorizer we convert the text columns of our train, validation and  test data into three sparse matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo9nOv7jy-ci"
      },
      "source": [
        "bows_train = cv.fit_transform(df_train.text)\n",
        "bows_train.sort_indices() # comes from TF2.0 sparse implementation, obscure requirement\n",
        "bow_length = bows_train.shape[1]  ## the number of features (lemmas) used\n",
        "print(\"BoW length:\", bow_length)\n",
        "bows_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## not necessary for the model, just so that we can check some encodings:\n",
        "cv_key_lemmahash_dict = {v: [k for k in cv.vocabulary_.keys() if cv.vocabulary_[k] == v][0] for v in cv.vocabulary_.values()}\n",
        "cv_key_lemmastring_dict = {k: nlp.tokenizer.vocab.strings[cv_key_lemmahash_dict[k]] for k in cv_key_lemmahash_dict.keys() }"
      ],
      "metadata": {
        "id": "Q092Cu0yHVr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "print(f\"The sentence at index {idx}:\")\n",
        "print(df_train[\"text\"].iloc[idx], \"\\n\")\n",
        "\n",
        "print(\"Its cv representation in the sparse matrix (index: (document, term), values: occurrences):\")\n",
        "tmp = bows_train[idx,:]\n",
        "print()\n",
        "print(tmp, \"\\n\")\n",
        "\n",
        "## using the dict we defined:\n",
        "print(\"CountVectorizer-encoded terms turned into the strings they encode:\")\n",
        "tmps = pd.Series(tmp.toarray()[0][tmp.indices], index=[cv_key_lemmastring_dict[k] for k in tmp.indices])\n",
        "print(tmps, \"\\n\")"
      ],
      "metadata": {
        "id": "p0s66OdQHOvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Just a little insight into spacy lemmatizer:\n",
        "print(\"'Awesome' lemmas:\")\n",
        "print([k for k in cv_key_lemmastring_dict.values() if k.lower() == \"awesome\"], \"\\n\")\n",
        "wholetext = df[\"text\"].str.cat(sep=\" \")\n",
        "for k in [\"Awesome\", \"awesome\", \"AWESOME\"]:\n",
        "  print(k, \"in a sentence:\", k in wholetext)"
      ],
      "metadata": {
        "id": "pcg7X1F9sSC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLd4g4oMy-cz"
      },
      "source": [
        "bows_valid = cv.transform(df_valid.text)\n",
        "bows_valid.sort_indices() # comes from TF2.0 sparse implementation, obscure requirement\n",
        "bows_test = cv.transform(df_test.text)\n",
        "bows_test.sort_indices() # comes from TF2.0 sparse implementation, obscure requirement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57_ybJKay-c5"
      },
      "source": [
        "# Task: The model\n",
        "\n",
        "We build a feed-forward neural network in Keras for our binary classification task, which will be trained with cross-entropy loss and minibatch SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me1ZXTecpfkT"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import tensorflow as tf\n",
        "\n",
        "## set random seed for reproducibility:\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "## clear session: important if we retrain the model with different hyperparams\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "# USE KERAS FUNCTIONAl API if possible, or SEQUENTIAL API if you prefer\n",
        "\n",
        "# Parameters\n",
        "############\n",
        "\n",
        "hidden_size = 100\n",
        "\n",
        "# Model\n",
        "#######\n",
        "## Create an \"empty\" model when using Sequential API.\n",
        "## Don't forget to import the class you need to do this!\n",
        "\n",
        "# Define (instantiate) the input layer if you use Functional API.\n",
        "# Give the shape parameter the length of a BoW vector as length\n",
        "# WARNING: shape only accepts a tuple, even if it is one dimensional \n",
        "# (do not forget the comma after the single number in that case)!\n",
        "\n",
        "\n",
        "# Hidden layer\n",
        "##############\n",
        "# Define a fully connected hidden layer that can be modified by the parameters above.\n",
        "# Use the ReLU activation function.\n",
        "# Give the inputs to the hidden layer if you use keras functional API,\n",
        "# or pay attention to giving the input shape (specified above) when using sequential API.\n",
        "# Please be aware, that in Keras functional API, the parameters defining the layer are \n",
        "# \"instantiation\" parameters, but the input of the layer is already a \"function call\" parameter!\n",
        "# (The magic lies in the brackets... ;-)\n",
        "\n",
        "\n",
        "\n",
        "# Softmax \n",
        "#########\n",
        "# Define the output softmax layer.\n",
        "# (Which is a fully connected layer with activation accordingly...)\n",
        "# Please remember, we have exactly two classes! \n",
        "# (We choose to use this generalized, Softmax approach...)\n",
        "# We feed the layer with the output of the hidden one in functional API.\n",
        "\n",
        "\n",
        "# Whole model\n",
        "##############\n",
        "# Nothing more is left than to instantiate the model when using functional API.\n",
        "# Please ensure input and output is right!\n",
        "\n",
        "\n",
        "# Optimization\n",
        "##############\n",
        "# For now, we stick to the basic SGD with a relatively large learning rate..\n",
        "optimizer = SGD(learning_rate=0.1)\n",
        " \n",
        "\n",
        "# Compilation and teaching\n",
        "##########################\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy', # use this cross entropy variant\n",
        "                                                      # since the input is not one-hot encoded\n",
        "              metrics=['accuracy']) #We measure and print accuracy during training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## print out a summary of the model\n"
      ],
      "metadata": {
        "id": "Cx-Rvw3dJJmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQjW47xDpfkU"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFZ22CKRpfkU"
      },
      "source": [
        "history = model.fit(x=bows_train, \n",
        "          y=df_train.sentiment.values,\n",
        "          validation_data=(bows_valid, df_valid.sentiment.values),\n",
        "          epochs=10,\n",
        "          batch_size=200)\n",
        "\n",
        "# Please don't just run, understand!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Run the code and interpret the plots.\n",
        "\n",
        "historydf = pd.DataFrame(history.history)\n",
        "\n",
        "historydf[[\"loss\", \"val_loss\"]].plot();\n",
        "\n",
        "historydf[[\"accuracy\", \"val_accuracy\"]].plot();"
      ],
      "metadata": {
        "id": "el6FAfEMBkQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ3S8n4LpfkU"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c_t_yy0pfkV"
      },
      "source": [
        "print(\"=== INTERACTIVE DEMO ===\")\n",
        "while True:\n",
        "    s = input(\"Enter a short text to evaluate or press return to quit: \")\n",
        "    if s == \"\":\n",
        "        break\n",
        "    else:\n",
        "        bow = cv.transform([s])\n",
        "        prob_pred = model.predict(bow[0])\n",
        "        print(f\"Positive vs negative sentiment probability: {prob_pred[0,1]} vs {prob_pred[0,0]}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}